# =====================================
# ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œæ”¯æ´ã‚·ã‚¹ãƒ†ãƒ 
# =====================================

import json
import hashlib
import threading
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, deque
import numpy as np
import pandas as pd
from pathlib import Path
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Rectangle
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# =====================================
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ å®šç¾©
# =====================================

@dataclass
class IncidentMetadata:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    incident_id: str
    detection_time: datetime
    severity: str  # critical, high, medium, low
    attack_type: str
    affected_users: List[str] = field(default_factory=list)
    affected_systems: List[str] = field(default_factory=list)
    status: str = "detected"  # detected, analyzing, contained, resolved
    
@dataclass
class ForensicEvidence:
    """ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯è¨¼æ‹ """
    evidence_id: str
    incident_id: str
    timestamp: datetime
    evidence_type: str  # log, memory, network, file
    data: Dict[str, Any]
    hash_value: str = ""
    
    def __post_init__(self):
        if not self.hash_value:
            self.hash_value = self._calculate_hash()
    
    def _calculate_hash(self) -> str:
        """è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—"""
        data_str = json.dumps(self.data, sort_keys=True)
        return hashlib.sha256(data_str.encode()).hexdigest()

# =====================================
# Phase 1: å³æ™‚å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ 
# =====================================

class ImmediateResponseSystem:
    """å³æ™‚å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ï¼ˆã€œ30ç§’ï¼‰"""
    
    def __init__(self, chain_manager):
        self.chain_manager = chain_manager
        self.response_queue = deque()
        self.containment_rules = self._init_containment_rules()
        
    def _init_containment_rules(self) -> Dict:
        """å°ã˜è¾¼ã‚ãƒ«ãƒ¼ãƒ«ã®åˆæœŸåŒ–"""
        return {
            "investigating": {
                "actions": ["enhance_logging", "notify_admin"],
                "auto_execute": True
            },
            "suspicious": {
                "actions": ["alert_soc", "capture_evidence", "limit_access"],
                "auto_execute": True
            },
            "critical": {
                "actions": ["isolate_network", "freeze_account", "kill_process"],
                "auto_execute": False,  # æ‰¿èªå¿…è¦
                "approval_timeout": 30  # 30ç§’
            }
        }
    
    async def process_incident(self, event: Dict, detection_result: Dict) -> Dict:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®å³æ™‚å‡¦ç†"""
        start_time = datetime.now()
        
        # 1. ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        incident = self._create_incident(event, detection_result)
        
        # 2. è‡ªå‹•å°ã˜è¾¼ã‚åˆ¤å®š
        containment_actions = await self._determine_containment(incident, event)
        
        # 3. ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆç™ºå ±
        alert_result = self._send_emergency_alert(incident, containment_actions)
        
        # 4. è¨¼æ‹ ä¿å…¨é–‹å§‹
        evidence_task = asyncio.create_task(
            self._preserve_initial_evidence(incident, event)
        )
        
        # 5. ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¨˜éŒ²
        response_time = (datetime.now() - start_time).total_seconds()
        
        return {
            "incident": incident,
            "containment_actions": containment_actions,
            "alert_result": alert_result,
            "evidence_task": evidence_task,
            "response_time": response_time,
            "phase": "immediate_response"
        }
    
    def _create_incident(self, event: Dict, detection_result: Dict) -> IncidentMetadata:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
        incident_id = f"INC-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{event.get('user_id', 'unknown')}"
        
        return IncidentMetadata(
            incident_id=incident_id,
            detection_time=datetime.now(),
            severity=detection_result.get("status", "unknown"),
            attack_type=self._classify_attack_type(event),
            affected_users=[event.get("user_id", "unknown")],
            affected_systems=self._identify_affected_systems(event)
        )
    
    def _classify_attack_type(self, event: Dict) -> str:
        """æ”»æ’ƒã‚¿ã‚¤ãƒ—ã®åˆ†é¡"""
        operation = event.get("operation", "").lower()
        file_path = event.get("file_path", "").lower()
        process = event.get("process_name", "").lower()
        
        # æ”»æ’ƒãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        if "powershell" in process or "cmd" in process:
            if "confidential" in file_path:
                return "data_exfiltration"
            return "privilege_escalation"
        elif operation in ["filedelete", "processeterminate"]:
            return "destructive_attack"
        elif operation == "filecopy" and event.get("file_size_kb", 0) > 100000:
            return "data_theft"
        elif event.get("cross_dept_warning"):
            return "lateral_movement"
        
        return "unknown_attack"
    
    def _identify_affected_systems(self, event: Dict) -> List[str]:
        """å½±éŸ¿ã‚’å—ã‘ãŸã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å®š"""
        systems = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ã‚·ã‚¹ãƒ†ãƒ æ¨å®š
        file_path = event.get("file_path", "")
        if "\\sales\\" in file_path:
            systems.append("å–¶æ¥­ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ")
        if "\\finance\\" in file_path:
            systems.append("è²¡å‹™ã‚·ã‚¹ãƒ†ãƒ ")
        if "\\hr\\" in file_path:
            systems.append("äººäº‹ã‚·ã‚¹ãƒ†ãƒ ")
        
        # IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‹ã‚‰ã‚·ã‚¹ãƒ†ãƒ æ¨å®š
        dest_ip = event.get("destination_ip", "")
        if dest_ip.startswith("192.168.1."):
            systems.append("å–¶æ¥­éƒ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
        elif dest_ip.startswith("192.168.3."):
            systems.append("çµŒç†éƒ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
        
        return systems or ["ä¸æ˜ãªã‚·ã‚¹ãƒ†ãƒ "]
    
    async def _determine_containment(self, incident: IncidentMetadata, event: Dict) -> Dict:
        """å°ã˜è¾¼ã‚æˆ¦ç•¥ã®æ±ºå®š"""
        severity = incident.severity
        rules = self.containment_rules.get(severity, {})
        
        actions_to_execute = []
        
        if rules.get("auto_execute", False):
            # è‡ªå‹•å®Ÿè¡Œ
            for action in rules.get("actions", []):
                result = await self._execute_containment_action(action, incident, event)
                actions_to_execute.append({
                    "action": action,
                    "status": "executed",
                    "result": result
                })
        else:
            # æ‰¿èªå¾…ã¡
            for action in rules.get("actions", []):
                actions_to_execute.append({
                    "action": action,
                    "status": "pending_approval",
                    "timeout": rules.get("approval_timeout", 30)
                })
        
        return {
            "severity": severity,
            "actions": actions_to_execute,
            "timestamp": datetime.now()
        }
    
    async def _execute_containment_action(self, action: str, incident: IncidentMetadata, 
                                        event: Dict) -> Dict:
        """å°ã˜è¾¼ã‚ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ"""
        # å®Ÿéš›ã®ç’°å¢ƒã§ã¯å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å¯¾å¿œã™ã‚‹APIã‚’å‘¼ã³å‡ºã™
        action_map = {
            "enhance_logging": self._enhance_logging,
            "notify_admin": self._notify_admin,
            "alert_soc": self._alert_soc,
            "capture_evidence": self._capture_evidence,
            "limit_access": self._limit_access,
            "isolate_network": self._isolate_network,
            "freeze_account": self._freeze_account,
            "kill_process": self._kill_process
        }
        
        handler = action_map.get(action)
        if handler:
            return await handler(incident, event)
        
        return {"status": "error", "message": f"Unknown action: {action}"}
    
    async def _enhance_logging(self, incident: IncidentMetadata, event: Dict) -> Dict:
        """ãƒ­ã‚°è¨˜éŒ²ã®å¼·åŒ–"""
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        await asyncio.sleep(0.1)
        return {
            "status": "success",
            "enhanced_targets": incident.affected_systems,
            "log_level": "DEBUG"
        }
    
    async def _isolate_network(self, incident: IncidentMetadata, event: Dict) -> Dict:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é›¢"""
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        await asyncio.sleep(0.2)
        return {
            "status": "success",
            "isolated_ip": event.get("source_ip"),
            "vlan_id": "quarantine_vlan_999"
        }
    
    def _send_emergency_alert(self, incident: IncidentMetadata, 
                            containment_actions: Dict) -> Dict:
        """ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆã®é€ä¿¡"""
        alert = {
            "alert_id": f"ALERT-{incident.incident_id}",
            "severity": incident.severity,
            "title": f"{incident.attack_type} detected - {incident.incident_id}",
            "affected_users": incident.affected_users,
            "affected_systems": incident.affected_systems,
            "containment_status": containment_actions,
            "timestamp": datetime.now(),
            "channels": ["email", "slack", "pagerduty"]
        }
        
        # å®Ÿéš›ã®ç’°å¢ƒã§ã¯é€šçŸ¥APIã‚’å‘¼ã³å‡ºã™
        print(f"ğŸš¨ EMERGENCY ALERT: {alert['title']}")
        
        return alert
    
    async def _preserve_initial_evidence(self, incident: IncidentMetadata, 
                                       event: Dict) -> ForensicEvidence:
        """åˆæœŸè¨¼æ‹ ã®ä¿å…¨"""
        evidence = ForensicEvidence(
            evidence_id=f"EVD-{incident.incident_id}-001",
            incident_id=incident.incident_id,
            timestamp=datetime.now(),
            evidence_type="initial_detection",
            data={
                "raw_event": event,
                "detection_context": {
                    "chain_blocks": self._get_relevant_blocks(event),
                    "user_history": self._get_user_history(event)
                }
            }
        )
        
        # è¨¼æ‹ ã®æ°¸ç¶šåŒ–ï¼ˆå®Ÿéš›ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚„å°‚ç”¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼‰
        await self._store_evidence(evidence)
        
        return evidence
    
    def _get_relevant_blocks(self, event: Dict) -> List[Dict]:
        """é–¢é€£ã™ã‚‹ãƒã‚§ãƒ¼ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã®å–å¾—"""
        # ãƒã‚§ãƒ¼ãƒ³ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰é–¢é€£ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—
        user_id = event.get("user_id")
        dept = event.get("department")
        
        blocks = []
        if dept and dept in self.chain_manager.department_chains:
            chain = self.chain_manager.department_chains[dept]
            # æœ€æ–°20ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—
            for block in chain.blocks[-20:]:
                if block.metadata.get("user_id") == user_id:
                    blocks.append({
                        "index": block.index,
                        "timestamp": block.metadata.get("timestamp"),
                        "security_mode": block.metadata.get("security_mode"),
                        "divergence": block.divergence
                    })
        
        return blocks
    
    def _get_user_history(self, event: Dict) -> Dict:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å±¥æ­´ã®å–å¾—"""
        user_id = event.get("user_id")
        if user_id in self.chain_manager.user_chains:
            user_chain = self.chain_manager.user_chains[user_id]
            return {
                "baseline_behavior": user_chain.baseline_behavior,
                "recent_alerts": user_chain.get_recent_alerts(n=10)
            }
        return {}
    
    async def _store_evidence(self, evidence: ForensicEvidence):
        """è¨¼æ‹ ã®ä¿å­˜"""
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        await asyncio.sleep(0.1)
        print(f"ğŸ“¦ Evidence preserved: {evidence.evidence_id}")

# =====================================
# Phase 2: åˆå‹•åˆ†æã‚·ã‚¹ãƒ†ãƒ 
# =====================================

class InitialAnalysisSystem:
    """åˆå‹•åˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼ˆã€œ5åˆ†ï¼‰"""
    
    def __init__(self, chain_manager):
        self.chain_manager = chain_manager
        self.timeline_generator = IncidentTimeline()
        
    async def analyze_incident(self, incident: IncidentMetadata, 
                             initial_evidence: ForensicEvidence) -> Dict:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®åˆå‹•åˆ†æ"""
        print(f"\nğŸ” Phase 2: åˆå‹•åˆ†æé–‹å§‹ - {incident.incident_id}")
        
        # 1. ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³è‡ªå‹•ç”Ÿæˆ
        timeline = await self.timeline_generator.generate_timeline(
            incident, 
            self.chain_manager
        )
        
        # 2. å½±éŸ¿ç¯„å›²ã®ç‰¹å®š
        impact_scope = self._identify_impact_scope(incident, initial_evidence)
        
        # 3. æ”»æ’ƒæ‰‹æ³•ã®åˆ†é¡
        attack_classification = self._classify_attack_technique(
            incident, 
            timeline, 
            initial_evidence
        )
        
        return {
            "incident_id": incident.incident_id,
            "timeline": timeline,
            "impact_scope": impact_scope,
            "attack_classification": attack_classification,
            "phase": "initial_analysis",
            "analysis_time": datetime.now()
        }
    
    def _identify_impact_scope(self, incident: IncidentMetadata, 
                             evidence: ForensicEvidence) -> Dict:
        """å½±éŸ¿ç¯„å›²ã®ç‰¹å®š"""
        scope = {
            "direct_impact": {
                "users": set(incident.affected_users),
                "systems": set(incident.affected_systems),
                "data_assets": []
            },
            "potential_impact": {
                "users": set(),
                "systems": set(),
                "departments": set()
            },
            "risk_level": "unknown"
        }
        
        # è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å½±éŸ¿ç¯„å›²ã‚’æ‹¡å¼µ
        event_data = evidence.data.get("raw_event", {})
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã®å½±éŸ¿
        if "file_path" in event_data:
            file_path = event_data["file_path"]
            scope["direct_impact"]["data_assets"].append(file_path)
            
            # å…±æœ‰ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆã€ä»–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å½±éŸ¿å¯èƒ½æ€§
            if "\\shared\\" in file_path or "\\fileserver\\" in file_path:
                scope["potential_impact"]["users"].update(
                    self._get_shared_folder_users(file_path)
                )
        
        # æ¨ªå±•é–‹ã®å¯èƒ½æ€§è©•ä¾¡
        if incident.attack_type == "lateral_movement":
            scope["potential_impact"]["departments"].update(
                self._get_connected_departments(incident.affected_users[0])
            )
        
        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—
        total_affected = (
            len(scope["direct_impact"]["users"]) + 
            len(scope["direct_impact"]["systems"]) +
            len(scope["potential_impact"]["users"]) +
            len(scope["potential_impact"]["departments"])
        )
        
        if total_affected > 10:
            scope["risk_level"] = "critical"
        elif total_affected > 5:
            scope["risk_level"] = "high"
        elif total_affected > 2:
            scope["risk_level"] = "medium"
        else:
            scope["risk_level"] = "low"
        
        return scope
    
    def _get_shared_folder_users(self, file_path: str) -> List[str]:
        """å…±æœ‰ãƒ•ã‚©ãƒ«ãƒ€ã®ã‚¢ã‚¯ã‚»ã‚¹ãƒ¦ãƒ¼ã‚¶ãƒ¼å–å¾—ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        # å®Ÿéš›ã¯Active Directoryã‚„ãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒ¼ãƒãƒ¼ã®ACLã‹ã‚‰å–å¾—
        dept_users = {
            "\\sales\\": ["yamada_t", "suzuki_m", "tanaka_s"],
            "\\finance\\": ["ito_h", "nakamura_r", "ogawa_s"],
            "\\hr\\": ["kato_m", "ishida_j", "hayashi_r"]
        }
        
        for path_pattern, users in dept_users.items():
            if path_pattern in file_path:
                return users
        
        return []
    
    def _get_connected_departments(self, user_id: str) -> List[str]:
        """æ¥ç¶šå¯èƒ½ãªéƒ¨ç½²ã®å–å¾—"""
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éƒ¨ç½²ã‹ã‚‰æ¨ªå±•é–‹å¯èƒ½ãªéƒ¨ç½²ã‚’æ¨å®š
        user_dept_map = {
            "sales": ["sales", "finance"],
            "engineering": ["engineering", "sales"],
            "finance": ["finance", "hr"],
            "hr": ["hr", "finance"]
        }
        
        user_dept = self.chain_manager.user_department_map.get(user_id, "unknown")
        return user_dept_map.get(user_dept, [])
    
    def _classify_attack_technique(self, incident: IncidentMetadata, 
                                 timeline: Dict, evidence: ForensicEvidence) -> Dict:
        """æ”»æ’ƒæ‰‹æ³•ã®åˆ†é¡ï¼ˆMITRE ATT&CKãƒ™ãƒ¼ã‚¹ï¼‰"""
        classification = {
            "tactics": [],
            "techniques": [],
            "confidence": 0.0,
            "mitre_mapping": []
        }
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‹ã‚‰æ”»æ’ƒæ‰‹æ³•ã‚’æ¨å®š
        if timeline.get("reconnaissance"):
            classification["tactics"].append("Reconnaissance")
            classification["techniques"].append("Active Scanning")
            classification["mitre_mapping"].append("T1595")
        
        if timeline.get("privilege_escalation"):
            classification["tactics"].append("Privilege Escalation")
            classification["techniques"].append("Valid Accounts")
            classification["mitre_mapping"].append("T1078")
        
        if timeline.get("lateral_movement"):
            classification["tactics"].append("Lateral Movement")
            classification["techniques"].append("Remote Services")
            classification["mitre_mapping"].append("T1021")
        
        if timeline.get("data_collection"):
            classification["tactics"].append("Collection")
            classification["techniques"].append("Data from Local System")
            classification["mitre_mapping"].append("T1005")
        
        # ä¿¡é ¼åº¦ã®è¨ˆç®—
        evidence_count = len([k for k, v in timeline.items() if v])
        classification["confidence"] = min(evidence_count * 0.25, 1.0)
        
        return classification

# =====================================
# ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ç”Ÿæˆã‚¯ãƒ©ã‚¹
# =====================================

class IncidentTimeline:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ç”Ÿæˆ"""
    
    async def generate_timeline(self, incident: IncidentMetadata, 
                              chain_manager) -> Dict:
        """æ”»æ’ƒã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®ç”Ÿæˆ"""
        timeline = {
            "incident_id": incident.incident_id,
            "phases": {},
            "events": [],
            "visualization": None
        }
        
        # æ”»æ’ƒãƒ•ã‚§ãƒ¼ã‚ºã®ç‰¹å®š
        phases = await self._identify_attack_phases(incident, chain_manager)
        timeline["phases"] = phases
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã®æ™‚ç³»åˆ—æ•´ç†
        events = self._extract_timeline_events(incident, chain_manager)
        timeline["events"] = sorted(events, key=lambda x: x["timestamp"])
        
        # å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
        timeline["visualization"] = self._create_timeline_visualization(
            timeline["phases"], 
            timeline["events"]
        )
        
        return timeline
    
    async def _identify_attack_phases(self, incident: IncidentMetadata, 
                                    chain_manager) -> Dict:
        """æ”»æ’ƒãƒ•ã‚§ãƒ¼ã‚ºã®ç‰¹å®š"""
        phases = {
            "reconnaissance": None,
            "initial_access": None,
            "privilege_escalation": None,
            "lateral_movement": None,
            "data_collection": None,
            "data_exfiltration": None,
            "cleanup": None
        }
        
        # ãƒã‚§ãƒ¼ãƒ³ã‹ã‚‰é–¢é€£ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡º
        user_id = incident.affected_users[0] if incident.affected_users else None
        if not user_id:
            return phases
        
        # éƒ¨ç½²ãƒã‚§ãƒ¼ãƒ³ã‹ã‚‰æ™‚ç³»åˆ—ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–å¾—
        dept = chain_manager.user_department_map.get(user_id)
        if dept and dept in chain_manager.department_chains:
            chain = chain_manager.department_chains[dept]
            
            for block in chain.blocks:
                if block.metadata.get("user_id") != user_id:
                    continue
                
                # ãƒ•ã‚§ãƒ¼ã‚ºåˆ¤å®š
                phase = self._determine_phase(block)
                if phase and not phases[phase]:
                    phases[phase] = {
                        "timestamp": block.metadata.get("timestamp"),
                        "block_index": block.index,
                        "evidence": block.metadata
                    }
        
        return phases
    
    def _determine_phase(self, block) -> Optional[str]:
        """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰æ”»æ’ƒãƒ•ã‚§ãƒ¼ã‚ºã‚’åˆ¤å®š"""
        metadata = block.metadata
        operation = metadata.get("operation", "").lower()
        mode = metadata.get("security_mode", "")
        
        # ãƒ•ã‚§ãƒ¼ã‚ºãƒãƒƒãƒ”ãƒ³ã‚°
        if mode == "normal" and operation in ["fileread", "processlist"]:
            return "reconnaissance"
        elif mode in ["investigating", "suspicious"] and operation == "login":
            return "initial_access"
        elif "powershell" in metadata.get("process_name", "").lower():
            return "privilege_escalation"
        elif metadata.get("cross_dept_warning"):
            return "lateral_movement"
        elif operation in ["filecopy", "fileread"] and metadata.get("file_size_kb", 0) > 10000:
            return "data_collection"
        elif metadata.get("destination_ip", "").startswith("203."):
            return "data_exfiltration"
        elif operation == "filedelete":
            return "cleanup"
        
        return None
    
    def _extract_timeline_events(self, incident: IncidentMetadata, 
                               chain_manager) -> List[Dict]:
        """ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆã®æŠ½å‡º"""
        events = []
        user_id = incident.affected_users[0] if incident.affected_users else None
        
        if not user_id:
            return events
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒã‚§ãƒ¼ãƒ³ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º
        if user_id in chain_manager.user_chains:
            user_chain = chain_manager.user_chains[user_id]
            
            for block in user_chain.blocks[-50:]:  # æœ€æ–°50ãƒ–ãƒ­ãƒƒã‚¯
                event = {
                    "timestamp": block.metadata.get("timestamp", ""),
                    "operation": block.metadata.get("operation", ""),
                    "severity": block.metadata.get("security_mode", ""),
                    "divergence": block.divergence,
                    "details": {
                        "file_path": block.metadata.get("file_path"),
                        "process": block.metadata.get("process_name"),
                        "destination": block.metadata.get("destination_ip")
                    }
                }
                events.append(event)
        
        return events
    
    def _create_timeline_visualization(self, phases: Dict, events: List[Dict]) -> Dict:
        """ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
        viz_data = {
            "type": "timeline",
            "phases": [],
            "events": [],
            "severity_map": {
                "normal": "ğŸŸ¢",
                "investigating": "ğŸŸ¡",
                "suspicious": "ğŸŸ ",
                "critical": "ğŸ”´"
            }
        }
        
        # ãƒ•ã‚§ãƒ¼ã‚ºãƒ‡ãƒ¼ã‚¿
        for phase_name, phase_data in phases.items():
            if phase_data:
                viz_data["phases"].append({
                    "name": phase_name,
                    "timestamp": phase_data["timestamp"],
                    "icon": self._get_phase_icon(phase_name)
                })
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        sampled_events = events[::max(1, len(events)//20)]  # æœ€å¤§20ã‚¤ãƒ™ãƒ³ãƒˆ
        for event in sampled_events:
            viz_data["events"].append({
                "time": event["timestamp"].split()[1] if event["timestamp"] else "00:00:00",
                "operation": event["operation"],
                "severity_icon": viz_data["severity_map"].get(event["severity"], "âšª"),
                "divergence": event["divergence"]
            })
        
        return viz_data
    
    def _get_phase_icon(self, phase: str) -> str:
        """ãƒ•ã‚§ãƒ¼ã‚ºã‚¢ã‚¤ã‚³ãƒ³ã®å–å¾—"""
        icons = {
            "reconnaissance": "ğŸ”",
            "initial_access": "ğŸšª",
            "privilege_escalation": "â¬†ï¸",
            "lateral_movement": "â¡ï¸",
            "data_collection": "ğŸ“Š",
            "data_exfiltration": "ğŸ“¤",
            "cleanup": "ğŸ§¹"
        }
        return icons.get(phase, "â“")

# =====================================
# Phase 3: è©³ç´°èª¿æŸ»ã‚·ã‚¹ãƒ†ãƒ 
# =====================================

class DetailedInvestigationSystem:
    """è©³ç´°èª¿æŸ»ã‚·ã‚¹ãƒ†ãƒ ï¼ˆã€œ30åˆ†ï¼‰"""
    
    def __init__(self, chain_manager):
        self.chain_manager = chain_manager
        self.root_cause_analyzer = RootCauseAnalyzer()
        self.risk_evaluator = LateralMovementRiskEvaluator()
        
    async def investigate_incident(self, incident: IncidentMetadata, 
                                 initial_analysis: Dict) -> Dict:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®è©³ç´°èª¿æŸ»"""
        print(f"\nğŸ”¬ Phase 3: è©³ç´°èª¿æŸ»é–‹å§‹ - {incident.incident_id}")
        
        # 1. æ ¹æœ¬åŸå› åˆ†æ
        root_cause = await self.root_cause_analyzer.analyze(
            incident, 
            initial_analysis,
            self.chain_manager
        )
        
        # 2. æ¨ªå±•é–‹ãƒªã‚¹ã‚¯è©•ä¾¡
        lateral_risk = self.risk_evaluator.evaluate(
            incident,
            initial_analysis["impact_scope"],
            self.chain_manager
        )
        
        # 3. å¯¾ç­–å„ªå…ˆåº¦æ±ºå®š
        mitigation_priority = self._determine_mitigation_priority(
            root_cause,
            lateral_risk,
            initial_analysis
        )
        
        return {
            "incident_id": incident.incident_id,
            "root_cause_analysis": root_cause,
            "lateral_movement_risk": lateral_risk,
            "mitigation_priority": mitigation_priority,
            "phase": "detailed_investigation",
            "investigation_time": datetime.now()
        }
    
    def _determine_mitigation_priority(self, root_cause: Dict, 
                                     lateral_risk: Dict, 
                                     initial_analysis: Dict) -> List[Dict]:
        """å¯¾ç­–å„ªå…ˆåº¦ã®æ±ºå®š"""
        mitigations = []
        
        # æ ¹æœ¬åŸå› ã«åŸºã¥ãå¯¾ç­–
        if root_cause.get("vulnerability_type") == "weak_authentication":
            mitigations.append({
                "action": "enforce_mfa",
                "priority": "critical",
                "estimated_time": "30 minutes",
                "impact": "high"
            })
        
        if root_cause.get("vulnerability_type") == "unpatched_system":
            mitigations.append({
                "action": "emergency_patching",
                "priority": "high",
                "estimated_time": "2 hours",
                "impact": "medium"
            })
        
        # æ¨ªå±•é–‹ãƒªã‚¹ã‚¯ã«åŸºã¥ãå¯¾ç­–
        if lateral_risk.get("risk_score", 0) > 0.7:
            mitigations.append({
                "action": "network_segmentation",
                "priority": "critical",
                "estimated_time": "1 hour",
                "impact": "high"
            })
        
        # å½±éŸ¿ç¯„å›²ã«åŸºã¥ãå¯¾ç­–
        if initial_analysis["impact_scope"]["risk_level"] == "critical":
            mitigations.append({
                "action": "full_system_isolation",
                "priority": "critical",
                "estimated_time": "15 minutes",
                "impact": "very_high"
            })
        
        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
        priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        mitigations.sort(key=lambda x: priority_order.get(x["priority"], 999))
        
        return mitigations

# =====================================
# æ ¹æœ¬åŸå› åˆ†æã‚¯ãƒ©ã‚¹
# =====================================

class RootCauseAnalyzer:
    """æ ¹æœ¬åŸå› åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    async def analyze(self, incident: IncidentMetadata, 
                     initial_analysis: Dict, 
                     chain_manager) -> Dict:
        """æ ¹æœ¬åŸå› ã®åˆ†æ"""
        root_cause = {
            "incident_id": incident.incident_id,
            "initial_vector": None,
            "vulnerability_type": None,
            "contributing_factors": [],
            "confidence": 0.0,
            "evidence_chain": []
        }
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‹ã‚‰åˆæœŸä¾µå…¥ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç‰¹å®š
        timeline = initial_analysis.get("timeline", {})
        phases = timeline.get("phases", {})
        
        # åˆæœŸã‚¢ã‚¯ã‚»ã‚¹ãƒ•ã‚§ãƒ¼ã‚ºã®åˆ†æ
        if phases.get("initial_access"):
            initial_access = phases["initial_access"]
            root_cause["initial_vector"] = self._analyze_initial_vector(
                initial_access["evidence"]
            )
        
        # è„†å¼±æ€§ã®ç‰¹å®š
        root_cause["vulnerability_type"] = self._identify_vulnerability(
            incident,
            initial_analysis,
            chain_manager
        )
        
        # å¯„ä¸è¦å› ã®åˆ†æ
        root_cause["contributing_factors"] = self._analyze_contributing_factors(
            incident,
            chain_manager
        )
        
        # è¨¼æ‹ ãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰
        root_cause["evidence_chain"] = self._build_evidence_chain(
            timeline,
            chain_manager
        )
        
        # ä¿¡é ¼åº¦ã®è¨ˆç®—
        root_cause["confidence"] = self._calculate_confidence(root_cause)
        
        return root_cause
    
    def _analyze_initial_vector(self, evidence: Dict) -> str:
        """åˆæœŸä¾µå…¥ãƒ™ã‚¯ãƒˆãƒ«ã®åˆ†æ"""
        operation = evidence.get("operation", "").lower()
        source_ip = evidence.get("source_ip", "")
        
        if operation == "login" and not source_ip.startswith("192.168."):
            return "external_remote_access"
        elif operation == "loginfailed":
            return "brute_force_attempt"
        elif "phishing" in evidence.get("file_path", "").lower():
            return "phishing_email"
        else:
            return "unknown_vector"
    
    def _identify_vulnerability(self, incident: IncidentMetadata, 
                               initial_analysis: Dict, 
                               chain_manager) -> str:
        """è„†å¼±æ€§ã‚¿ã‚¤ãƒ—ã®ç‰¹å®š"""
        # æ”»æ’ƒåˆ†é¡ã‹ã‚‰è„†å¼±æ€§ã‚’æ¨å®š
        attack_class = initial_analysis.get("attack_classification", {})
        techniques = attack_class.get("techniques", [])
        
        if "Valid Accounts" in techniques:
            # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆé–¢é€£ã®è„†å¼±æ€§ãƒã‚§ãƒƒã‚¯
            user_id = incident.affected_users[0] if incident.affected_users else None
            if user_id and user_id in chain_manager.user_chains:
                user_chain = chain_manager.user_chains[user_id]
                # éå»ã®ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—ã‚’ãƒã‚§ãƒƒã‚¯
                failed_logins = sum(
                    1 for block in user_chain.blocks 
                    if block.metadata.get("operation") == "LoginFailed"
                )
                if failed_logins > 5:
                    return "weak_authentication"
        
        if "PowerShell" in str(initial_analysis):
            return "unrestricted_script_execution"
        
        if incident.attack_type == "lateral_movement":
            return "excessive_privileges"
        
        return "unknown_vulnerability"
    
    def _analyze_contributing_factors(self, incident: IncidentMetadata, 
                                    chain_manager) -> List[str]:
        """å¯„ä¸è¦å› ã®åˆ†æ"""
        factors = []
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        user_id = incident.affected_users[0] if incident.affected_users else None
        if user_id and user_id in chain_manager.user_chains:
            user_chain = chain_manager.user_chains[user_id]
            
            # ç•°å¸¸ãªæ™‚é–“å¸¯ã®ã‚¢ã‚¯ã‚»ã‚¹
            night_access = sum(
                1 for block in user_chain.blocks[-100:]
                if block.metadata.get("timestamp", "").split()[1].startswith(("00:", "01:", "02:", "03:", "04:", "05:"))
            )
            if night_access > 5:
                factors.append("unusual_access_hours")
            
            # æ–°ã—ã„ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
            if user_chain.baseline_behavior:
                baseline_dirs = set(user_chain.baseline_behavior.get("common_directories", []))
                recent_dirs = set()
                for block in user_chain.blocks[-50:]:
                    file_path = block.metadata.get("file_path", "")
                    if file_path:
                        directory = "\\".join(file_path.split("\\")[:-1])
                        recent_dirs.add(directory)
                
                new_dirs = recent_dirs - baseline_dirs
                if new_dirs:
                    factors.append("access_to_new_systems")
        
        # çµ„ç¹”çš„è¦å› 
        if incident.attack_type == "data_exfiltration":
            factors.append("inadequate_dlp_controls")
        
        if "cross_dept_warning" in str(chain_manager.cross_department_access):
            factors.append("weak_access_controls")
        
        return factors
    
    def _build_evidence_chain(self, timeline: Dict, chain_manager) -> List[Dict]:
        """è¨¼æ‹ ãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰"""
        evidence_chain = []
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã«è¨¼æ‹ ã‚’åé›†
        phases = timeline.get("phases", {})
        for phase_name, phase_data in phases.items():
            if phase_data:
                evidence_chain.append({
                    "phase": phase_name,
                    "timestamp": phase_data["timestamp"],
                    "block_index": phase_data["block_index"],
                    "key_evidence": self._extract_key_evidence(phase_data["evidence"])
                })
        
        return sorted(evidence_chain, key=lambda x: x["timestamp"] if x["timestamp"] else "")
    
    def _extract_key_evidence(self, evidence: Dict) -> Dict:
        """é‡è¦ãªè¨¼æ‹ ã®æŠ½å‡º"""
        return {
            "operation": evidence.get("operation"),
            "file_path": evidence.get("file_path"),
            "process": evidence.get("process_name"),
            "security_mode": evidence.get("security_mode"),
            "divergence": evidence.get("divergence_score")
        }
    
    def _calculate_confidence(self, root_cause: Dict) -> float:
        """åˆ†æã®ä¿¡é ¼åº¦è¨ˆç®—"""
        confidence = 0.0
        
        # å„è¦ç´ ã®å­˜åœ¨ã§ä¿¡é ¼åº¦ã‚’åŠ ç®—
        if root_cause["initial_vector"] and root_cause["initial_vector"] != "unknown_vector":
            confidence += 0.3
        
        if root_cause["vulnerability_type"] and root_cause["vulnerability_type"] != "unknown_vulnerability":
            confidence += 0.3
        
        if len(root_cause["contributing_factors"]) > 0:
            confidence += 0.2
        
        if len(root_cause["evidence_chain"]) > 3:
            confidence += 0.2
        
        return min(confidence, 1.0)

# =====================================
# æ¨ªå±•é–‹ãƒªã‚¹ã‚¯è©•ä¾¡ã‚¯ãƒ©ã‚¹
# =====================================

class LateralMovementRiskEvaluator:
    """æ¨ªå±•é–‹ãƒªã‚¹ã‚¯è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def evaluate(self, incident: IncidentMetadata, impact_scope: Dict, 
                chain_manager) -> Dict:
        """æ¨ªå±•é–‹ãƒªã‚¹ã‚¯ã®è©•ä¾¡"""
        risk_assessment = {
            "incident_id": incident.incident_id,
            "risk_score": 0.0,
            "affected_departments": [],
            "critical_systems_at_risk": [],
            "propagation_paths": [],
            "containment_recommendations": []
        }
        
        # ç¾åœ¨ã®å½±éŸ¿éƒ¨ç½²
        current_dept = self._get_user_department(
            incident.affected_users[0] if incident.affected_users else None,
            chain_manager
        )
        
        # ãƒªã‚¹ã‚¯ã®ã‚ã‚‹éƒ¨ç½²ã®ç‰¹å®š
        risk_assessment["affected_departments"] = self._identify_at_risk_departments(
            current_dept,
            impact_scope,
            chain_manager
        )
        
        # é‡è¦ã‚·ã‚¹ãƒ†ãƒ ã®ãƒªã‚¹ã‚¯è©•ä¾¡
        risk_assessment["critical_systems_at_risk"] = self._identify_critical_systems(
            risk_assessment["affected_departments"]
        )
        
        # ä¼æ’­çµŒè·¯ã®åˆ†æ
        risk_assessment["propagation_paths"] = self._analyze_propagation_paths(
            incident,
            chain_manager
        )
        
        # ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        risk_assessment["risk_score"] = self._calculate_risk_score(risk_assessment)
        
        # å°ã˜è¾¼ã‚æ¨å¥¨äº‹é …
        risk_assessment["containment_recommendations"] = self._generate_containment_recommendations(
            risk_assessment
        )
        
        return risk_assessment
    
    def _get_user_department(self, user_id: str, chain_manager) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éƒ¨ç½²å–å¾—"""
        return chain_manager.user_department_map.get(user_id, "unknown")
    
    def _identify_at_risk_departments(self, current_dept: str, 
                                    impact_scope: Dict, 
                                    chain_manager) -> List[str]:
        """ãƒªã‚¹ã‚¯ã®ã‚ã‚‹éƒ¨ç½²ã®ç‰¹å®š"""
        at_risk_depts = set()
        
        # ç›´æ¥å½±éŸ¿ã‚’å—ã‘ãŸéƒ¨ç½²
        at_risk_depts.add(current_dept)
        
        # æ½œåœ¨çš„å½±éŸ¿ã®éƒ¨ç½²
        at_risk_depts.update(impact_scope.get("potential_impact", {}).get("departments", []))
        
        # éƒ¨ç½²é–“ã‚¢ã‚¯ã‚»ã‚¹å±¥æ­´ã‹ã‚‰è¿½åŠ 
        for access_key in chain_manager.cross_department_access:
            parts = access_key.split("_")
            if len(parts) >= 3:
                from_dept = parts[-2]
                to_dept = parts[-1]
                if from_dept == current_dept:
                    at_risk_depts.add(to_dept)
        
        return list(at_risk_depts)
    
    def _identify_critical_systems(self, departments: List[str]) -> List[Dict]:
        """é‡è¦ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å®š"""
        critical_systems = []
        
        # éƒ¨ç½²ã¨é‡è¦ã‚·ã‚¹ãƒ†ãƒ ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        dept_critical_systems = {
            "finance": [
                {"name": "æ±ºæ¸ˆã‚·ã‚¹ãƒ†ãƒ ", "criticality": "extreme"},
                {"name": "è²¡å‹™ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ", "criticality": "high"}
            ],
            "hr": [
                {"name": "çµ¦ä¸ã‚·ã‚¹ãƒ†ãƒ ", "criticality": "extreme"},
                {"name": "äººäº‹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ", "criticality": "high"}
            ],
            "sales": [
                {"name": "é¡§å®¢ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ", "criticality": "high"},
                {"name": "å—æ³¨ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ", "criticality": "medium"}
            ],
            "engineering": [
                {"name": "ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ç®¡ç†", "criticality": "extreme"},
                {"name": "CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³", "criticality": "high"}
            ]
        }
        
        for dept in departments:
            if dept in dept_critical_systems:
                critical_systems.extend(dept_critical_systems[dept])
        
        # é‡è¤‡é™¤å»ã¨é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        unique_systems = {s["name"]: s for s in critical_systems}.values()
        criticality_order = {"extreme": 0, "high": 1, "medium": 2, "low": 3}
        
        return sorted(unique_systems, key=lambda x: criticality_order.get(x["criticality"], 999))
    
    def _analyze_propagation_paths(self, incident: IncidentMetadata, 
                                  chain_manager) -> List[Dict]:
        """ä¼æ’­çµŒè·¯ã®åˆ†æ"""
        paths = []
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã‹ã‚‰çµŒè·¯ã‚’æ¨å®š
        user_id = incident.affected_users[0] if incident.affected_users else None
        if not user_id:
            return paths
        
        # å…±æœ‰ãƒªã‚½ãƒ¼ã‚¹çµŒç”±ã®ä¼æ’­
        paths.append({
            "type": "shared_resource",
            "path": "User â†’ Shared Folder â†’ Other Users",
            "probability": 0.7,
            "speed": "fast"
        })
        
        # èªè¨¼æƒ…å ±çµŒç”±ã®ä¼æ’­
        if incident.attack_type in ["privilege_escalation", "lateral_movement"]:
            paths.append({
                "type": "credential_reuse",
                "path": "Compromised Account â†’ Domain Controller â†’ All Systems",
                "probability": 0.9,
                "speed": "very_fast"
            })
        
        # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµŒç”±ã®ä¼æ’­
        if "process" in str(incident.__dict__):
            paths.append({
                "type": "application_vulnerability",
                "path": "Infected Process â†’ Connected Services â†’ Backend Systems",
                "probability": 0.5,
                "speed": "medium"
            })
        
        return paths
    
    def _calculate_risk_score(self, risk_assessment: Dict) -> float:
        """ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        score = 0.0
        
        # å½±éŸ¿éƒ¨ç½²æ•°ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢
        dept_count = len(risk_assessment["affected_departments"])
        score += min(dept_count * 0.2, 0.4)
        
        # é‡è¦ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢
        critical_count = sum(
            1 for s in risk_assessment["critical_systems_at_risk"] 
            if s["criticality"] in ["extreme", "high"]
        )
        score += min(critical_count * 0.3, 0.6)
        
        # ä¼æ’­çµŒè·¯ã®å±é™ºåº¦
        high_prob_paths = sum(
            1 for p in risk_assessment["propagation_paths"]
            if p["probability"] > 0.7
        )
        score += min(high_prob_paths * 0.2, 0.4)
        
        return min(score, 1.0)
    
    def _generate_containment_recommendations(self, risk_assessment: Dict) -> List[Dict]:
        """å°ã˜è¾¼ã‚æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        # ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã«åŸºã¥ãæ¨å¥¨
        if risk_assessment["risk_score"] > 0.8:
            recommendations.append({
                "action": "immediate_network_isolation",
                "target": "all_affected_departments",
                "urgency": "critical",
                "description": "å½±éŸ¿éƒ¨ç½²ã®å®Œå…¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é›¢"
            })
        
        # é‡è¦ã‚·ã‚¹ãƒ†ãƒ ä¿è­·
        for system in risk_assessment["critical_systems_at_risk"]:
            if system["criticality"] == "extreme":
                recommendations.append({
                    "action": "system_protection",
                    "target": system["name"],
                    "urgency": "high",
                    "description": f"{system['name']}ã¸ã®å…¨ã‚¢ã‚¯ã‚»ã‚¹ã‚’ä¸€æ™‚é®æ–­"
                })
        
        # ä¼æ’­çµŒè·¯ã®é®æ–­
        for path in risk_assessment["propagation_paths"]:
            if path["probability"] > 0.7:
                recommendations.append({
                    "action": "block_propagation_path",
                    "target": path["type"],
                    "urgency": "high",
                    "description": f"{path['path']}ã®é®æ–­"
                })
        
        return recommendations

# =====================================
# Phase 4: å¾©æ—§æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ 
# =====================================

class RecoveryAssistanceSystem:
    """å¾©æ—§æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ï¼ˆã€œæ•°æ™‚é–“ï¼‰"""
    
    def __init__(self, chain_manager):
        self.chain_manager = chain_manager
        
    async def assist_recovery(self, incident: IncidentMetadata, 
                            investigation_results: Dict) -> Dict:
        """å¾©æ—§æ”¯æ´ã®å®Ÿè¡Œ"""
        print(f"\nğŸ”§ Phase 4: å¾©æ—§æ”¯æ´é–‹å§‹ - {incident.incident_id}")
        
        # 1. å°ã˜è¾¼ã‚ç¯„å›²ã®æœ€é©åŒ–
        optimized_containment = self._optimize_containment(
            incident,
            investigation_results
        )
        
        # 2. å¾©æ—§æ‰‹é †ã®ç”Ÿæˆ
        recovery_procedures = self._generate_recovery_procedures(
            incident,
            investigation_results,
            optimized_containment
        )
        
        # 3. å†ç™ºé˜²æ­¢ç­–ã®å®Ÿè£…ææ¡ˆ
        prevention_measures = self._design_prevention_measures(
            investigation_results["root_cause_analysis"],
            investigation_results["lateral_movement_risk"]
        )
        
        return {
            "incident_id": incident.incident_id,
            "optimized_containment": optimized_containment,
            "recovery_procedures": recovery_procedures,
            "prevention_measures": prevention_measures,
            "phase": "recovery_assistance",
            "recovery_time": datetime.now()
        }
    
    def _optimize_containment(self, incident: IncidentMetadata, 
                            investigation_results: Dict) -> Dict:
        """å°ã˜è¾¼ã‚ç¯„å›²ã®æœ€é©åŒ–"""
        optimization = {
            "current_containment": self._get_current_containment(incident),
            "recommended_adjustments": [],
            "business_impact_analysis": {}
        }
        
        # ãƒ“ã‚¸ãƒã‚¹å½±éŸ¿ã®åˆ†æ
        impact = self._analyze_business_impact(
            incident,
            optimization["current_containment"]
        )
        optimization["business_impact_analysis"] = impact
        
        # å°ã˜è¾¼ã‚ç¯„å›²ã®èª¿æ•´ææ¡ˆ
        risk_score = investigation_results["lateral_movement_risk"]["risk_score"]
        
        if risk_score < 0.3:
            # ä½ãƒªã‚¹ã‚¯ï¼šå°ã˜è¾¼ã‚ã‚’ç·©å’Œ
            optimization["recommended_adjustments"].append({
                "action": "reduce_isolation",
                "target": "non_critical_systems",
                "rationale": "æ¨ªå±•é–‹ãƒªã‚¹ã‚¯ãŒä½ã„ãŸã‚ã€æ¥­å‹™å½±éŸ¿ã‚’æœ€å°åŒ–"
            })
        elif risk_score > 0.7:
            # é«˜ãƒªã‚¹ã‚¯ï¼šå°ã˜è¾¼ã‚ã‚’å¼·åŒ–
            optimization["recommended_adjustments"].append({
                "action": "expand_isolation",
                "target": "connected_systems",
                "rationale": "æ¨ªå±•é–‹ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚ã€äºˆé˜²çš„éš”é›¢ã‚’å®Ÿæ–½"
            })
        
        return optimization
    
    def _get_current_containment(self, incident: IncidentMetadata) -> Dict:
        """ç¾åœ¨ã®å°ã˜è¾¼ã‚çŠ¶æ…‹ã‚’å–å¾—"""
        # å®Ÿéš›ã®ç’°å¢ƒã§ã¯ã€ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã‚„EDRã‹ã‚‰æƒ…å ±å–å¾—
        return {
            "isolated_users": incident.affected_users,
            "isolated_systems": incident.affected_systems,
            "blocked_processes": ["powershell.exe", "cmd.exe"],
            "network_restrictions": ["external_access_blocked"]
        }
    
    def _analyze_business_impact(self, incident: IncidentMetadata, 
                               containment: Dict) -> Dict:
        """ãƒ“ã‚¸ãƒã‚¹ã¸ã®å½±éŸ¿åˆ†æ"""
        impact = {
            "affected_business_processes": [],
            "estimated_downtime": 0,
            "revenue_impact": "unknown",
            "user_productivity_loss": 0
        }
        
        # å½±éŸ¿ã‚’å—ã‘ã‚‹ãƒ“ã‚¸ãƒã‚¹ãƒ—ãƒ­ã‚»ã‚¹ã®ç‰¹å®š
        for system in containment["isolated_systems"]:
            if "å–¶æ¥­" in system:
                impact["affected_business_processes"].append("å—æ³¨å‡¦ç†")
                impact["estimated_downtime"] += 2  # æ™‚é–“
            elif "è²¡å‹™" in system:
                impact["affected_business_processes"].append("æ”¯æ‰•å‡¦ç†")
                impact["estimated_downtime"] += 4
            elif "äººäº‹" in system:
                impact["affected_business_processes"].append("å‹¤æ€ ç®¡ç†")
                impact["estimated_downtime"] += 1
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç”Ÿç”£æ€§ã®æå¤±
        impact["user_productivity_loss"] = len(containment["isolated_users"]) * 8  # äººæ™‚
        
        return impact
    
    def _generate_recovery_procedures(self, incident: IncidentMetadata,
                                    investigation_results: Dict,
                                    optimized_containment: Dict) -> List[Dict]:
        """å¾©æ—§æ‰‹é †ã®ç”Ÿæˆ"""
        procedures = []
        
        # 1. åˆæœŸç¢ºèªæ‰‹é †
        procedures.append({
            "step": 1,
            "title": "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå°ã˜è¾¼ã‚ã®ç¢ºèª",
            "actions": [
                "å½±éŸ¿ã‚’å—ã‘ãŸã‚·ã‚¹ãƒ†ãƒ ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é›¢çŠ¶æ…‹ã‚’ç¢ºèª",
                "æ‚ªæ„ã®ã‚ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã®åœæ­¢ã‚’ç¢ºèª",
                "ãƒ­ã‚°åé›†ã®ç¶™ç¶šã‚’ç¢ºèª"
            ],
            "estimated_time": "15åˆ†",
            "responsible_team": "SOC"
        })
        
        # 2. ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        procedures.append({
            "step": 2,
            "title": "å½±éŸ¿ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—",
            "actions": [
                "ãƒãƒ«ã‚¦ã‚§ã‚¢ã‚¹ã‚­ãƒ£ãƒ³ã®å®Ÿè¡Œ",
                "ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®å‰Šé™¤",
                "æ­£è¦ã®è¨­å®šã¸ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"
            ],
            "estimated_time": "1æ™‚é–“",
            "responsible_team": "ITé‹ç”¨"
        })
        
        # 3. ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ
        if incident.attack_type in ["privilege_escalation", "lateral_movement"]:
            procedures.append({
                "step": 3,
                "title": "èªè¨¼æƒ…å ±ã®ãƒªã‚»ãƒƒãƒˆ",
                "actions": [
                    "å½±éŸ¿ã‚’å—ã‘ãŸã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒªã‚»ãƒƒãƒˆ",
                    "é–¢é€£ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ç¢ºèªã¨æ›´æ–°",
                    "MFAï¼ˆå¤šè¦ç´ èªè¨¼ï¼‰ã®å†è¨­å®š"
                ],
                "estimated_time": "30åˆ†",
                "responsible_team": "IDç®¡ç†"
            })
        
        # 4. æ®µéšçš„å¾©æ—§
        procedures.append({
            "step": 4,
            "title": "ã‚·ã‚¹ãƒ†ãƒ ã®æ®µéšçš„å¾©æ—§",
            "actions": [
                "é‡è¦åº¦ã®ä½ã„ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰é †æ¬¡æ¥ç¶šã‚’å›å¾©",
                "å„æ®µéšã§ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£è¦–ã®å¼·åŒ–",
                "ç•°å¸¸æ¤œçŸ¥æ™‚ã®å³æ™‚ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æº–å‚™"
            ],
            "estimated_time": "2æ™‚é–“",
            "responsible_team": "ITé‹ç”¨"
        })
        
        # 5. æ¤œè¨¼ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
        procedures.append({
            "step": 5,
            "title": "å¾©æ—§å¾Œã®æ¤œè¨¼",
            "actions": [
                "å…¨ã‚·ã‚¹ãƒ†ãƒ ã®æ­£å¸¸å‹•ä½œç¢ºèª",
                "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°ã®è©³ç´°ãƒ¬ãƒ“ãƒ¥ãƒ¼",
                "24æ™‚é–“ã®å¼·åŒ–ç›£è¦–ä½“åˆ¶"
            ],
            "estimated_time": "ç¶™ç¶šçš„",
            "responsible_team": "SOC"
        })
        
        return procedures
    
    def _design_prevention_measures(self, root_cause: Dict, 
                                  lateral_risk: Dict) -> List[Dict]:
        """å†ç™ºé˜²æ­¢ç­–ã®è¨­è¨ˆ"""
        measures = []
        
        # æ ¹æœ¬åŸå› ã«å¯¾ã™ã‚‹å¯¾ç­–
        vulnerability = root_cause.get("vulnerability_type")
        
        if vulnerability == "weak_authentication":
            measures.append({
                "measure": "èªè¨¼å¼·åŒ–",
                "actions": [
                    "å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®MFAç¾©å‹™åŒ–",
                    "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒãƒªã‚·ãƒ¼ã®å¼·åŒ–",
                    "å®šæœŸçš„ãªãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å¤‰æ›´ã®å®Ÿæ–½"
                ],
                "priority": "high",
                "implementation_time": "1é€±é–“"
            })
        
        elif vulnerability == "unrestricted_script_execution":
            measures.append({
                "measure": "ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œåˆ¶å¾¡",
                "actions": [
                    "PowerShellå®Ÿè¡Œãƒãƒªã‚·ãƒ¼ã®åˆ¶é™",
                    "AppLockerã«ã‚ˆã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆ¶å¾¡",
                    "ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã®ç›£æŸ»ãƒ­ã‚°å¼·åŒ–"
                ],
                "priority": "high",
                "implementation_time": "3æ—¥"
            })
        
        # æ¨ªå±•é–‹ãƒªã‚¹ã‚¯ã«å¯¾ã™ã‚‹å¯¾ç­–
        if lateral_risk.get("risk_score", 0) > 0.5:
            measures.append({
                "measure": "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³",
                "actions": [
                    "éƒ¨ç½²é–“ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è«–ç†åˆ†é›¢",
                    "æœ€å°æ¨©é™ã®åŸå‰‡ã®å¾¹åº•",
                    "ç‰¹æ¨©ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®åˆ©ç”¨åˆ¶é™"
                ],
                "priority": "critical",
                "implementation_time": "2é€±é–“"
            })
        
        # æ¤œçŸ¥èƒ½åŠ›ã®å‘ä¸Š
        measures.append({
            "measure": "æ¤œçŸ¥èƒ½åŠ›ã®å¼·åŒ–",
            "actions": [
                "EDRã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã®å°å…¥ãƒ»æ›´æ–°",
                "SIEMãƒ«ãƒ¼ãƒ«ã®è¿½åŠ ãƒ»èª¿æ•´",
                "è„…å¨ãƒãƒ³ãƒ†ã‚£ãƒ³ã‚°ã®å®šæœŸå®Ÿæ–½"
            ],
            "priority": "medium",
            "implementation_time": "1ãƒ¶æœˆ"
        })
        
        # æ•™è‚²ãƒ»è¨“ç·´
        measures.append({
            "measure": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ„è­˜å‘ä¸Š",
            "actions": [
                "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäº‹ä¾‹ã®å…±æœ‰",
                "æ¨™çš„å‹æ”»æ’ƒå¯¾å¿œè¨“ç·´ã®å®Ÿæ–½",
                "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ•™è‚²ã®å¼·åŒ–"
            ],
            "priority": "medium",
            "implementation_time": "ç¶™ç¶šçš„"
        })
        
        return measures

# =====================================
# å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ 
# =====================================

class IncidentVisualizationSystem:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.colors = {
            "normal": "#4CAF50",
            "investigating": "#FFC107",
            "suspicious": "#FF9800",
            "critical": "#F44336"
        }
    
    def create_timeline_visualization(self, timeline_data: Dict) -> go.Figure:
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®ä½œæˆ"""
        fig = make_subplots(
            rows=2, cols=1,
            row_heights=[0.7, 0.3],
            subplot_titles=("æ”»æ’ƒã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³", "ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«æ¨ç§»")
        )
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
        phases = timeline_data.get("phases", {})
        events = timeline_data.get("events", [])
        
        # ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ—ãƒ­ãƒƒãƒˆ
        phase_times = []
        phase_names = []
        phase_colors = []
        
        for phase_name, phase_data in phases.items():
            if phase_data:
                phase_times.append(phase_data["timestamp"])
                phase_names.append(phase_name)
                phase_colors.append(self._get_phase_color(phase_name))
        
        fig.add_trace(
            go.Scatter(
                x=phase_times,
                y=[1] * len(phase_times),
                mode='markers+text',
                name='æ”»æ’ƒãƒ•ã‚§ãƒ¼ã‚º',
                text=phase_names,
                textposition="top center",
                marker=dict(size=20, color=phase_colors),
                showlegend=True
            ),
            row=1, col=1
        )
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã®ãƒ—ãƒ­ãƒƒãƒˆ
        event_times = [e["timestamp"] for e in events if e.get("timestamp")]
        event_severities = [e["severity"] for e in events if e.get("severity")]
        event_colors = [self.colors.get(s, "#999999") for s in event_severities]
        
        fig.add_trace(
            go.Scatter(
                x=event_times,
                y=[0.5] * len(event_times),
                mode='markers',
                name='ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆ',
                marker=dict(size=10, color=event_colors),
                showlegend=True
            ),
            row=1, col=1
        )
        
        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«æ¨ç§»
        divergences = [e.get("divergence", 0) for e in events]
        
        fig.add_trace(
            go.Scatter(
                x=event_times,
                y=divergences,
                mode='lines+markers',
                name='Divergenceã‚¹ã‚³ã‚¢',
                line=dict(color='red', width=2),
                showlegend=True
            ),
            row=2, col=1
        )
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
        fig.update_layout(
            title="ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åˆ†æ",
            height=800,
            showlegend=True
        )
        
        fig.update_xaxes(title_text="æ™‚åˆ»", row=2, col=1)
        fig.update_yaxes(title_text="ãƒ¬ãƒ™ãƒ«", range=[0, 1.5], row=1, col=1)
        fig.update_yaxes(title_text="ã‚¹ã‚³ã‚¢", row=2, col=1)
        
        return fig
    
    def create_impact_heatmap(self, impact_data: Dict, time_range: int = 24) -> plt.Figure:
        """éƒ¨ç½²åˆ¥ãƒªã‚¹ã‚¯ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä½œæˆ"""
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿéš›ã¯chain_managerã‹ã‚‰å–å¾—ï¼‰
        departments = ["å–¶æ¥­éƒ¨", "çµŒç†éƒ¨", "é–‹ç™ºéƒ¨", "äººäº‹éƒ¨"]
        hours = list(range(time_range))
        
        # ãƒªã‚¹ã‚¯ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã®ç”Ÿæˆ
        risk_matrix = np.random.rand(len(departments), len(hours))
        
        # å½±éŸ¿ãƒ‡ãƒ¼ã‚¿ã®åæ˜ 
        affected_depts = impact_data.get("affected_departments", [])
        for i, dept in enumerate(departments):
            if dept in affected_depts:
                risk_matrix[i, :] *= 2  # å½±éŸ¿ã‚’å—ã‘ãŸéƒ¨ç½²ã®ãƒªã‚¹ã‚¯ã‚’ä¸Šã’ã‚‹
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä½œæˆ
        fig, ax = plt.subplots(figsize=(15, 8))
        
        im = ax.imshow(risk_matrix, cmap='RdYlGn_r', aspect='auto')
        
        # è»¸ã®è¨­å®š
        ax.set_xticks(np.arange(len(hours)))
        ax.set_yticks(np.arange(len(departments)))
        ax.set_xticklabels([f"{h:02d}:00" for h in hours])
        ax.set_yticklabels(departments)
        
        # ã‚°ãƒªãƒƒãƒ‰ã®è¿½åŠ 
        ax.set_xticks(np.arange(len(hours)+1)-.5, minor=True)
        ax.set_yticks(np.arange(len(departments)+1)-.5, minor=True)
        ax.grid(which="minor", color="w", linestyle='-', linewidth=3)
        ax.tick_params(which="minor", size=0)
        
        # ã‚«ãƒ©ãƒ¼ãƒãƒ¼ã®è¿½åŠ 
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label('ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«', rotation=270, labelpad=20)
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        plt.title('éƒ¨ç½²åˆ¥ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆ24æ™‚é–“ï¼‰', fontsize=16, pad=20)
        plt.xlabel('æ™‚åˆ»', fontsize=12)
        plt.ylabel('éƒ¨ç½²', fontsize=12)
        
        plt.tight_layout()
        return fig
    
    def create_attack_path_3d(self, incident_data: Dict) -> go.Figure:
        """3Dæ”»æ’ƒçµŒè·¯ã®å¯è¦–åŒ–"""
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã®ä½œæˆ
        G = nx.Graph()
        
        # ãƒãƒ¼ãƒ‰ã®è¿½åŠ ï¼ˆå½±éŸ¿ã‚’å—ã‘ãŸã‚·ã‚¹ãƒ†ãƒ ï¼‰
        affected_systems = incident_data.get("affected_systems", [])
        for i, system in enumerate(affected_systems):
            G.add_node(system, pos=(i, i, 0), node_type="system")
        
        # æ”»æ’ƒè€…ãƒãƒ¼ãƒ‰
        attacker = incident_data.get("attacker", "Attacker")
        G.add_node(attacker, pos=(-1, -1, 1), node_type="attacker")
        
        # ã‚¨ãƒƒã‚¸ã®è¿½åŠ ï¼ˆæ”»æ’ƒçµŒè·¯ï¼‰
        attack_paths = incident_data.get("attack_paths", [])
        for path in attack_paths:
            if len(path) >= 2:
                for i in range(len(path)-1):
                    G.add_edge(path[i], path[i+1])
        
        # 3Dåº§æ¨™ã®ç”Ÿæˆ
        pos = nx.spring_layout(G, dim=3, seed=42)
        
        # Plotlyã§ã®3Då¯è¦–åŒ–
        edge_trace = []
        for edge in G.edges():
            x0, y0, z0 = pos[edge[0]]
            x1, y1, z1 = pos[edge[1]]
            edge_trace.append(go.Scatter3d(
                x=[x0, x1, None],
                y=[y0, y1, None],
                z=[z0, z1, None],
                mode='lines',
                line=dict(width=4, color='red'),
                hoverinfo='none'
            ))
        
        # ãƒãƒ¼ãƒ‰ã®ãƒ—ãƒ­ãƒƒãƒˆ
        node_trace = go.Scatter3d(
            x=[],
            y=[],
            z=[],
            mode='markers+text',
            name='Systems',
            marker=dict(
                size=20,
                color=[],
                colorscale='Viridis',
                line=dict(width=2)
            ),
            text=[],
            textposition="top center",
            hoverinfo='text'
        )
        
        for node in G.nodes():
            x, y, z = pos[node]
            node_trace['x'] += tuple([x])
            node_trace['y'] += tuple([y])
            node_trace['z'] += tuple([z])
            node_trace['text'] += tuple([node])
            
            # ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹è‰²åˆ†ã‘
            if G.nodes[node].get('node_type') == 'attacker':
                node_trace['marker']['color'] += tuple(['red'])
            else:
                node_trace['marker']['color'] += tuple(['blue'])
        
        # å›³ã®ä½œæˆ
        fig = go.Figure(data=edge_trace + [node_trace])
        
        fig.update_layout(
            title="3Dæ”»æ’ƒçµŒè·¯ãƒãƒƒãƒ—",
            showlegend=False,
            scene=dict(
                xaxis=dict(showgrid=False, zeroline=False, visible=False),
                yaxis=dict(showgrid=False, zeroline=False, visible=False),
                zaxis=dict(showgrid=False, zeroline=False, visible=False)
            ),
            margin=dict(b=20, l=5, r=5, t=40),
            height=600
        )
        
        return fig
    
    def _get_phase_color(self, phase: str) -> str:
        """ãƒ•ã‚§ãƒ¼ã‚ºã«å¿œã˜ãŸè‰²ã®å–å¾—"""
        phase_colors = {
            "reconnaissance": "#2196F3",
            "initial_access": "#9C27B0",
            "privilege_escalation": "#F44336",
            "lateral_movement": "#FF9800",
            "data_collection": "#FFC107",
            "data_exfiltration": "#FF5722",
            "cleanup": "#795548"
        }
        return phase_colors.get(phase, "#999999")

# =====================================
# ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ 
# =====================================

class ForensicAssistanceSystem:
    """ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, chain_manager):
        self.chain_manager = chain_manager
        self.evidence_store = {}
        
    async def preserve_evidence(self, incident: IncidentMetadata) -> Dict:
        """è¨¼æ‹ ä¿å…¨ã®å®Ÿè¡Œ"""
        evidence_package = {
            "incident_id": incident.incident_id,
            "preservation_time": datetime.now(),
            "evidence_items": []
        }
        
        # 1. æ®ç™ºæ€§ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        volatile_data = await self._capture_volatile_data(incident)
        evidence_package["evidence_items"].append(volatile_data)
        
        # 2. ãƒã‚§ãƒ¼ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã®ä¿å­˜
        chain_evidence = self._export_chain_blocks(incident)
        evidence_package["evidence_items"].append(chain_evidence)
        
        # 3. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        log_evidence = await self._collect_logs(incident)
        evidence_package["evidence_items"].append(log_evidence)
        
        # 4. ãƒãƒƒã‚·ãƒ¥å€¤ã®è¨ˆç®—ã¨ä¿å­˜
        evidence_hash = self._calculate_package_hash(evidence_package)
        evidence_package["hash"] = evidence_hash
        
        # 5. æ”¹ç«„é˜²æ­¢ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ä½œæˆ
        sealed_package = self._create_forensic_container(evidence_package)
        
        return sealed_package
    
    async def _capture_volatile_data(self, incident: IncidentMetadata) -> ForensicEvidence:
        """æ®ç™ºæ€§ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒ—ãƒãƒ£"""
        volatile_data = {
            "capture_time": datetime.now().isoformat(),
            "active_connections": self._get_active_connections(),
            "running_processes": self._get_running_processes(),
            "memory_snapshot": self._capture_memory_snapshot(),
            "affected_users": incident.affected_users,
            "affected_systems": incident.affected_systems
        }
        
        return ForensicEvidence(
            evidence_id=f"VOLATILE-{incident.incident_id}",
            incident_id=incident.incident_id,
            timestamp=datetime.now(),
            evidence_type="volatile",
            data=volatile_data
        )
    
    def _export_chain_blocks(self, incident: IncidentMetadata) -> ForensicEvidence:
        """é–¢é€£ã™ã‚‹ãƒã‚§ãƒ¼ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        blocks_data = []
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒã‚§ãƒ¼ãƒ³ã‹ã‚‰åé›†
        for user_id in incident.affected_users:
            if user_id in self.chain_manager.user_chains:
                user_chain = self.chain_manager.user_chains[user_id]
                for block in user_chain.blocks[-100:]:  # æœ€æ–°100ãƒ–ãƒ­ãƒƒã‚¯
                    blocks_data.append({
                        "chain_type": "user",
                        "user_id": user_id,
                        "block_index": block.index,
                        "hash": block.hash,
                        "timestamp": block.metadata.get("timestamp"),
                        "security_mode": block.metadata.get("security_mode"),
                        "divergence": block.divergence,
                        "metadata": self._sanitize_metadata(block.metadata)
                    })
        
        # éƒ¨ç½²ãƒã‚§ãƒ¼ãƒ³ã‹ã‚‰åé›†
        for dept_name, chain in self.chain_manager.department_chains.items():
            relevant_blocks = [
                block for block in chain.blocks[-200:]
                if any(user in str(block.metadata) for user in incident.affected_users)
            ]
            
            for block in relevant_blocks:
                blocks_data.append({
                    "chain_type": "department",
                    "department": dept_name,
                    "block_index": block.index,
                    "hash": block.hash,
                    "timestamp": block.metadata.get("timestamp"),
                    "security_mode": block.metadata.get("security_mode"),
                    "divergence": block.divergence,
                    "metadata": self._sanitize_metadata(block.metadata)
                })
        
        return ForensicEvidence(
            evidence_id=f"BLOCKS-{incident.incident_id}",
            incident_id=incident.incident_id,
            timestamp=datetime.now(),
            evidence_type="blockchain",
            data={"blocks": blocks_data, "total_blocks": len(blocks_data)}
        )
    
    async def _collect_logs(self, incident: IncidentMetadata) -> ForensicEvidence:
        """é–¢é€£ãƒ­ã‚°ã®åé›†"""
        logs = {
            "security_logs": [],
            "application_logs": [],
            "system_logs": [],
            "network_logs": []
        }
        
        # ã‚¿ã‚¤ãƒ ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®è¨­å®šï¼ˆã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œçŸ¥ã®å‰å¾Œ1æ™‚é–“ï¼‰
        start_time = incident.detection_time - timedelta(hours=1)
        end_time = incident.detection_time + timedelta(hours=1)
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°ã®åé›†ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        logs["security_logs"] = [
            {
                "timestamp": (start_time + timedelta(minutes=i*5)).isoformat(),
                "event_id": f"SEC-{i}",
                "severity": "high" if i % 3 == 0 else "medium",
                "description": f"Security event related to {incident.affected_users[0] if incident.affected_users else 'unknown'}"
            }
            for i in range(24)
        ]
        
        return ForensicEvidence(
            evidence_id=f"LOGS-{incident.incident_id}",
            incident_id=incident.incident_id,
            timestamp=datetime.now(),
            evidence_type="logs",
            data=logs
        )
    
    def _create_forensic_container(self, evidence_package: Dict) -> Dict:
        """æ”¹ç«„é˜²æ­¢ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯ã‚³ãƒ³ãƒ†ãƒŠã®ä½œæˆ"""
        container = {
            "format_version": "1.0",
            "created_at": datetime.now().isoformat(),
            "evidence_package": evidence_package,
            "integrity": {
                "hash_algorithm": "SHA256",
                "package_hash": evidence_package.get("hash", ""),
                "signature": self._create_digital_signature(evidence_package)
            },
            "chain_of_custody": [
                {
                    "timestamp": datetime.now().isoformat(),
                    "action": "evidence_collected",
                    "performed_by": "ForensicAssistanceSystem",
                    "hash": evidence_package.get("hash", "")
                }
            ]
        }
        
        return container
    
    def _sanitize_metadata(self, metadata: Dict) -> Dict:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆæ©Ÿå¯†æƒ…å ±ã®é™¤å»ï¼‰"""
        sanitized = metadata.copy()
        
        # æ©Ÿå¯†ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®é™¤å»ã¾ãŸã¯åŒ¿ååŒ–
        sensitive_fields = ["password", "token", "secret", "key"]
        for field in sensitive_fields:
            if field in sanitized:
                sanitized[field] = "[REDACTED]"
        
        return sanitized
    
    def _calculate_package_hash(self, package: Dict) -> str:
        """è¨¼æ‹ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        # è¾æ›¸ã‚’å®‰å®šã—ãŸJSONæ–‡å­—åˆ—ã«å¤‰æ›
        package_str = json.dumps(package, sort_keys=True, default=str)
        return hashlib.sha256(package_str.encode()).hexdigest()
    
    def _create_digital_signature(self, data: Dict) -> str:
        """ãƒ‡ã‚¸ã‚¿ãƒ«ç½²åã®ä½œæˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        # å®Ÿéš›ã®ç’°å¢ƒã§ã¯ç§˜å¯†éµã‚’ä½¿ç”¨ã—ã¦ç½²å
        data_str = json.dumps(data, sort_keys=True, default=str)
        return hashlib.sha512(data_str.encode()).hexdigest()
    
    def _get_active_connections(self) -> List[Dict]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ¥ç¶šæƒ…å ±ã®å–å¾—ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        return [
            {
                "local_address": "192.168.1.100:445",
                "remote_address": "192.168.1.200:50123",
                "state": "ESTABLISHED",
                "process": "svchost.exe"
            }
        ]
    
    def _get_running_processes(self) -> List[Dict]:
        """å®Ÿè¡Œä¸­ãƒ—ãƒ­ã‚»ã‚¹ã®å–å¾—ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        return [
            {
                "pid": 1234,
                "name": "powershell.exe",
                "user": "attacker",
                "cpu_percent": 45.2,
                "memory_mb": 128
            }
        ]
    
    def _capture_memory_snapshot(self) -> Dict:
        """ãƒ¡ãƒ¢ãƒªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®å–å¾—ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        return {
            "snapshot_id": f"MEM-{datetime.now().strftime('%Y%m%d%H%M%S')}",
            "size_mb": 8192,
            "capture_method": "live_capture",
            "compressed": True
        }

# =====================================
# ç›¸é–¢åˆ†æã‚·ã‚¹ãƒ†ãƒ 
# =====================================

class CorrelationAnalysisSystem:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç›¸é–¢åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, chain_manager):
        self.chain_manager = chain_manager
        
    def analyze_correlations(self, incident: IncidentMetadata) -> Dict:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®ç›¸é–¢åˆ†æ"""
        correlations = {
            "incident_id": incident.incident_id,
            "time_correlation": self._analyze_time_correlation(incident),
            "user_correlation": self._analyze_user_correlation(incident),
            "technique_correlation": self._analyze_technique_correlation(incident),
            "summary": {}
        }
        
        # ç›¸é–¢ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
        correlations["summary"] = self._generate_correlation_summary(correlations)
        
        return correlations
    
    def _analyze_time_correlation(self, incident: IncidentMetadata) -> Dict:
        """æ™‚é–“çš„ç›¸é–¢ã®åˆ†æ"""
        time_correlation = {
            "concurrent_incidents": [],
            "historical_patterns": [],
            "periodic_pattern": None
        }
        
        # åŒæ™‚æœŸã®ä»–ã®ç•°å¸¸ã‚’æ¤œå‡º
        detection_window = timedelta(hours=1)
        start_time = incident.detection_time - detection_window
        end_time = incident.detection_time + detection_window
        
        # å…¨éƒ¨ç½²ã®ãƒã‚§ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        for dept_name, chain in self.chain_manager.department_chains.items():
            for block in chain.blocks:
                block_time_str = block.metadata.get("timestamp", "")
                if block_time_str:
                    try:
                        block_time = datetime.strptime(block_time_str, "%Y-%m-%d %H:%M:%S")
                        if start_time <= block_time <= end_time:
                            if block.metadata.get("security_mode") in ["suspicious", "critical"]:
                                time_correlation["concurrent_incidents"].append({
                                    "department": dept_name,
                                    "user": block.metadata.get("user_id"),
                                    "timestamp": block_time_str,
                                    "severity": block.metadata.get("security_mode")
                                })
                    except:
                        continue
        
        # éå»ã®é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        time_correlation["historical_patterns"] = self._find_historical_patterns(incident)
        
        # å‘¨æœŸæ€§ã®æ¤œå‡º
        time_correlation["periodic_pattern"] = self._detect_periodicity(incident)
        
        return time_correlation
    
    def _analyze_user_correlation(self, incident: IncidentMetadata) -> Dict:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ç›¸é–¢ã®åˆ†æ"""
        user_correlation = {
            "same_ip_users": [],
            "behavior_similarity": [],
            "access_pattern_match": []
        }
        
        if not incident.affected_users:
            return user_correlation
        
        primary_user = incident.affected_users[0]
        
        # åŒä¸€IPã‹ã‚‰ã®ä»–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œç´¢
        if primary_user in self.chain_manager.user_chains:
            primary_chain = self.chain_manager.user_chains[primary_user]
            
            # æœ€æ–°ã®IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å–å¾—
            recent_ips = set()
            for block in primary_chain.blocks[-20:]:
                ip = block.metadata.get("source_ip")
                if ip:
                    recent_ips.add(ip)
            
            # ä»–ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒã‚§ãƒ¼ãƒ³ã§åŒã˜IPã‚’ãƒã‚§ãƒƒã‚¯
            for user_id, user_chain in self.chain_manager.user_chains.items():
                if user_id != primary_user:
                    for block in user_chain.blocks[-20:]:
                        if block.metadata.get("source_ip") in recent_ips:
                            user_correlation["same_ip_users"].append({
                                "user_id": user_id,
                                "shared_ip": block.metadata.get("source_ip"),
                                "timestamp": block.metadata.get("timestamp")
                            })
                            break
        
        # è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¡ä¼¼æ€§åˆ†æ
        user_correlation["behavior_similarity"] = self._calculate_behavior_similarity(
            primary_user
        )
        
        return user_correlation
    
    def _analyze_technique_correlation(self, incident: IncidentMetadata) -> Dict:
        """æ”»æ’ƒæ‰‹æ³•ã®ç›¸é–¢åˆ†æ"""
        technique_correlation = {
            "known_apt_match": [],
            "internal_threat_score": 0.0,
            "technique_combinations": []
        }
        
        # æ—¢çŸ¥ã®APTãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®ç…§åˆ
        apt_patterns = {
            "APT28": {
                "techniques": ["powershell", "lateral_movement", "data_exfiltration"],
                "confidence": 0.0
            },
            "APT29": {
                "techniques": ["privilege_escalation", "stealth", "long_term_access"],
                "confidence": 0.0
            }
        }
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®ç‰¹å¾´æŠ½å‡º
        incident_features = self._extract_incident_features(incident)
        
        # APTãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®ãƒãƒƒãƒãƒ³ã‚°
        for apt_name, apt_data in apt_patterns.items():
            match_score = self._calculate_technique_match(
                incident_features,
                apt_data["techniques"]
            )
            if match_score > 0.3:
                technique_correlation["known_apt_match"].append({
                    "apt_group": apt_name,
                    "confidence": match_score,
                    "matched_techniques": apt_data["techniques"]
                })
        
        # å†…éƒ¨è„…å¨ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        technique_correlation["internal_threat_score"] = self._calculate_internal_threat_score(
            incident,
            incident_features
        )
        
        return technique_correlation
    
    def _find_historical_patterns(self, incident: IncidentMetadata) -> List[Dict]:
        """éå»ã®é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œç´¢"""
        patterns = []
        
        # éå»30æ—¥é–“ã®é¡ä¼¼ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã‚’æ¤œç´¢
        lookback_days = 30
        lookback_time = incident.detection_time - timedelta(days=lookback_days)
        
        # ç°¡æ˜“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        for dept_name, chain in self.chain_manager.department_chains.items():
            similar_blocks = []
            
            for block in chain.blocks:
                if block.metadata.get("security_mode") == incident.severity:
                    if block.metadata.get("attack_type") == incident.attack_type:
                        similar_blocks.append(block)
            
            if similar_blocks:
                patterns.append({
                    "department": dept_name,
                    "occurrence_count": len(similar_blocks),
                    "first_seen": similar_blocks[0].metadata.get("timestamp"),
                    "last_seen": similar_blocks[-1].metadata.get("timestamp")
                })
        
        return patterns
    
    def _detect_periodicity(self, incident: IncidentMetadata) -> Optional[Dict]:
        """å‘¨æœŸæ€§ã®æ¤œå‡º"""
        # åŒã˜æ”»æ’ƒã‚¿ã‚¤ãƒ—ã®ç™ºç”Ÿæ™‚åˆ»ã‚’åé›†
        occurrence_times = []
        
        for dept_name, chain in self.chain_manager.department_chains.items():
            for block in chain.blocks:
                if block.metadata.get("attack_type") == incident.attack_type:
                    timestamp_str = block.metadata.get("timestamp")
                    if timestamp_str:
                        try:
                            timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
                            occurrence_times.append(timestamp)
                        except:
                            continue
        
        if len(occurrence_times) < 3:
            return None
        
        # æ™‚é–“é–“éš”ã®è¨ˆç®—
        occurrence_times.sort()
        intervals = []
        for i in range(1, len(occurrence_times)):
            interval = (occurrence_times[i] - occurrence_times[i-1]).total_seconds() / 3600
            intervals.append(interval)
        
        # å‘¨æœŸæ€§ã®åˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if intervals:
            avg_interval = sum(intervals) / len(intervals)
            std_interval = np.std(intervals)
            
            if std_interval < avg_interval * 0.3:  # å¤‰å‹•ãŒ30%ä»¥å†…
                return {
                    "detected": True,
                    "average_interval_hours": avg_interval,
                    "confidence": 1 - (std_interval / avg_interval)
                }
        
        return {"detected": False}
    
    def _calculate_behavior_similarity(self, target_user: str) -> List[Dict]:
        """è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¡ä¼¼æ€§è¨ˆç®—"""
        similarities = []
        
        if target_user not in self.chain_manager.user_chains:
            return similarities
        
        target_chain = self.chain_manager.user_chains[target_user]
        target_baseline = target_chain.baseline_behavior
        
        # ä»–ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®æ¯”è¼ƒ
        for user_id, user_chain in self.chain_manager.user_chains.items():
            if user_id != target_user:
                similarity_score = self._compare_baselines(
                    target_baseline,
                    user_chain.baseline_behavior
                )
                
                if similarity_score > 0.6:
                    similarities.append({
                        "user_id": user_id,
                        "similarity_score": similarity_score,
                        "matching_patterns": self._get_matching_patterns(
                            target_baseline,
                            user_chain.baseline_behavior
                        )
                    })
        
        return sorted(similarities, key=lambda x: x["similarity_score"], reverse=True)
    
    def _extract_incident_features(self, incident: IncidentMetadata) -> List[str]:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®ç‰¹å¾´æŠ½å‡º"""
        features = []
        
        # æ”»æ’ƒã‚¿ã‚¤ãƒ—ã‹ã‚‰ç‰¹å¾´æŠ½å‡º
        if incident.attack_type == "privilege_escalation":
            features.extend(["powershell", "admin_tools"])
        elif incident.attack_type == "lateral_movement":
            features.extend(["lateral_movement", "network_discovery"])
        elif incident.attack_type == "data_exfiltration":
            features.extend(["data_exfiltration", "large_transfer"])
        
        return features
    
    def _calculate_technique_match(self, incident_features: List[str], 
                                 apt_techniques: List[str]) -> float:
        """æŠ€è¡“çš„ç‰¹å¾´ã®ãƒãƒƒãƒãƒ³ã‚°ç‡è¨ˆç®—"""
        if not apt_techniques:
            return 0.0
        
        matches = sum(1 for tech in apt_techniques if tech in incident_features)
        return matches / len(apt_techniques)
    
    def _calculate_internal_threat_score(self, incident: IncidentMetadata, 
                                       features: List[str]) -> float:
        """å†…éƒ¨è„…å¨ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        score = 0.0
        
        # å†…éƒ¨è„…å¨ã®æŒ‡æ¨™
        if incident.affected_users:
            user_id = incident.affected_users[0]
            if user_id in self.chain_manager.user_chains:
                user_chain = self.chain_manager.user_chains[user_id]
                
                # é•·æœŸé–“ã®æ­£å¸¸ã‚¢ã‚¯ã‚»ã‚¹å¾Œã®ç•°å¸¸
                if len(user_chain.blocks) > 100:
                    normal_blocks = sum(
                        1 for block in user_chain.blocks[:80]
                        if block.metadata.get("security_mode") == "normal"
                    )
                    if normal_blocks > 70:
                        score += 0.3
                
                # é€šå¸¸ã®æ¥­å‹™æ™‚é–“å†…ã®ã‚¢ã‚¯ã‚»ã‚¹
                if hasattr(user_chain, "baseline_behavior"):
                    typical_hours = user_chain.baseline_behavior.get("typical_hours", [])
                    if 9 <= incident.detection_time.hour <= 17:
                        score += 0.2
                
                # æ—¢çŸ¥ã®å†…éƒ¨ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
                if "lateral_movement" in features:
                    score += 0.3
        
        return min(score, 1.0)
    
    def _compare_baselines(self, baseline1: Dict, baseline2: Dict) -> float:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¡Œå‹•ã®æ¯”è¼ƒ"""
        if not baseline1 or not baseline2:
            return 0.0
        
        similarity = 0.0
        comparison_count = 0
        
        # æ´»å‹•æ™‚é–“å¸¯ã®æ¯”è¼ƒ
        hours1 = set(baseline1.get("typical_hours", []))
        hours2 = set(baseline2.get("typical_hours", []))
        if hours1 and hours2:
            similarity += len(hours1 & hours2) / len(hours1 | hours2)
            comparison_count += 1
        
        # ã‚¢ã‚¯ã‚»ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¯”è¼ƒ
        dirs1 = set(baseline1.get("common_directories", []))
        dirs2 = set(baseline2.get("common_directories", []))
        if dirs1 and dirs2:
            similarity += len(dirs1 & dirs2) / len(dirs1 | dirs2)
            comparison_count += 1
        
        # ä½¿ç”¨ãƒ—ãƒ­ã‚»ã‚¹ã®æ¯”è¼ƒ
        procs1 = set(baseline1.get("common_processes", []))
        procs2 = set(baseline2.get("common_processes", []))
        if procs1 and procs2:
            similarity += len(procs1 & procs2) / len(procs1 | procs2)
            comparison_count += 1
        
        return similarity / comparison_count if comparison_count > 0 else 0.0
    
    def _get_matching_patterns(self, baseline1: Dict, baseline2: Dict) -> List[str]:
        """ä¸€è‡´ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å–å¾—"""
        patterns = []
        
        # å…±é€šã®æ´»å‹•æ™‚é–“
        hours1 = set(baseline1.get("typical_hours", []))
        hours2 = set(baseline2.get("typical_hours", []))
        common_hours = hours1 & hours2
        if common_hours:
            patterns.append(f"æ´»å‹•æ™‚é–“å¸¯: {sorted(common_hours)}")
        
        # å…±é€šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        dirs1 = set(baseline1.get("common_directories", []))
        dirs2 = set(baseline2.get("common_directories", []))
        common_dirs = dirs1 & dirs2
        if common_dirs:
            patterns.append(f"ã‚¢ã‚¯ã‚»ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {len(common_dirs)}ç®‡æ‰€")
        
        return patterns
    
    def _generate_correlation_summary(self, correlations: Dict) -> Dict:
        """ç›¸é–¢åˆ†æã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        summary = {
            "risk_level": "low",
            "key_findings": [],
            "recommendations": []
        }
        
        # æ™‚é–“ç›¸é–¢ã®è©•ä¾¡
        concurrent_count = len(correlations["time_correlation"]["concurrent_incidents"])
        if concurrent_count > 3:
            summary["risk_level"] = "high"
            summary["key_findings"].append(
                f"åŒæ™‚æœŸã«{concurrent_count}ä»¶ã®ç•°å¸¸ã‚’æ¤œå‡º - çµ„ç¹”çš„æ”»æ’ƒã®å¯èƒ½æ€§"
            )
            summary["recommendations"].append(
                "å…¨ç¤¾çš„ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£è¦–ã®å¼·åŒ–"
            )
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç›¸é–¢ã®è©•ä¾¡
        if correlations["user_correlation"]["same_ip_users"]:
            summary["key_findings"].append(
                "è¤‡æ•°ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåŒä¸€IPã‚’ä½¿ç”¨ - ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä¾µå®³ã®å¯èƒ½æ€§"
            )
            summary["recommendations"].append(
                "å½±éŸ¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èªè¨¼æƒ…å ±ãƒªã‚»ãƒƒãƒˆ"
            )
        
        # æŠ€è¡“ç›¸é–¢ã®è©•ä¾¡
        if correlations["technique_correlation"]["known_apt_match"]:
            apt_match = correlations["technique_correlation"]["known_apt_match"][0]
            summary["risk_level"] = "critical"
            summary["key_findings"].append(
                f"{apt_match['apt_group']}ã¨ã®æ‰‹æ³•ä¸€è‡´ç‡: {apt_match['confidence']:.0%}"
            )
            summary["recommendations"].append(
                "è„…å¨ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹ã«åŸºã¥ãå¯¾ç­–ã®å®Ÿæ–½"
            )
        
        return summary

# =====================================
# çµ±åˆã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œã‚·ã‚¹ãƒ†ãƒ 
# =====================================

class IntegratedIncidentResponseSystem:
    """çµ±åˆã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, chain_manager):
        self.chain_manager = chain_manager
        self.immediate_response = ImmediateResponseSystem(chain_manager)
        self.initial_analysis = InitialAnalysisSystem(chain_manager)
        self.detailed_investigation = DetailedInvestigationSystem(chain_manager)
        self.recovery_assistance = RecoveryAssistanceSystem(chain_manager)
        self.forensic_assistance = ForensicAssistanceSystem(chain_manager)
        self.correlation_analysis = CorrelationAnalysisSystem(chain_manager)
        self.visualization = IncidentVisualizationSystem()
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå±¥æ­´
        self.incident_history = {}
        
    async def respond_to_incident(self, event: Dict, detection_result: Dict) -> Dict:
        """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œã®çµ±åˆå®Ÿè¡Œ"""
        print(f"\n{'='*60}")
        print(f"ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œé–‹å§‹")
        print(f"{'='*60}")
        
        response_timeline = []
        
        # Phase 1: å³æ™‚å¯¾å¿œï¼ˆã€œ30ç§’ï¼‰
        phase1_start = datetime.now()
        phase1_result = await self.immediate_response.process_incident(
            event, 
            detection_result
        )
        response_timeline.append({
            "phase": "immediate_response",
            "duration": (datetime.now() - phase1_start).total_seconds(),
            "result": phase1_result
        })
        
        incident = phase1_result["incident"]
        
        # Phase 2: åˆå‹•åˆ†æï¼ˆã€œ5åˆ†ï¼‰
        phase2_start = datetime.now()
        phase2_result = await self.initial_analysis.analyze_incident(
            incident,
            phase1_result["evidence_task"]
        )
        response_timeline.append({
            "phase": "initial_analysis",
            "duration": (datetime.now() - phase2_start).total_seconds(),
            "result": phase2_result
        })
        
        # Phase 3: è©³ç´°èª¿æŸ»ï¼ˆã€œ30åˆ†ï¼‰
        phase3_start = datetime.now()
        phase3_result = await self.detailed_investigation.investigate_incident(
            incident,
            phase2_result
        )
        response_timeline.append({
            "phase": "detailed_investigation",
            "duration": (datetime.now() - phase3_start).total_seconds(),
            "result": phase3_result
        })
        
        # Phase 4: å¾©æ—§æ”¯æ´ï¼ˆã€œæ•°æ™‚é–“ï¼‰
        phase4_start = datetime.now()
        phase4_result = await self.recovery_assistance.assist_recovery(
            incident,
            phase3_result
        )
        response_timeline.append({
            "phase": "recovery_assistance",
            "duration": (datetime.now() - phase4_start).total_seconds(),
            "result": phase4_result
        })
        
        # è¿½åŠ åˆ†æ
        correlation_result = self.correlation_analysis.analyze_correlations(incident)
        forensic_result = await self.forensic_assistance.preserve_evidence(incident)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        final_report = self._generate_incident_report(
            incident,
            response_timeline,
            correlation_result,
            forensic_result
        )
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå±¥æ­´ã«ä¿å­˜
        self.incident_history[incident.incident_id] = final_report
        
        # å¯è¦–åŒ–
        self._create_visualizations(final_report)
        
        return final_report
    
    def _generate_incident_report(self, incident: IncidentMetadata,
                                response_timeline: List[Dict],
                                correlation_result: Dict,
                                forensic_result: Dict) -> Dict:
        """æœ€çµ‚ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report = {
            "incident_summary": {
                "id": incident.incident_id,
                "detection_time": incident.detection_time.isoformat(),
                "severity": incident.severity,
                "attack_type": incident.attack_type,
                "status": incident.status,
                "affected_scope": {
                    "users": incident.affected_users,
                    "systems": incident.affected_systems
                }
            },
            "response_timeline": response_timeline,
            "analysis_results": {
                "root_cause": response_timeline[2]["result"]["root_cause_analysis"],
                "lateral_risk": response_timeline[2]["result"]["lateral_movement_risk"],
                "correlations": correlation_result
            },
            "mitigation_actions": {
                "immediate_containment": response_timeline[0]["result"]["containment_actions"],
                "recovery_procedures": response_timeline[3]["result"]["recovery_procedures"],
                "prevention_measures": response_timeline[3]["result"]["prevention_measures"]
            },
            "forensic_evidence": {
                "evidence_id": forensic_result.get("evidence_package", {}).get("incident_id"),
                "preservation_time": forensic_result.get("created_at"),
                "hash": forensic_result.get("integrity", {}).get("package_hash")
            },
            "total_response_time": sum(phase["duration"] for phase in response_timeline),
            "report_generated": datetime.now().isoformat()
        }
        
        return report
    
    def _create_visualizations(self, report: Dict):
        """ãƒ¬ãƒãƒ¼ãƒˆã®å¯è¦–åŒ–"""
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–
        timeline_data = report["analysis_results"]["root_cause"].get("evidence_chain", [])
        if timeline_data:
            timeline_viz = self.visualization.create_timeline_visualization({
                "phases": {item["phase"]: item for item in timeline_data},
                "events": []
            })
            # timeline_viz.show()  # å®Ÿéš›ã®ç’°å¢ƒã§ã¯è¡¨ç¤º
        
        # ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        impact_data = report["incident_summary"]["affected_scope"]
        heatmap = self.visualization.create_impact_heatmap(impact_data)
        # plt.show()  # å®Ÿéš›ã®ç’°å¢ƒã§ã¯è¡¨ç¤º
        
        print("\nğŸ“Š å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ")

# =====================================
# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
# =====================================

async def test_incident_response():
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    
    # ãƒ€ãƒŸãƒ¼ã®chain_managerã‚’ä½œæˆ
    class DummyChainManager:
        def __init__(self):
            self.department_chains = {}
            self.user_chains = {}
            self.user_department_map = {"suzuki_m": "sales"}
            self.cross_department_access = {}
    
    chain_manager = DummyChainManager()
    
    # ãƒ†ã‚¹ãƒˆã‚¤ãƒ™ãƒ³ãƒˆï¼ˆæ”»æ’ƒã‚·ãƒŠãƒªã‚ªï¼‰
    test_event = {
        "timestamp": "2025-03-14 10:14:00",
        "user_id": "suzuki_m",
        "department": "sales",
        "operation": "FileCopy",
        "file_path": "\\\\fileserver\\sales\\confidential\\customer_list.xlsx",
        "file_size_kb": 45000,
        "process_name": "powershell.exe",
        "destination_ip": "203.0.113.50",
        "source_ip": "192.168.1.120",
        "status": "SUCCESS"
    }
    
    # æ¤œçŸ¥çµæœï¼ˆchain_managerã‹ã‚‰ã®å‡ºåŠ›ã‚’æƒ³å®šï¼‰
    detection_result = {
        "status": "critical",
        "divergence": 45.8,
        "alert_level": "HIGH",
        "cross_dept_warning": True
    }
    
    # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    response_system = IntegratedIncidentResponseSystem(chain_manager)
    
    # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œã®å®Ÿè¡Œ
    response = await response_system.respond_to_incident(test_event, detection_result)
    
    # çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\n{'='*60}")
    print("ğŸ“‹ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œå®Œäº†")
    print(f"{'='*60}")
    print(f"ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆID: {response['incident_summary']['id']}")
    print(f"æ·±åˆ»åº¦: {response['incident_summary']['severity']}")
    print(f"æ”»æ’ƒã‚¿ã‚¤ãƒ—: {response['incident_summary']['attack_type']}")
    print(f"ç·å¯¾å¿œæ™‚é–“: {response['total_response_time']:.2f}ç§’")
    print(f"\næ ¹æœ¬åŸå› : {response['analysis_results']['root_cause']['vulnerability_type']}")
    print(f"æ¨ªå±•é–‹ãƒªã‚¹ã‚¯: {response['analysis_results']['lateral_risk']['risk_score']:.2f}")
    
    # æ¨å¥¨å¯¾ç­–ã®è¡¨ç¤º
    print("\nğŸ›¡ï¸ æ¨å¥¨å¯¾ç­–:")
    for measure in response['mitigation_actions']['prevention_measures']:
        print(f"  - {measure['measure']} (å„ªå…ˆåº¦: {measure['priority']})")
    
    return response

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    # éåŒæœŸå®Ÿè¡Œ
    asyncio.run(test_incident_response())
