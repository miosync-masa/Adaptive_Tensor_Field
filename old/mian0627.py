# -------------------------------
# ğŸŒ NumPyï¼ˆBLAS/LAPACKï¼‰Code
# -------------------------------

# ğŸŒ æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import os
import sys
import json
import time
import random
import pickle
import gzip
import hashlib

from pathlib import Path
from collections import deque, OrderedDict, Counter,defaultdict
from datetime import datetime, timedelta

from functools import lru_cache
from multiprocessing import Pool, Queue
from typing import Tuple, Dict, Any, Optional
import asyncio
import concurrent.futures

# ğŸ§® æ•°å€¤ãƒ»ãƒ‡ãƒ¼ã‚¿å‡¦ç†
import math
import numpy as np
import pandas as pd
from numba import njit, prange

# åˆ¥åã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆdatetimeè¡çªæ™‚ã®ã¿!ï¼‰
from datetime import datetime as dtmod

# ğŸ§ª æ©Ÿæ¢°å­¦ç¿’/çµ±è¨ˆç³»
from cuml.cluster import KMeans
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.preprocessing import StandardScaler

# ğŸ› ï¸ è¨­å®šãƒ»å‹å®‰å…¨
from dataclasses import dataclass, field

# -------------------------------
# ğŸ§¬configé–¢æ•°
# -------------------------------
@dataclass
class ChainTuneConfig:

    # === ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ ===
    enable_scalable_backend: bool = True
    enable_user_scalable_backend: bool = True

    # === EMA/é‡ã¿ä¿‚æ•°ï¼ˆçµ„ç¹”ç”¨ãƒ»å€‹äººç”¨ï¼‰ ===
    ema_alpha_org:  float = 0.3        # çµ„ç¹”EMAå¹³æ»‘åŒ–ä¿‚æ•°
    ema_alpha_user: float = 0.6        # å€‹äººEMAå¹³æ»‘åŒ–ä¿‚æ•°
    alpha_org: float = 1.2             # çµ„ç¹”ï¼šnormal_div é‡ã¿
    beta_org: float = 0.7              # çµ„ç¹”ï¼šseverity_levelé‡ã¿
    gamma_org: float = 0.5             # çµ„ç¹”ï¼šaction_magnitudeé‡ã¿
    alpha_user: float = 1.3            # å€‹äººï¼šnormal_div é‡ã¿
    beta_user: float = 0.9             # å€‹äººï¼šseverity_levelé‡ã¿
    gamma_user: float = 0.8            # å€‹äººï¼šaction_magnitudeé‡ã¿

    # === åˆ¤å®šã—ãã„å€¤ï¼ˆsuspicious, investigating, criticalï¼‰ ===
    threshold_suspicious_org: float = 15.0      # çµ„ç¹”ï¼šsuspiciousåˆ¤å®šã—ãã„å€¤
    threshold_suspicious_user: float = 20.0     # å€‹äººï¼šsuspiciousåˆ¤å®šã—ãã„å€¤
    threshold_investigating_org: float = 8.0   # çµ„ç¹”ï¼šinvestigatingåˆ¤å®šã—ãã„å€¤ï¼ˆâ€»è¿½åŠ ï¼‰
    threshold_investigating_user: float = 10.0  # å€‹äººï¼šinvestigatingåˆ¤å®šã—ãã„å€¤ï¼ˆâ€»è¿½åŠ ï¼‰
    trust_error_threshold_org: float = 0.5      # çµ„ç¹”ï¼šä¿¡é ¼ã‚¹ã‚³ã‚¢ã§critical
    trust_error_threshold_user: float = 0.65    # å€‹äººï¼šä¿¡é ¼ã‚¹ã‚³ã‚¢ã§critical
    threshold_div_suspicious : float = 14.0
    threshold_div_crittcal : float = 25.0

    # === DivergenceÃ—ä¿¡é ¼ã‚¹ã‚³ã‚¢ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹è¿½åŠ åˆ¤å®š ===
    trust_score_investigating_1: float = 0.65   # investigatingåˆ¤å®šãã®1ï¼ˆä¿¡é ¼ã‚¹ã‚³ã‚¢ï¼‰
    trust_score_investigating_2: float = 0.70   # investigatingåˆ¤å®šãã®2ï¼ˆä¿¡é ¼ã‚¹ã‚³ã‚¢ï¼‰
    normal_div_investigating_1: float = 10.0    # investigatingåˆ¤å®šãã®1ï¼ˆnormal_divï¼‰
    normal_div_investigating_2: float = 8.0     # investigatingåˆ¤å®šãã®2ï¼ˆnormal_divï¼‰

    # === ãƒ•ã‚©ãƒ¼ã‚¯ãƒ»åŒæœŸé–¢é€£ã—ãã„å€¤ ===
    fork_threshold_user: float = 12.0           # å€‹äººï¼šãƒã‚§ãƒ¼ãƒ³åˆ†å²é–¾å€¤
    fork_threshold_org: float = 15.0            # çµ„ç¹”ï¼šãƒã‚§ãƒ¼ãƒ³åˆ†å²é–¾å€¤

    # ğŸŒŸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
    cache_size:        int = 10_000
    max_memory_mb:     int = 1_000
    block_history_min: int = 60
    block_history_hr:  int = 24
    block_history_day: int = 30

    # ğŸŒŸ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    target_sample_rate: float = 0.10
    high_load_thresh:   float = 0.80

    # ğŸŒŸ éƒ¨ç½²ã”ã¨ã®ã—ãã„å€¤
    dept_threshold_table: dict = field(default_factory=lambda: {
        "sales":       {"morning": 12.0, "afternoon": 10.0, "evening": 11.0},
        "engineering": {"morning": 18.0, "afternoon": 10.0, "evening": 20.0},
        "finance":     {"morning": 12.0, "afternoon":  9.0, "evening": 11.0},
        "hr":          {"morning": 14.0, "afternoon":  9.0, "evening": 12.0},
        "executive":   {"morning": 15.0, "afternoon": 12.0, "evening": 18.0}
    })

    # ğŸŒŸ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ä¸€å…ƒç®¡ç†
    security_chain_filepath:  str = "realistic_security_logs.json"
    error_chain_filepath:     str = "attack_only_logs.json"
    suspicious_events_file:   str = "chain_attackments.json"
    zero_day_events_file:     str = "attackments.json"
    normal_test_events_file:  str = "normal_test_events.json"
    attack_patterns_file:     str = "attack_patterns.json"


    # ğŸŒŸ æ„å‘³ãƒ†ãƒ³ã‚½ãƒ«ã®é‡ã¿ä¿‚æ•°ï¼ˆL1/L2ï¼‰
    weights_l1: dict = field(default_factory=lambda: {
        "severity_level": 1.0,
        "action_magnitude": 0.5,
        "trust_score": 1.0,
        "threat_context": 0.7,
        "event_score": 0.4,
        "security_mode": 0.6,
        "is_confidential_access": 2.5,
        "is_cross_dept_access": 2.0,
        "is_external_network": 2.5,
        "permission_level_diff": 1.5,
        "operation_success_flag": 1.8
    })
    weights_l2: dict = field(default_factory=lambda: {
        "severity_level": 0.7,
        "action_magnitude": 0.3,
        "trust_score": 0.9,
        "threat_context": 1.0,
        "event_score": 0.7,
        "security_mode": 0.8,
        "is_confidential_access": 2.5,
        "is_cross_dept_access": 2.0,
        "is_external_network": 2.5,
        "permission_level_diff": 1.5,
        "operation_success_flag": 1.8
    })

    def to_org_dict(self):
        return {
            "alpha": self.alpha_org,
            "beta": self.beta_org,
            "gamma": self.gamma_org,
            "ema_alpha": self.ema_alpha_org,
            "threshold_suspicious": self.threshold_suspicious_org,
            "trust_error_threshold": self.trust_error_threshold_org,
        }

    def to_user_dict(self):
        return {
            "alpha": self.alpha_user,
            "beta": self.beta_user,
            "gamma": self.gamma_user,
            "ema_alpha": self.ema_alpha_user,
            "threshold_suspicious": self.threshold_suspicious_user,
            "trust_error_threshold": self.trust_error_threshold_user,
        }

    # ğŸŒŸ Divergence
    divergence_max: float = 120.0

    # ğŸŒŸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¢ãƒ¼ãƒ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    security_mode_mapping: dict = field(default_factory=lambda: {
        "normal": 0.0,
        "suspicious": 1.0,
        "critical": 2.0,
        "investigating": 1.5,
        "latent_suspicious": 1.2  # â†è¿½åŠ ï¼
    })

    reverse_security_mode_mapping: dict = field(init=False)

    def __post_init__(self):
        # REVERSE_SECURITY_MODE_MAPPINGã®è‡ªå‹•ç”Ÿæˆ
        self.reverse_security_mode_mapping = {v: k for k, v in self.security_mode_mapping.items()}

# ===== security_constants.py =====

# --- 1. æ“ä½œã”ã¨ã®åŸºæœ¬ã‚¹ã‚³ã‚¢ ---
OPERATION_SCORES = {
    "FileRead": 15, "FileWrite": 30, "FileCopy": 60, "FileDelete": 50,
    "FileMove": 35, "ProcessCreate": 40, "ProcessTerminate": 20,
    "NetworkConnect": 35, "NetworkListen": 30, "Login": 10,
    "LoginFailed": 40, "Logout": 2
}

# --- 2. ãƒ‘ã‚¹ç¨®åˆ¥â†’åŠ ç‚¹å€¤ ---
PATH_SCORES = {
    "high": [
        ("\\system32\\", 50), ("\\syswow64\\", 50), ("\\admin\\", 50),
        ("\\config\\", 50), ("\\windows\\", 50), ("\\program files\\", 50), ("\\programdata\\", 50),
    ],
    "sensitive": [
        ("\\finance\\", 45), ("\\hr\\", 35), ("\\payroll\\", 35), ("\\çµŒç†\\", 45), ("\\äººäº‹\\", 35),
        ("\\çµ¦ä¸\\", 35), ("\\å½¹å“¡\\", 45), ("\\executive\\", 45), ("\\confidential\\", 50),
    ],
    "normal": [
        ("\\sales\\", 12), ("\\å–¶æ¥­\\", 12), ("\\é¡§å®¢\\", 12), ("\\customer\\", 12),
        ("\\å¥‘ç´„\\", 12), ("\\contract\\", 12), ("\\source\\", 12),
        ("\\backup\\", 18), ("\\audit\\", 18), ("\\log\\", 18), ("\\archive\\", 18),
    ]
}

# --- 3. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºé–¾å€¤ ---
FILE_SIZE_THRESHOLDS = [
    (1000, 60),
    (500, 50),
    (300, 40),
    (100, 35),
    (50, 15)
]

# --- 4. ãƒ—ãƒ­ã‚»ã‚¹åŠ ç‚¹ ---
PROCESS_SCORES = {
    "dangerous": [("powershell", 90), ("cmd", 85), ("wmic", 70), ("psexec", 70), ("net.exe", 70), ("reg.exe", 70)],
    "admin": [("mmc.exe", 20), ("regedit", 20), ("services.exe", 20), ("taskmgr", 20)],
    "archiver": [("7z", 25), ("winrar", 25), ("winzip", 25), ("gpg", 25), ("truecrypt", 25)],
}

# --- 5. æ™‚é–“å¸¯ãƒ»é€±æœ« ---
TIME_PENALTY_SCORES = [
    ((0, 5), 30),
    ((5, 7), 20),
    ((22, 24), 25),
    ((12, 13), -5),
]
TIME_ANOMALY_SCORES = [
    ((9, 18), 0.0),
    ((7, 9), 0.2),
    ((18, 20), 0.2),
    ((6, 7), 0.4),
    ((20, 22), 0.4),
    ((5, 6), 0.6),
    ((22, 23), 0.6),
    ((0, 5), 0.9),   # æ·±å¤œå¸¯
]
WEEKEND_BONUS = 0.3
MONDAY_MORNING = (0, 6, 8, 0.8)
FRIDAY_NIGHT = (4, 19, 22, 0.8)
BUSINESS_HOURS = {"start": 6, "end": 18}
HOURS_SINCE_ACCESS = 720

# --- 6. é‡å¤§åº¦ãƒ¬ãƒ™ãƒ« ---
SEVERITY_LEVEL_THRESHOLDS = [
    (85, 1.0),
    (70, 0.9),
    (50, 0.7),
    (30, 0.5),
    (15, 0.3),
    (0,  0.1),
]

# --- 7. ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ› ---
ACTION_OPERATION_VECTORS = {
    "Login": np.array([1.0, 0.0, 0.0]),
    "Logout": np.array([0.0, 0.0, 1.0]),
    "LoginFailed": np.array([3.0, 0.0, 0.0]),
    "FileRead": np.array([0.0, 1.0, 0.0]),
    "NetworkConnect": np.array([0.0, 2.0, 0.0]),
    "NetworkListen": np.array([0.0, 1.0, 0.0]),
    "ProcessCreate": np.array([0.0, 0.8, 0.2]),
    "FileWrite": np.array([0.0, 0.0, 1.0]),
    "FileDelete": np.array([0.0, 0.0, 1.0]),
    "FileCopy": np.array([0.0, 0.3, 0.7]),
    "FileMove": np.array([0.0, 0.0, 1.0]),
    "ProcessTerminate": np.array([0.0, 0.0, 1.0]),
}
ACTION_DEFAULT_VECTOR = np.array([0.33, 0.33, 0.34])

def operation_type_vector(operation):
    return ACTION_OPERATION_VECTORS.get(operation, ACTION_DEFAULT_VECTOR)

# --- 8. éƒ¨ç½²Ã—ãƒªã‚½ãƒ¼ã‚¹ ---
DEPT_RESOURCE_SCORES = {
    ("sales", ("\\source\\", "\\dev\\", "\\git\\")): (15, -0.1),
    ("engineering", ("\\finance\\", "\\accounting\\")): (15, -0.1),
}

# --- 9. å¤–éƒ¨IPã‚¹ã‚³ã‚¢ ---
EXTERNAL_IP_SCORES = {
    "internal": 20,
    "external": 60,
}

TRUSTED_HOSTNAMES = [
    "crm.company.com", "bank.example.com", "intranet.company.com"
]
TRUSTED_IPS = [
    "192.168.1.0/24", "172.16.0.0/12"
]

# --- 10. ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆIP, ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ ---
PRIVATE_IP_PREFIXES = ("192.168.", "10.", "172.")
FILE_TYPE_MAPPING = {
    "office": (".docx", ".xlsx", ".pdf", ".pptx"),
    "exec": (".exe", ".bat", ".cmd", ".msi"),
}

# --- 11. æ¥­å‹™æ“ä½œã‚»ãƒƒãƒˆ ---
BUSINESS_OPERATIONS = {
    "Login", "Logout", "FileRead", "FileWrite", "ProcessCreate", "NetworkConnect"
}

# --- 12. æ©Ÿå¯†ãƒ•ã‚©ãƒ«ãƒ€æ¨©é™éƒ¨ç½² ---
CONFIDENTIAL_ACCESS_DEPTS = ["engineering", "executive"]

# --- 13. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‡ã¿ ---
THREAT_CONTEXT_WEIGHTS = {
    "time_anomaly": 0.3,
    "outside_active_hours": 0.1,
    "external_network": 0.4,
    "gateway_access": 0.2,
    "dangerous_process": 0.3,
    "unknown_process": 0.1,
    "operation_failed": 0.2,
    "system_path_access": 0.2,
    "max_threat": 1.0,
}

# --- 14. ä¿¡é ¼åº¦ãƒ­ã‚¸ãƒƒã‚¯ ---
TRUST_SCORES = {
    "operation_penalty": {
        "FileRead": 0.0,
        "FileWrite": 0.0,
        "FileDelete": -0.02,
        "FileCopy": -0.02,
        "ProcessCreate": -0.01,
        "LoginFailed": -0.4,
    },
    "first_access_penalty": 0.1,
    "recency_divisor": 8000.0,
    "recency_penalty_cap": 0.3,
    "internal_ip_penalty": 0.15,
    "external_ip_penalty": 0.35,
    "time_penalty": 0.1,
    "min_trust": 0.3,
    "max_trust": 1.0,
    "business_context_year_end_bonus": 0.1,
    "business_context_cross_dept_bonus": 0.2,
}

# --- 15. åˆ¤å®šé–¾å€¤ ---
THRESHOLDS = {
    "failed_attempts_critical": 3,
    "large_file_kb": 100_000,
    "abnormal_access_count": 100,
    "pattern_count_threshold": 8,
    "repeat_count_threshold": 12,
    "anomaly_score_threshold": 3.0,
    "normal_div_threshold": 6.5,
    "trust_score_threshold": 0.80,
    "trust_score_high": 0.8,
    "trust_score_medium": 0.7,
    "trust_score_low": 0.72,
    "normal_div_high": 8.5,
    "pattern_accumulation_limit": 8
}

# --- 16. éƒ¨ç½²ã¨æ©Ÿå¯†ãƒ»æ¥­å‹™ãƒ‘ã‚¹ ---
DEPT_PATH_RULES = {
    "sales": {
        # ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ãƒ‘ã‚¹
        "restricted": ["\\finance\\", "\\hr\\", "\\payroll\\", "\\accounting\\", "\\bank\\"],
        "patterns": ["\\sales\\", "\\å–¶æ¥­\\", "\\crm\\", "\\customer\\", "\\proposal\\"],  # â† è¿½åŠ åˆ†
        "message": "å–¶æ¥­ãŒçµŒç†/äººäº‹ãƒªã‚½ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹"
    },
    "engineering": {
        "restricted": ["\\payroll\\", "\\hr\\", "\\finance\\"],
        "patterns": ["\\dev\\", "\\source\\", "\\repos\\", "\\é–‹ç™º\\", "\\git\\"],
        "message": "ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒçµŒç†ãƒ»äººäº‹ãƒªã‚½ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹"
    },
    "finance": {
        "restricted": ["\\source\\", "\\dev\\", "\\engineering\\", "\\ci_cd_pipeline\\"],
        "patterns": ["\\finance\\", "\\çµŒç†\\", "\\accounting\\", "\\è²¡å‹™\\", "\\budget\\"],
        "message": "çµŒç†ãŒé–‹ç™ºãƒªã‚½ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹"
    },
    "hr": {
        "restricted": ["\\source\\", "\\dev\\", "\\engineering\\", "\\finance\\"],
        "patterns": ["\\hr\\", "\\äººäº‹\\", "\\payroll\\", "\\employee\\", "\\recruitment\\"],
        "message": "äººäº‹ãŒé–‹ç™ºãƒ»çµŒç†ãƒªã‚½ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹"
    },
    # å¿…è¦ãªã‚‰ executive ã‚‚è¿½åŠ 
}

# --- 17. ãƒ¢ãƒ¼ãƒ‰åˆ¤å®šç”¨ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚³ã‚¢é–¾å€¤ ---
MODE_THRESHOLDS = {
    "critical": 40,
    "suspicious": 20,
    "investigating": 10,
}

# --- 18. é«˜ãƒªã‚¹ã‚¯ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ ---
HIGH_RISK_PATHS = [
    "\\admin\\", "\\administrator\\", "\\config\\",
    "\\audit\\", "\\backup\\", "\\system32\\"
]

# --- 20. éƒ¨ç½²ã”ã¨ã®IPãƒ¬ãƒ³ã‚¸ ---
DEPT_IP_RANGES = {
    "sales": "192.168.1.",
    "engineering": "192.168.2.",
    "finance": "192.168.3.",
    "hr": "192.168.4."
}

# --- 21. ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹/ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é–¢é€£å®šæ•°ï¼ˆè¿½åŠ åˆ†ï¼‰ ---

# é€¸è„±ã‚¹ã‚³ã‚¢ã®è¨­å®š
DEVIATION_SCORES = {
    "hour_diff_divisor": 12.0,
    "hour_weight": 0.2,
    "unknown_directory": 0.3,
    "high_risk_directory": 0.3,
    "abnormal_file_size": 0.2,
    "unknown_process": 0.2,
    "dangerous_process": 0.3,
    "new_operation": 0.2,
    "rare_operation": 0.1,
    "unknown_internal_ip": 0.1,
    "unknown_external_ip": 0.3,
    "max_score": 1.0
}

FREQUENT_DIRECTORY_ACCESS_THRESHOLD = 10  # ãã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒ10å›ä»¥ä¸Šã‚¢ã‚¯ã‚»ã‚¹ã•ã‚ŒãŸã‚‰â€œé »ç¹â€ã¨è¦‹ãªã™
OPERATION_FREQUENCY_THRESHOLD = 0.01 # æ“ä½œé »åº¦ã®é–¾å€¤
RECENT_ACCESS_HOURS = 24 # æœ€è¿‘ã®ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“ï¼ˆæ™‚é–“ï¼‰
MIN_ACCESS_COUNT = 10  # ã‚¢ã‚¯ã‚»ã‚¹é »åº¦åˆ¤å®šã®é–¾å€¤ï¼ˆä¾‹å€¤ã€‚è¦èª¿æ•´ï¼‰
NORMAL_ACCESS_GRACE_PERIOD = 24  # æ­£å¸¸ã‚¢ã‚¯ã‚»ã‚¹çŒ¶äºˆæ™‚é–“ï¼ˆhï¼‰
HIGH_FREQUENCY_OPERATION_RATIO = 0.2 # é«˜é »åº¦æ“ä½œã®æ¯”ç‡
HIGH_ALERT_MODES = ["critical", "suspicious", "latent_suspicious"]

# Divergenceèª¿æ•´ä¿‚æ•°
DIVERGENCE_MULTIPLIERS = {
    "recent_normal_access": 0.7,
    "frequent_user": 0.8,
    "frequent_directory": 0.8,
    "safe_process": 0.9
}

# ãƒ“ã‚¸ãƒã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿‚æ•°
BUSINESS_CONTEXT_MULTIPLIERS = {
    "year_end": 0.7,
    "quarter_end": 0.7,
    "cross_dept_allowed": 0.6,
    "emergency": 0.5,
    "maintenance": 0.6
}

# Divergenceèª¿æ•´ä¿‚æ•°ï¼ˆå€‹äººç”¨ï¼‰
DIVERGENCE_MULTIPLIERS.update({
    "personal_frequent_directory": 0.7,
    "recent_access": 0.85,
    "trusted_user_process": 0.8,
    "frequent_operation": 0.75
})

THRESHOLDS["default_adaptive"] = 8.0 # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé©å¿œé–¾å€¤

# é‡è¦åº¦é †åºãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆè©•ä¾¡ã‚„ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒè¡¨ç¤ºç”¨ï¼‰
SEVERITY_ORDER = {
    "normal": 0,
    "investigating": 1,
    "suspicious": 2,
    "critical": 3
}

CRITICAL_STATUSES = ["suspicious", "critical"] # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«æ‰±ã„ã¨ãªã‚‹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¾¤

# å®‰å…¨ã¨ã¿ãªã™ãƒ—ãƒ­ã‚»ã‚¹åï¼ˆãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆï¼‰
SAFE_PROCESSES = [
    "excel.exe", "winword.exe", "word.exe", "outlook.exe",
    "chrome.exe", "firefox.exe", "docker.exe", "Adobe.exe", "git.exe"
]

# åˆæœŸå­¦ç¿’ã®é–¾å€¤
INITIAL_LEARNING_THRESHOLD = 100
INITIAL_CLUSTER_COUNT = 10

global_chain_tune_config = ChainTuneConfig()
CONFIG = ChainTuneConfig()
THRESHOLD_SUSPICIOUS_USER = CONFIG.threshold_suspicious_user
THRESHOLD_INVESTIGATING_USER = CONFIG.threshold_investigating_user
TRUST_ERROR_THRESHOLD_USER = CONFIG.trust_error_threshold_user

THRESHOLD_SUSPICIOUS_ORG = CONFIG.threshold_suspicious_org
THRESHOLD_INVESTIGATING_ORG = CONFIG.threshold_investigating_org
TRUST_ERROR_THRESHOLD_ORG = CONFIG.trust_error_threshold_org

TRUST_SCORE_INVESTIGATING_1 = CONFIG.trust_score_investigating_1
TRUST_SCORE_INVESTIGATING_2 = CONFIG.trust_score_investigating_2
NORMAL_DIV_INVESTIGATING_1 = CONFIG.normal_div_investigating_1
NORMAL_DIV_INVESTIGATING_2 = CONFIG.normal_div_investigating_2
THRESHOLD_DIV_SUSPICIOUS = CONFIG.threshold_div_suspicious
THRESHOLD_DIV_CRITTCAL = CONFIG.threshold_div_crittcal

# -------------------------------
# ğŸ§¬ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# -------------------------------
HIGH_RISK_PATHS = [p for (p, _) in PATH_SCORES["high"]]
SENSITIVE_PATHS = [p for (p, _) in PATH_SCORES["sensitive"]]

def safe_values(v):
    """å€¤ã‚’NumPyé…åˆ—ã«å¤‰æ›ï¼ˆJITå¯¾å¿œï¼‰"""
    if isinstance(v, dict):
        return np.array(list(v.values()), dtype=np.float32)
    elif isinstance(v, np.ndarray):
        return v.astype(np.float32)
    else:
        return np.array(list(v), dtype=np.float32)

def vectorize_state(state_params):
    # å¿…è¦ãªæ„å‘³ç‰¹å¾´é‡ã‚­ãƒ¼ã®ãƒªã‚¹ãƒˆ
    FEATURE_KEYS = [
        "severity_level",
        "action_magnitude",
        "threat_context",
        "trust_score",
        "security_mode",
        "event_score",
        "is_confidential_access",
        "is_cross_dept_access",
        "is_external_network",
        "permission_level_diff",
        "operation_success_flag"
    ]
    vec = []
    for k in FEATURE_KEYS:
        v = state_params.get(k, 0)
        try:
            vec.append(float(v))
        except Exception:
            vec.append(0.0)
    return np.array(vec, dtype=float)

def is_global_ip(ip: str) -> bool:
    if not ip:
        return False  # Noneã‚„ç©ºæ–‡å­—ã¯ãƒ­ãƒ¼ã‚«ãƒ«æ‰±ã„ã§å®‰å…¨
    return not ip.startswith(PRIVATE_IP_PREFIXES)

def get_filetype(file_path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
    ext = os.path.splitext(file_path)[1].lower()
    for file_type, extensions in FILE_TYPE_MAPPING.items():
        if ext in extensions:
            return file_type
    return "other"

def get_hour_from_timestamp(timestamp: str) -> Optional[int]:
    try:
        # "2025-03-14 08:15:00" â†’ "08"
        return int(timestamp.split()[1].split(":")[0])
    except Exception:
        return None

def is_business_hours(hour: Optional[int]) -> bool:
    """æ¥­å‹™æ™‚é–“å†…ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    if hour is None:
        return True  # æ™‚é–“ä¸æ˜ã®å ´åˆã¯æ¥­å‹™æ™‚é–“ã¨ã¿ãªã™
    return BUSINESS_HOURS["start"] <= hour <= BUSINESS_HOURS["end"]

def assess_path_risk(file_path):
    """ãƒ‘ã‚¹ã®ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š"""
    if not file_path:  # Noneã‚„""ã§ã‚‚å•é¡Œãªã
        return "normal", 0
    path_lower = file_path.lower()
    if any(p in path_lower for p in HIGH_RISK_PATHS):
        return "high", 50
    elif any(p in path_lower for p in SENSITIVE_PATHS):
        return "sensitive", 35
    return "normal", 0

def create_state_dict(
    severity: int,
    action_mag: float,
    threat: str,
    trust: float,
    mode: str,
    score: float,
    extra: dict = None
) -> dict:
    """çŠ¶æ…‹è¾æ›¸ã‚’ä½œæˆã™ã‚‹å…±é€šé–¢æ•°"""
    state = {
        "severity_level": severity,
        "action_magnitude": action_mag,
        "threat_context": threat,
        "trust_score": trust,
        "security_mode": mode,
        "event_score": score,
    }
    if extra:
        state.update(extra)
    return state
# -----------------
# ğŸ” Security Log Data Handler
# -----------------
class SecurityLogData:
    """
    å¼·åŒ–ç‰ˆ SecurityLogData: çµ„ç¹”æ§‹é€ ãƒ»æŒ¯ã‚‹èˆã„ãƒ»è¨±å®¹ãƒªã‚½ãƒ¼ã‚¹ã‚’ç–‘ä¼¼çš„ã«ç®¡ç†
    """
    def __init__(self):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®ã€Œæ‰€å±ãƒ»è¨±å®¹IPãƒ¬ãƒ³ã‚¸ã€å®šç¾©ï¼ˆappsã¯çœç•¥ï¼‰
        self.user_profiles = {
            "yamada_t":    {"dept": "sales",       "ips": ["192.168.1.100", "192.168.1.101"]},
            "suzuki_m":    {"dept": "sales",       "ips": ["192.168.1.120"]},
            "tanaka_s":    {"dept": "sales",       "ips": ["192.168.1.105"]},
            "sato_k":      {"dept": "engineering", "ips": ["192.168.2.50"]},
            "kato_m":      {"dept": "hr",          "ips": ["192.168.4.20"]},
            "ito_h":       {"dept": "finance",     "ips": ["192.168.3.30"]},
            "nakamura_r":  {"dept": "finance",     "ips": ["192.168.3.35"]},
            "honda_m":     {"dept": "engineering", "ips": ["192.168.2.65"]},
            "kimura_t":    {"dept": "engineering", "ips": ["192.168.2.55"]},
            "ito_a":       {"dept": "sales",       "ips": ["192.168.1.110"]},
            "fujita_k":    {"dept": "finance",     "ips": ["192.168.3.45"]},
            "kobayashi_t": {"dept": "finance",     "ips": ["192.168.3.40"]},
            "ogawa_s":     {"dept": "finance",     "ips": ["192.168.3.25"]},
            "ishida_j":    {"dept": "hr",          "ips": ["192.168.4.22"]},
            "hayashi_r":   {"dept": "hr",          "ips": ["192.168.4.35"]},
            "morita_n":    {"dept": "hr",          "ips": ["192.168.4.30"]},
            # å¿…è¦ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã“ã“ã«è¿½åŠ ...
        }
        self.dept_ip_ranges = {
            "sales":       "192.168.1.",
            "engineering": "192.168.2.",
            "finance":     "192.168.3.",
            "hr":          "192.168.4.",
        }
        self.allowed_cross_dept = {
            "sales":       ["sales"],
            "engineering": ["engineering", "sales"],
            "finance":     ["finance"],
            "hr":          ["hr"],
            # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
        }
        self.safe_processes = [
            "excel.exe", "winword.exe", "outlook.exe", "chrome.exe", "firefox.exe", "docker.exe"
        ]
        self.dangerous_processes = [
            "powershell.exe", "cmd.exe", "wmic.exe", "unknown.exe"
        ]

    def get_user_common_locations(self, user_id):
        return self.user_profiles.get(user_id, {}).get("ips", [])

    def get_user_department(self, user_id):
        return self.user_profiles.get(user_id, {}).get("dept", "unknown")

    def get_dept_ip_range(self, dept):
        return self.dept_ip_ranges.get(dept, "")

    def is_ip_allowed_for_user(self, user_id, ip):
        dept = self.get_user_department(user_id)
        # éƒ¨ç½²IPãƒ¬ãƒ³ã‚¸ï¼‹å€‹åˆ¥ç™»éŒ²IPã‚’è¨±å®¹
        return (ip.startswith(self.get_dept_ip_range(dept)) or ip in self.get_user_common_locations(user_id))

    def is_cross_dept_access_allowed(self, from_dept, to_dept):
        # éƒ¨ç½²é–“ã‚¢ã‚¯ã‚»ã‚¹ã®å¯å¦
        return to_dept in self.allowed_cross_dept.get(from_dept, [])

    def get_user_common_applications(self, user_id):
        return self.user_profiles.get(user_id, {}).get("apps", [])

    def is_process_safe(self, process_name):
        proc = process_name.lower()
        return any(safe in proc for safe in self.safe_processes)

    def is_process_dangerous(self, process_name):
        proc = process_name.lower()
        return any(danger in proc for danger in self.dangerous_processes)

    # â†“ä»¥ä¸‹ã¯ãƒ€ãƒŸãƒ¼ã®ã¾ã¾
    def get_last_access_timestamp(self, user_id, file_path):
        hours_ago = random.randint(1, 720)
        return datetime.now() - timedelta(hours=hours_ago)

    def get_file_access_count(self, user_id, file_path, days=30):
        return random.randint(0, 100)

    def get_login_failure_rate(self, user_id, days=7):
        return random.uniform(0.0, 0.3)

    def get_user_active_hours(self, user_id):
        return {
            "weekday_hours": list(range(8, 19)),
            "weekend_hours": []
        }

    def user_has_permission(self, user_id, permission_type):
        executive_users = ["executive", "ceo", "cfo"]
        if user_id.lower() in executive_users:
            return True
        return False

    def get_directory_access_stats(self, directory_path):
        return {
            "total_users": random.randint(5, 50),
            "avg_daily_access": random.randint(10, 200),
            "common_operations": ["FileRead", "FileWrite"]
        }

# -------------------------------
# ğŸ§¬ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é–¢æ•°
# -------------------------------
def fit_normal_clusters(state_vectors, n_clusters=5, save_examples=True):

    km = KMeans(n_clusters=n_clusters, random_state=42)
    km.fit(state_vectors)
    cluster_info = {}

    for i in range(n_clusters):
        cluster_points = state_vectors[km.labels_ == i]
        center = km.cluster_centers_[i]
        distances = np.linalg.norm(cluster_points - center, axis=1)
        radius = np.percentile(distances, 95)
        # â˜…åˆ†æ•£ã‚‚è¨ˆç®—
        cov = np.cov(cluster_points, rowvar=False)
        # â˜…ä»£è¡¨ãƒ‘ã‚¿ãƒ¼ãƒ³
        idx_nearest = np.argmin(distances)
        typical = cluster_points[idx_nearest] if save_examples else None

        cluster_info[i] = {
            "center": center,
            "radius": radius,
            "cov": cov,
            "size": len(cluster_points),
            "typical": typical,
        }
    return {"model": km, "info": cluster_info}

def compute_information_tensor(state_vector_list, epsilon=1e-6, max_norm=100):
    """æƒ…å ±ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¨ˆç®—ï¼ˆJITéå¯¾å¿œ:NumPyï¼‰"""
    matrix = np.array([safe_values(v) for v in state_vector_list])
    variances = np.var(matrix, axis=0)
    info_tensor = 1.0 / (variances + epsilon)
    return info_tensor * min(1.0, max_norm / np.max(info_tensor))

# ===== ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹è¨ˆç®—é–¢æ•° =====
def get_adaptive_weight(diff, key, weights_l1, weights_l2):
    if diff < 0.5:
        return weights_l1.get(key, 1.0)
    elif diff < 2.0:
        t = (diff - 0.5) / 1.5
        return (1 - t) * weights_l1.get(key, 1.0) + t * weights_l2.get(key, 1.0)
    else:
        return weights_l2.get(key, 1.0)

def adaptive_metric(diff):
    """é©å¿œçš„ãƒ¡ãƒˆãƒªãƒƒã‚¯"""
    if diff < 0.5:
        return diff
    elif diff < 2.0:
        return diff ** 2
    elif diff < 6.0:
        return 4 + (diff - 2) ** 2.2
    else:
        return 30 * np.log(diff - 4 + 2)

def compute_security_divergence(
    prev_state, curr_state,
    weights_l1=None, weights_l2=None,
    metric="weighted_hybrid",
    info_tensor=None,
    hybrid_ratio=0.7,
    config=None
):
    divergence = 0.0
    all_keys = set(prev_state.keys()) | set(curr_state.keys())

    # â˜… configå„ªå…ˆã§é‡ã¿ã‚»ãƒƒãƒˆ
    if config is not None:
        weights_l1 = config.weights_l1
        weights_l2 = config.weights_l2

    for key in all_keys:
        v1, v2 = prev_state.get(key, 0), curr_state.get(key, 0)
        diff = np.linalg.norm(np.array(v1) - np.array(v2)) if isinstance(v1, np.ndarray) else abs(v1 - v2)
        w = get_adaptive_weight(diff, key, weights_l1, weights_l2)

        if metric == "l1":
            delta = diff
        elif metric == "l2":
            delta = diff ** 2
        elif metric == "cosine":
            delta = 1.0 - np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
        elif metric == "zscore" and info_tensor is not None:
            mean = info_tensor["mean"].get(key, 0)
            std = info_tensor["std"].get(key, 1e-6)
            z = (v2 - mean) / std
            delta = np.abs(z) * w
        elif metric == "hybrid_z":
            mean = info_tensor["mean"].get(key, 0)
            std = info_tensor["std"].get(key, 1e-6)
            z = (v2 - mean) / std
            delta = hybrid_ratio * (w * adaptive_metric(diff)) + (1-hybrid_ratio) * (np.abs(z) * w)
        else:
            delta = w * adaptive_metric(diff)
        divergence += delta

    return divergence

# -------------------------------
# ğŸ§¬ ã‚¤ãƒ™ãƒ³ãƒˆè©•ä¾¡é–¢æ•°
# -------------------------------
def evaluate_security_event_by_cluster(
    new_state, model, info_tensor, weights_l1=None, weights_l2=None,
    metric="weighted_hybrid", config=None
):
    """ã‚¯ãƒ©ã‚¹ã‚¿ã«ã‚ˆã‚‹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆè©•ä¾¡"""
    if isinstance(model, dict) and "model" in model:
        model = model["model"]

    # â˜… configå„ªå…ˆã§é‡ã¿ã‚’å–å¾—
    if config is not None:
        if weights_l1 is None:
            weights_l1 = config.weights_l1.copy()
        if weights_l2 is None:
            weights_l2 = config.weights_l2.copy()

    state_dict = new_state if isinstance(new_state, dict) else {f"dim_{j}": v for j, v in enumerate(new_state)}

    min_divergence, best_cluster = float('inf'), -1
    for i, center in enumerate(model.cluster_centers_):
        center_dict = {k: v for k, v in zip(state_dict.keys(), center)}
        divergence = compute_security_divergence(
            center_dict, state_dict,
            weights_l1, weights_l2, metric,
            config=config
        )

        if divergence < min_divergence:
            min_divergence, best_cluster = divergence, i

    divergence_max = config.divergence_max if config and hasattr(config, 'divergence_max') else 100.0
    normalized_divergence = min(min_divergence, divergence_max)
    return normalized_divergence, best_cluster


def classify_event_by_multi_clusters(
    new_state,
    normal_model, exceptional_model, anomaly_model,
    info_tensor,
    config=None,
    weights_l1=None, weights_l2=None
):
    if config is not None:
        if weights_l1 is None:
            weights_l1 = config.weights_l1.copy()
        if weights_l2 is None:
            weights_l2 = config.weights_l2.copy()

    norm_div, norm_cl = evaluate_security_event_by_cluster(
        new_state, normal_model, info_tensor, weights_l1, weights_l2, config=config
    )

    if exceptional_model is not None:
        exc_div, exc_cl = evaluate_security_event_by_cluster(
            new_state, exceptional_model, info_tensor, weights_l1, weights_l2, config=config
        )
    else:
        exc_div, exc_cl = float('inf'), -1

    if anomaly_model is not None:
        ano_div, ano_cl = evaluate_security_event_by_cluster(
            new_state, anomaly_model, info_tensor, weights_l1, weights_l2, config=config
        )
    else:
        ano_div, ano_cl = float('inf'), -1

    min_div = min(norm_div, exc_div, ano_div)
    if min_div == ano_div and anomaly_model is not None:
        verdict = "anomaly"
        cluster = ano_cl
    elif min_div == exc_div and exceptional_model is not None:
        verdict = "exceptional"
        cluster = exc_cl
    else:
        verdict = "normal"
        cluster = norm_cl

    print(f"[XAI] verdict={verdict}, normal_div={norm_div:.2f}, exc_div={exc_div:.2f}, ano_div={ano_div:.2f}")

    return {
        "verdict": verdict,
        "cluster": cluster,
        "divergences": {
            "normal": norm_div,
            "exceptional": exc_div,
            "anomaly": ano_div
        },
        "best_divergence": min_div
    }

# -------------------------------
# ğŸ§¬ä¾‹å¤–ã®è¿½åŠ ã‚»ãƒƒãƒˆ
# -------------------------------
def random_private_ip():
    """PRIVATE_IP_PREFIXESã‹ã‚‰ç¤¾å†…å‘ã‘IPã‚’ãƒ©ãƒ³ãƒ€ãƒ ç”Ÿæˆ"""
    prefix = random.choice(PRIVATE_IP_PREFIXES)
    return f"{prefix}{random.randint(1, 4)}.{random.randint(1, 254)}"

def create_realistic_security_event(
    user_id, department, operation, timestamp,
    file_path=None, file_size_kb=None,
    process_name=None, destination_ip=None,
    status="SUCCESS", business_context=None
):
    """
    ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆã®ãƒªã‚¢ãƒ«ãªè¾æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    Args:
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        department: éƒ¨ç½²å
        operation: æ“ä½œç¨®åˆ¥ï¼ˆä¾‹: Login, FileRead, ...ï¼‰
        timestamp: datetimeå‹ï¼ˆè‡ªå‹•ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
        ...ï¼ˆç•¥ï¼‰
    Returns:
        event: dict
    """
    event = {
        "timestamp": timestamp.strftime("%Y-%m-%d %H:%M:%S"),
        "user_id": user_id,
        "department": department,
        "operation": operation,
        "status": status,
        "source_ip": random_private_ip()
    }
    if file_path is not None:
        event["file_path"] = file_path
    if file_size_kb is not None:
        event["file_size_kb"] = file_size_kb
    if process_name is not None:
        event["process_name"] = process_name
    if destination_ip is not None:
        event["destination_ip"] = destination_ip
    if business_context is not None:
        event["_business_context"] = business_context
    return event

def generate_morning_patterns(dept_name):
    """æŒ‡å®šéƒ¨ç½²ã®æœã®æ­£å¸¸ãªæ¥­å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ"""
    patterns = []
    dept_info = DEPARTMENTS.get(dept_name)
    if not dept_info:
        return patterns
    base_date = datetime(2025, 3, 10, 0, 0, 0)
    for user in dept_info["users"][:3]:
        login_time = base_date.replace(hour=random.randint(8, 9), minute=random.randint(0, 59))
        patterns.append(create_realistic_security_event(
            user, dept_name, "Login", login_time
        ))
        mail_time = login_time + timedelta(minutes=random.randint(5, 15))
        patterns.append(create_realistic_security_event(
            user, dept_name, "ProcessCreate", mail_time,
            process_name="outlook.exe"
        ))
    return patterns

def generate_noon_patterns(dept_name):
    """æŒ‡å®šéƒ¨ç½²ã®æ˜¼ã®æ­£å¸¸ãªæ¥­å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ"""
    patterns = []
    dept_info = DEPARTMENTS.get(dept_name)
    if not dept_info:
        return patterns
    base_date = datetime(2025, 3, 10, 0, 0, 0)
    for user in dept_info["users"][:2]:
        logout_time = base_date.replace(hour=12, minute=random.randint(0, 30))
        patterns.append(create_realistic_security_event(
            user, dept_name, "Logout", logout_time
        ))
        login_time = logout_time + timedelta(hours=1)
        patterns.append(create_realistic_security_event(
            user, dept_name, "Login", login_time
        ))
    return patterns


def generate_evening_patterns(dept_name):
    """å¤•æ–¹ã®æ­£å¸¸ãªãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼‹ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ"""
    patterns = []
    dept_info = DEPARTMENTS.get(dept_name)
    if not dept_info:
        return patterns

    base_date = datetime(2025, 3, 10, 0, 0, 0)
    for user in dept_info["users"][:3]:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒ»ãƒ—ãƒ­ã‚»ã‚¹ååˆ†å²ï¼ˆæ¥­å‹™ãƒ«ãƒ¼ãƒ«ã«å¿œã˜ã¦æ‹¡å¼µã—ã‚„ã™ã„è¨­è¨ˆï¼ï¼‰
        save_time = base_date.replace(hour=random.randint(17, 18), minute=random.randint(0, 59))
        file_path_map = {
            "sales": f"\\\\fileserver\\sales\\reports\\daily_report_{user}.xlsx",
            "engineering": f"\\\\fileserver\\engineering\\docs\\progress_{user}.md",
            "finance": f"\\\\fileserver\\finance\\reports\\summary_{user}.xlsx",
        }
        file_path = file_path_map.get(dept_name, f"\\\\fileserver\\{dept_name}\\docs\\report_{user}.docx")
        proc_name = "EXCEL.EXE" if dept_name in ["sales", "finance"] else "WINWORD.EXE"
        patterns.append(create_realistic_security_event(
            user, dept_name, "FileWrite", save_time,
            file_path=file_path,
            file_size_kb=random.randint(50, 500),
            process_name=proc_name
        ))
        logout_time = save_time + timedelta(minutes=random.randint(5, 30))
        patterns.append(create_realistic_security_event(
            user, dept_name, "Logout", logout_time
        ))
    return patterns

def generate_year_end_patterns(dept_name):
    """å¹´åº¦æœ«ã®ç¹å¿™ï¼†éƒ¨é–€æ¨ªæ–­ãƒ»ç‰¹æ¨©æ“ä½œå«ã‚€ãƒªã‚¢ãƒ«æ¥­å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ"""
    patterns = []
    dept_info = DEPARTMENTS.get(dept_name)
    if not dept_info:
        return patterns

    base_date = datetime(2025, 3, 29, 0, 0, 0)  # å¹´åº¦æœ«ã®åœŸæ›œæ—¥

    # é€±æœ«å‡ºç¤¾ãƒ­ã‚°ã‚¤ãƒ³
    for user in dept_info["users"]:
        login_time = base_date.replace(hour=random.randint(9, 11), minute=random.randint(0, 59))
        patterns.append(create_realistic_security_event(
            user, dept_name, "Login", login_time,
            business_context=["year_end", "weekend_work"]
        ))

    # éƒ¨ç½²ã”ã¨ã®ç‰¹æ®Šãªå”åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³
    if dept_name == "finance":
        # çµŒç†ã¯å–¶æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
        for user in dept_info["users"][:2]:  # ä»£è¡¨2å
            event_time = datetime(2025, 3, 30, random.randint(14, 16), random.randint(0, 59), 0)

            # å–¶æ¥­ã®å£²ä¸Šãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã‚€
            patterns.append(create_realistic_security_event(
                user, dept_name, "FileRead", event_time,
                file_path="\\\\fileserver\\sales\\reports\\2025\\03\\monthly_sales_202503.xlsx",
                file_size_kb=random.randint(2000, 5000),
                process_name="EXCEL.EXE",
                business_context=["year_end", "cross_dept_allowed"]
            ))

            # å–¶æ¥­ã®å¥‘ç´„ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¢ã‚¯ã‚»ã‚¹
            event_time = event_time.replace(hour=random.randint(16, 18))
            patterns.append(create_realistic_security_event(
                user, dept_name, "FileRead", event_time,
                file_path="\\\\fileserver\\sales\\contracts\\2025\\Q4\\contract_summary.xlsx",
                file_size_kb=random.randint(1000, 3000),
                process_name="EXCEL.EXE",
                business_context=["year_end", "cross_dept_allowed"]
            ))

    elif dept_name == "sales":
        # å–¶æ¥­ã¯çµŒç†ã‚·ã‚¹ãƒ†ãƒ ã«å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›
        for user in dept_info["users"][:2]:
            event_time = datetime(2025, 3, 31, random.randint(10, 12), random.randint(0, 59), 0)

            # çµŒç†ã®å¹´åº¦æœ«ãƒ¬ãƒãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿
            patterns.append(create_realistic_security_event(
                user, dept_name, "FileWrite", event_time,
                file_path="\\\\fileserver\\finance\\reports\\2025\\03\\å¹´åº¦æœ«æ±ºç®—_å£²ä¸Šå…¥åŠ›.xlsx",
                file_size_kb=random.randint(3000, 8000),
                process_name="EXCEL.EXE",
                business_context=["year_end", "cross_dept_allowed", "data_submission"]
            ))

            # è²¡å‹™ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶š
            patterns.append(create_realistic_security_event(
                user, dept_name, "NetworkConnect", event_time,
                destination_ip="192.168.3.100",  # è²¡å‹™ã‚µãƒ¼ãƒãƒ¼
                process_name="FinancePortal.exe",
                business_context=["year_end", "cross_dept_allowed"]
            ))

    elif dept_name == "hr":
        # HRã¯å…¨éƒ¨ç½²ã®æ®‹æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        for user in dept_info["users"][:1]:  # ä»£è¡¨1å
            base_time = datetime(2025, 3, 28, random.randint(17, 19), random.randint(0, 59), 0)

            # å„éƒ¨ç½²ã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            dept_paths = {
                "sales": "\\\\fileserver\\sales\\attendance\\2025\\03\\overtime_report.csv",
                "engineering": "\\\\fileserver\\engineering\\timesheet\\2025\\03\\dev_hours.xlsx",
                "finance": "\\\\fileserver\\finance\\attendance\\2025\\03\\working_hours.xlsx"
            }

            for dept, path in dept_paths.items():
                patterns.append(create_realistic_security_event(
                    user, dept_name, "FileRead", base_time,
                    file_path=path,
                    file_size_kb=random.randint(100, 500),
                    process_name="EXCEL.EXE",
                    business_context=["year_end", "cross_dept_allowed", "hr_audit"]
                ))
                base_time = base_time + timedelta(minutes=random.randint(5, 15))

    elif dept_name == "engineering":
        # ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¯å¹´åº¦æœ«ã®ã‚·ã‚¹ãƒ†ãƒ ç›£æŸ»å¯¾å¿œ
        for user in dept_info["users"][:1]:
            event_time = datetime(2025, 3, 27, random.randint(20, 22), random.randint(0, 59), 0)

            # ç›£æŸ»ãƒ­ã‚°ã¸ã®ç‰¹æ¨©ã‚¢ã‚¯ã‚»ã‚¹
            patterns.append(create_realistic_security_event(
                user, dept_name, "FileRead", event_time,
                file_path="\\\\auditserver\\logs\\system\\2025\\security_audit_log.txt",
                file_size_kb=random.randint(10000, 50000),
                process_name="notepad++.exe",
                business_context=["year_end", "security_audit", "elevated_access"]
            ))

            # ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
            patterns.append(create_realistic_security_event(
                user, dept_name, "FileWrite", event_time + timedelta(minutes=30),
                file_path="\\\\fileserver\\engineering\\audit\\2025\\annual_security_report.docx",
                file_size_kb=random.randint(500, 2000),
                process_name="WINWORD.EXE",
                business_context=["year_end", "security_audit"]
            ))

            # ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†ãƒ„ãƒ¼ãƒ«ã®èµ·å‹•
            patterns.append(create_realistic_security_event(
                user, dept_name, "ProcessCreate", event_time,
                process_name="mmc.exe",  # Microsoft Management Console
                business_context=["year_end", "security_audit", "elevated_access"]
            ))

    return patterns

# -------------------------------
# ğŸ§¬ ãƒªã‚¹ã‚¯åˆ¤å®šå¼·åŒ–
# -------------------------------
# ===== ãƒ¡ã‚¤ãƒ³åˆ¤å®šé–¢æ•° =====
def unified_security_judge(
    event: Dict[str, Any],
    user_history: Dict[str, Any],
    chain_context: Optional[Dict[str, Any]] = None,
    normal_div: Optional[float] = None,
    trust_score: Optional[float] = None
) -> Tuple[str, str]:

    # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®æŠ½å‡º
    op = event.get("operation", "")
    status = event.get("status", "")
    file_path = event.get("file_path", "")
    dept = event.get("department", "")
    process = event.get("process_name", "").lower()
    source_ip = event.get("source_ip", "")
    dest_ip = event.get("destination_ip", "")
    file_size_kb = event.get("file_size_kb", 0)
    hour = get_hour_from_timestamp(event.get("timestamp", ""))

    # ===== åŸºæœ¬åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ =====
    verdict, reason = _basic_security_rules(
        op, status, file_path, dept, process, source_ip, dest_ip,
        file_size_kb, hour, user_history
    )

    # chain_contextãŒãªã„å ´åˆã¯åŸºæœ¬åˆ¤å®šã®ã¿è¿”ã™
    if chain_context is None:
        return verdict, reason

    # ===== æ‹¡å¼µåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼‰ =====
    return _enhanced_security_analysis(
        verdict, reason, event, op, dept, hour,
        chain_context, normal_div, trust_score
    )

def _basic_security_rules(
    op: str, status: str, file_path: str, dept: str,
    process: str, source_ip: str, dest_ip: str,
    file_size_kb: int, hour: Optional[int],
    user_history: Dict[str, Any]
) -> Tuple[str, str]:
    """åŸºæœ¬çš„ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚‹åˆ¤å®š"""

    # 1. éƒ¨ç½²ã¨ãƒ‘ã‚¹ã®åˆ¶é™ãƒã‚§ãƒƒã‚¯
    if op in ("FileRead", "FileWrite") and dept in DEPT_PATH_RULES:
        rules = DEPT_PATH_RULES[dept]
        if any(restricted in file_path for restricted in rules["restricted"]):
            return "suspicious", rules["message"]

    # 2. æ©Ÿå¯†ãƒ•ã‚©ãƒ«ãƒ€æ“ä½œ
    if op in ("FileRead", "FileCopy") and "\\confidential\\" in file_path.lower():
        if dept not in CONFIDENTIAL_ACCESS_DEPTS:
            return "suspicious", "éæ¨©é™è€…ã«ã‚ˆã‚‹æ©Ÿå¯†ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ"

    # 3. æ¥­å‹™æ™‚é–“å¤–ã®ãƒ­ã‚°ã‚¤ãƒ³/ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ
    if op in ("Login", "Logout") and hour is not None:
        if hour < BUSINESS_HOURS["start"] or hour > BUSINESS_HOURS["end"] + 1:
            if op == "Login":
                return "investigating", "æ·±å¤œLogin"
            if op == "Logout" and status == "SUCCESS":
                return "normal", "æ·±å¤œLogoutç‰¹ä¾‹"

    # 4. å¤±æ•—ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ãƒã‚§ãƒƒã‚¯
    if status == "FAILED":
        failed_attempts = user_history.get("failed_attempts", 0)
        if failed_attempts >= THRESHOLDS["failed_attempts_critical"]:
            return "suspicious", "çŸ­æœŸé–“ã§å¤±æ•—å¤šç™º"
        else:
            return "investigating", "æ“ä½œå¤±æ•—"

    # 5. å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯å¤–éƒ¨é€ä¿¡
    if file_size_kb > THRESHOLDS["large_file_kb"]:
        return "critical", "å·¨å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ"

    if op == "NetworkConnect" and dest_ip and is_global_ip(dest_ip):
        return "critical", "å¤–éƒ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¸ã®æ¥ç¶š"

    # 6. å±é™ºãªãƒ—ãƒ­ã‚»ã‚¹
    DANGEROUS_PROCESSES = [name for name, _ in PROCESS_SCORES["dangerous"]]
    if any(d in process for d in DANGEROUS_PROCESSES):
        return "investigating", "å±é™ºãƒ—ãƒ­ã‚»ã‚¹åˆ©ç”¨"

    # 7. ç•°å¸¸ãªã‚¢ã‚¯ã‚»ã‚¹é »åº¦
    if user_history.get("access_count", 0) > THRESHOLDS["abnormal_access_count"]:
        return "latent_suspicious", "ç•°å¸¸ãªã‚¢ã‚¯ã‚»ã‚¹é »åº¦"

    # 8. ç®¡ç†é ˜åŸŸã¸ã®æ›¸ãè¾¼ã¿
    admin_paths = ["\\admin\\", "\\config\\"]
    if op == "FileWrite" and any(path in file_path.lower() for path in admin_paths):
        return "critical", "ç®¡ç†ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãè¾¼ã¿"

    # 9. ã‚°ãƒ­ãƒ¼ãƒãƒ«IPã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹
    if op in ("Login", "NetworkConnect"):
        if source_ip and is_global_ip(source_ip):
            return "critical", "æµ·å¤–IPã‹ã‚‰ã®æ“ä½œ"
        if dest_ip and is_global_ip(dest_ip):
            return "critical", "æµ·å¤–å®›ã¦æ¥ç¶š"

    return "normal", "é€šå¸¸"


def _enhanced_security_analysis(
    verdict: str, reason: str, event: Dict[str, Any],
    op: str, dept: str, hour: Optional[int],
    chain_context: Dict[str, Any],
    normal_div: Optional[float],
    trust_score: Optional[float]
) -> Tuple[str, str]:
    """ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã«ã‚ˆã‚‹æ‹¡å¼µã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åˆ¤å®š"""

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚­ãƒ¼ã®ç”Ÿæˆã¨å±¥æ­´æ›´æ–°
    pattern_key = f"{op}_{get_filetype(event.get('file_path', ''))}_{dept}"
    _update_pattern_history(chain_context, pattern_key)

    recent_patterns = sum(1 for v in chain_context['pattern_history'].values() if v > 1)

    # æ­£å¸¸åˆ¤å®šã®ç·©å’Œæ¡ä»¶ãƒã‚§ãƒƒã‚¯
    if verdict == "normal":
        if _should_relax_normal_verdict(
            recent_patterns, chain_context, normal_div, trust_score, op, hour
        ):
            return "normal", "æ­£å¸¸ï¼ˆæ¥­å‹™divï¼‹é »åº¦ï¼‹æ¥­å‹™æ™‚é–“ï¼‰"

    # investigating/latent_suspiciousã®ç·©å’Œ
    if verdict == "investigating":
        if _should_relax_investigating(
            op, trust_score, normal_div, chain_context, hour
        ):
            return "normal", "åˆå›/æ¥­å‹™divãƒ’ãƒƒãƒˆ(èª¿æ•´)"

    if verdict == "latent_suspicious":
        if _should_relax_latent_suspicious(
            op, trust_score, normal_div, chain_context
        ):
            return "normal", "åˆå›/æ¥­å‹™divãƒ’ãƒƒãƒˆ(èª¿æ•´)"

    # ç‰¹å®šã®ç†ç”±ã«å¯¾ã™ã‚‹ç´¯ç©ãƒã‚§ãƒƒã‚¯
    if reason in ("éæ¨©é™è€…ã«ã‚ˆã‚‹æ©Ÿå¯†ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ", "å–¶æ¥­ãŒçµŒç†/äººäº‹ãƒªã‚½ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹"):
        count = chain_context['pattern_history'].get(pattern_key, 0)
        if (count < THRESHOLDS["pattern_accumulation_limit"] and
            trust_score is not None and trust_score > THRESHOLDS["trust_score_medium"] and
            normal_div is not None and normal_div > THRESHOLDS["normal_div_threshold"]):
            return "investigating", f"{reason}ï¼ˆç·©å’Œ:ä¿¡é ¼ã‚¹ã‚³ã‚¢/åˆå›ç³»ï¼‰"
        return "suspicious", reason

    # critical/suspiciousã®ä¸€æ®µéšç·©å’Œ
    if verdict in ("suspicious", "critical"):
        if (trust_score and trust_score > THRESHOLDS["trust_score_low"] and
            normal_div and normal_div > THRESHOLDS["normal_div_high"]):
            return "investigating", "é«˜ä¿¡é ¼div: ä¸€æ®µéšç·©å’Œ"

    return verdict, reason

def _update_pattern_history(chain_context: Dict[str, Any], pattern_key: str) -> None:
    # ãƒ‘ã‚¿ãƒ¼ãƒ³å±¥æ­´ã®åˆæœŸåŒ–å®‰å…¨å¯¾å¿œ
    chain_context.setdefault('pattern_history', {})
    chain_context['pattern_history'][pattern_key] = \
        chain_context['pattern_history'].get(pattern_key, 0) + 1

    # é€£ç¶šã‚«ã‚¦ãƒ³ãƒˆã®æ›´æ–°
    last_pattern = chain_context.get("last_pattern")
    if last_pattern == pattern_key:
        chain_context['repeat_count'] = chain_context.get('repeat_count', 1) + 1
    else:
        chain_context['repeat_count'] = 1
    chain_context['last_pattern'] = pattern_key

def _should_relax_normal_verdict(
    recent_patterns: int, chain_context: Dict[str, Any],
    normal_div: Optional[float], trust_score: Optional[float],
    op: str, hour: Optional[int]
) -> bool:
    """æ­£å¸¸åˆ¤å®šã‚’ç·©å’Œã™ã¹ãã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    pattern_condition = (
        recent_patterns > THRESHOLDS["pattern_count_threshold"] or
        chain_context.get('anomaly_score', 0.0) > THRESHOLDS["anomaly_score_threshold"]
    )
    repeat_condition = chain_context['repeat_count'] > THRESHOLDS["repeat_count_threshold"]

    if pattern_condition or repeat_condition:
        return (
            normal_div is not None and normal_div < THRESHOLDS["normal_div_threshold"] and
            trust_score is not None and trust_score > THRESHOLDS["trust_score_threshold"] and
            op in BUSINESS_OPERATIONS and
            is_business_hours(hour)
        )
    return False

def _should_relax_investigating(
    op: str, trust_score: Optional[float], normal_div: Optional[float],
    chain_context: Dict[str, Any], hour: Optional[int]
) -> bool:
    """investigatingåˆ¤å®šã‚’ç·©å’Œã™ã¹ãã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    if op not in BUSINESS_OPERATIONS:
        return False

    # åˆå›ã‚¢ã‚¯ã‚»ã‚¹ã®å ´åˆ
    if chain_context.get('repeat_count', 1) <= 2:
        return (
            trust_score is not None and trust_score > THRESHOLDS["trust_score_threshold"] and
            normal_div is not None and normal_div < THRESHOLDS["normal_div_threshold"]
        )

    # æ“ä½œå¤±æ•—ã®å ´åˆ
    if chain_context.get('repeat_count', 1) < 3:
        return (
            normal_div is not None and normal_div < THRESHOLDS["normal_div_high"] and
            trust_score is not None and trust_score > THRESHOLDS["trust_score_medium"] and
            is_business_hours(hour)
        )

    return False

def _should_relax_latent_suspicious(
    op: str, trust_score: Optional[float], normal_div: Optional[float],
    chain_context: Dict[str, Any]
) -> bool:
    """latent_suspiciousåˆ¤å®šã‚’ç·©å’Œã™ã¹ãã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    return (
        op in BUSINESS_OPERATIONS and
        trust_score is not None and trust_score > THRESHOLDS["trust_score_high"] and
        normal_div is not None and normal_div < THRESHOLDS["normal_div_threshold"] and
        chain_context.get('repeat_count', 1) <= 2
    )

def is_internal_destination(dest):
    # IPï¼ˆæ–‡å­—åˆ—ï¼‰â†’ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä¸€è‡´
    if any(dest.startswith(prefix) for prefix in TRUSTED_IPS):
        return True
    # ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆFQDNä¸€è‡´
    if dest in TRUSTED_HOSTNAMES:
        return True
    return False
# -------------------------------
# ğŸ§¬ ãƒ¢ãƒ¼ãƒ‰åˆ†é¡é–¢æ•°
# -------------------------------
def classify_security_mode_auto(
    divergence,
    state_params,
    previous_ema=None,
    config=None,
    threshold_override=None,
    operation=None,
    is_user_chain=False  # ğŸ†• ã“ã“ã§è¿½åŠ ï¼
):
    if config is None:
        raise ValueError("configã¯å¿…é ˆã§ã™ï¼")

    severity_level = state_params.get("severity_level", 0.0)
    action_vector = state_params.get("action_vector", np.array([1.0, 0.0, 0.0]))
    action_magnitude = np.linalg.norm(action_vector)
    trust_score = state_params.get("trust_score", 1.0)
    operation = operation or state_params.get("operation", "")
    timestamp = state_params.get("timestamp")

    # ğŸ”„ é€šå¸¸ã®ã‚¹ã‚³ã‚¢ç®—å‡º
    weighted_score = (
        config["alpha"] * divergence +
        config["beta"] * severity_level +
        config["gamma"] * action_magnitude
    )
    if previous_ema is not None:
        weighted_score = (
            config["ema_alpha"] * weighted_score +
            (1 - config["ema_alpha"]) * previous_ema
        )

    threshold_suspicious = threshold_override or config["threshold_suspicious"]
    threshold_investigating = threshold_suspicious * 0.5

    # ğŸ§­ æ„å‘³å¯†åº¦ã‚¹ã‚³ã‚¢ã«åŸºã¥ãåˆ¤å®š
    if is_user_chain:
        threshold_suspicious = THRESHOLD_SUSPICIOUS_USER
        threshold_investigating = THRESHOLD_INVESTIGATING_USER
        trust_error_threshold = TRUST_ERROR_THRESHOLD_USER
    else:
        threshold_suspicious = THRESHOLD_SUSPICIOUS_ORG
        threshold_investigating = THRESHOLD_INVESTIGATING_ORG
        trust_error_threshold = TRUST_ERROR_THRESHOLD_ORG

    # ã“ã®ã‚ã¨ã®ãƒ–ãƒ­ãƒƒã‚¯ã§ã¯ã€"çµ±ä¸€ã•ã‚ŒãŸåå‰"ã‚’ä½¿ã†
    if trust_score < trust_error_threshold:
        mode = "critical"
    elif divergence > THRESHOLD_DIV_CRITTCAL:
        mode = "critical"
    elif weighted_score > threshold_suspicious:
        mode = "suspicious"
    elif divergence > THRESHOLD_DIV_SUSPICIOUS:
        mode = "suspicious"
    elif weighted_score > threshold_investigating:
        mode = "investigating"
    elif trust_score < TRUST_SCORE_INVESTIGATING_1 and divergence > NORMAL_DIV_INVESTIGATING_1:
        mode = "investigating"
    elif trust_score < TRUST_SCORE_INVESTIGATING_2 and divergence > NORMAL_DIV_INVESTIGATING_2:
        mode = "investigating"
    else:
        mode = "normal"

    return mode, weighted_score

def calculate_event_score_from_real_log(event, security_log):
    """
    LanScope Catãƒ­ã‚°ã‹ã‚‰å–å¾—å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ã‚¹ã‚³ã‚¢è¨ˆç®—
    """
    # --- åŸºæœ¬ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
    operation = event.get("operation", event.get("event_type", ""))
    file_path = event.get("file_path", "")
    process = event.get("process_name", "").lower()
    destination = event.get("destination_ip", "")
    status = event.get("status", "SUCCESS")
    dept = event.get("department", "")
    file_size_kb = event.get("file_size_kb", 0)
    file_size_mb = file_size_kb / 1024
    timestamp = event.get("timestamp", event.get("event_time", ""))
    user_id = event.get("user_id", "unknown")
    source_ip = event.get("source_ip", "")
    score = OPERATION_SCORES.get(operation, 10)
    trust_score = 1.0

    # --- 1. Loginæ™‚ã®IPãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼èªè¨¼ ---
    if operation == "Login":
        if source_ip and not source_ip.startswith(PRIVATE_IP_PREFIXES):
            score += EXTERNAL_IP_SCORES["external"]
        if user_id == "unknown":
            score += 40
        try:
            approved = security_log.get_user_approved_devices(user_id)
            if event.get("workstation_name") in approved:
                score = max(score - 10, 0)
        except Exception:
            pass

    # --- 2. æ™‚é–“å¸¯ãƒšãƒŠãƒ«ãƒ†ã‚£ ---
    hour = None
    if timestamp:
        try:
            hour = int(timestamp.split()[1].split(":")[0])
        except Exception:
            pass
    if hour is not None:
        for (start, end), add in TIME_PENALTY_SCORES:
            if start <= hour < end:
                score += add

    # --- 3. Logoutç‰¹ä¾‹ ---
    if operation == "Logout":
        score = 0
        if hour in [12, 17, 18, 19, 20, 21, 22]:
            score = 0
        if dept == "executive":
            score = 0

    # --- 4. é€šå¸¸å¤–IPã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ ---
    if source_ip and user_id != "unknown":
        try:
            typical_ips = security_log.get_user_common_locations(user_id)
            if typical_ips and source_ip not in typical_ips:
                if source_ip.startswith(PRIVATE_IP_PREFIXES):
                    score += EXTERNAL_IP_SCORES["internal"]
                else:
                    score += EXTERNAL_IP_SCORES["external"]
        except Exception:
            pass

    # --- 5. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ç¨®åˆ¥ã”ã¨ã®åŠ ç‚¹ ---
    def path_score_by_type(path):
        path_lower = path.lower()
        for key in ["high", "sensitive", "normal"]:
            for pattern, add in PATH_SCORES[key]:
                if pattern in path_lower:
                    return add
        return 0

    if file_path:
        score += path_score_by_type(file_path)
        path_lower = file_path.lower()

        # é¡§å®¢ãƒªã‚¹ãƒˆãƒ»å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
        if operation == "FileRead" and any(
            key in path_lower for key in [
                "customer", "é¡§å®¢", "full_customer_list", "bonus_plan", "employee_list"
            ]
        ):
            if file_size_mb > 20:
                score += 40
            elif file_size_mb > 5:
                score += 20

        # Confidentialåˆ¤å®šã¨å®¹é‡ä¾å­˜åŠ ç‚¹
        if "\\confidential\\" in path_lower:
            user_has_confidential_access = security_log.user_has_permission(user_id, "confidential")
            is_executive = dept.lower() == "executive"
            if file_size_mb > 50:
                score += 20 if (user_has_confidential_access or is_executive) else 80
            else:
                score += 5 if (user_has_confidential_access or is_executive) else 40

    # --- 6. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåŠ ç‚¹ ---
    for threshold, add in FILE_SIZE_THRESHOLDS:
        if file_size_mb > threshold:
            score += add
            break

    # --- 7. é€±æœ«åŠ ç‚¹ ---
    if timestamp:
        try:
            import datetime as dtmod
            date_str = timestamp.split()[0]
            if dtmod.datetime.strptime(date_str, "%Y-%m-%d").weekday() >= 5:
                score += 10  # WEEKEND_BONUSã§çµ±ä¸€ã—ã¦ã‚‚OK
        except Exception:
            pass

    # --- 8. ãƒ—ãƒ­ã‚»ã‚¹ååŠ ç‚¹ ---
    def match_process(process, procs):
        return any(tool in process for tool in procs)

    # å±é™ºãƒ—ãƒ­ã‚»ã‚¹
    dangerous_procs = [name for name, _ in PROCESS_SCORES["dangerous"]]
    admin_procs = [name for name, _ in PROCESS_SCORES["admin"]]
    archiver_procs = [name for name, _ in PROCESS_SCORES["archiver"]]

    if match_process(process, dangerous_procs):
        score += 70
    elif match_process(process, admin_procs):
        score += 20
    elif match_process(process, archiver_procs):
        score += 25

    # --- 9. å¤–éƒ¨å®›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ---
    if destination and not destination.startswith(PRIVATE_IP_PREFIXES):
        score += 50
        if destination.startswith(("198.51.", "203.0.113.")):
            score += 30

    # --- 10. éƒ¨ç½²Ã—ãƒªã‚½ãƒ¼ã‚¹åˆ¤å®š ---
    if destination and not is_internal_destination(destination):
        score += 50
        if destination.startswith(("198.51.", "203.0.113.")):
            score += 30

    # --- 11. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ä¾å­˜åŠ ç‚¹ ---
    if status == "FAILED":
        if operation == "LoginFailed":
            score += 10
        elif operation == "NetworkConnect":
            score += 25
        else:
            score += 15

    # --- 12. æ–‡æ›¸ä½œæˆã®ä¾‹å¤–æ¸›ç‚¹ ---
    if operation == "FileWrite" and file_path:
        if any(doc in file_path.lower() for doc in [".docx", ".xlsx", ".pptx", "proposals", "proposal_"]):
            score = max(score - 25, 2)

    # --- ã‚¹ã‚³ã‚¢ä¸Šé™ ---
    return min(score, 100)

def calculate_severity_level(event_score):
    """
    ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰ã‹ã‚‰é‡å¤§åº¦ãƒ¬ãƒ™ãƒ«ï¼ˆ0.0-1.0ï¼‰ã‚’è¨ˆç®—
    """
    for threshold, level in SEVERITY_LEVEL_THRESHOLDS:
        if event_score >= threshold:
            return level
    return SEVERITY_LEVEL_THRESHOLDS[-1][1]  # æœ€å°å€¤ã§è¿”ã™

def calculate_action_vector_from_operation(operation):
    """
    æ“ä½œã‚¿ã‚¤ãƒ—ã‹ã‚‰è¡Œå‹•ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™
    """
    return ACTION_OPERATION_VECTORS.get(operation, ACTION_DEFAULT_VECTOR)

def calculate_user_trust_from_history(event, security_log, prev_trust=1.0):
    user_id = event.get("user_id", "unknown")
    file_path = event.get("file_path", "")
    operation = event.get("operation", "")
    status = event.get("status", "SUCCESS")
    base_trust = prev_trust

    # --- å±¥æ­´ãƒ»æ´»å‹•ãƒ™ãƒ¼ã‚¹ã®æ¸›ç‚¹ ---
    try:
        login_failure_rate = security_log.get_login_failure_rate(user_id) or 0.0
        base_trust -= login_failure_rate
    except Exception:
        pass

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹å±¥æ­´ï¼ˆrecency, first accessï¼‰
    if file_path:
        try:
            last_access = security_log.get_last_access_timestamp(user_id, file_path)
            if last_access:
                hours_since_access = (datetime.now() - last_access).total_seconds() / 3600
            else:
                hours_since_access = HOURS_SINCE_ACCESS
            access_count = security_log.get_file_access_count(user_id, file_path) or 0
        except Exception:
            hours_since_access = HOURS_SINCE_ACCESS
            access_count = 0
    else:
        hours_since_access = HOURS_SINCE_ACCESS
        access_count = 0

    is_first_access = access_count == 0
    first_access_penalty = TRUST_SCORES["first_access_penalty"] if is_first_access else 0.0
    recency_penalty = min(hours_since_access / TRUST_SCORES["recency_divisor"], TRUST_SCORES["recency_penalty_cap"])

    # --- æ“ä½œã”ã¨ã®ä¿¡é ¼å¤‰å‹• ---
    operation_trust = TRUST_SCORES["operation_penalty"].get(operation, 0.0)

    # ç•°å¸¸ãƒ»å¤±æ•—ç³»ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ç´¯ç©å‹ã§è¿½åŠ 
    trust_delta = 0.0
    if status == "FAILED":
        if operation in ["Login", "LoginFailed"]:
            trust_delta -= 0.45
        elif operation in ["FileWrite", "FileDelete", "ProcessCreate"]:
            trust_delta -= 0.25
        elif operation == "NetworkConnect":
            trust_delta -= 0.30
        else:
            trust_delta -= 0.15
    if event.get("critical_flag", False):
        trust_delta -= 0.4
    if event.get("external_access_flag", False):
        trust_delta -= 0.25

    # æ—¥å¸¸æ¥­å‹™ã¯å¾®å›å¾©ï¼ˆã¾ãŸã¯0.0ï¼‰
    if status == "SUCCESS" and operation in ["Login", "FileRead", "FileWrite"]:
        trust_delta += 0.01

    # IPé€¸è„±ãƒšãƒŠãƒ«ãƒ†ã‚£
    source_ip = event.get("source_ip", "")
    if source_ip and user_id != "unknown":
        try:
            typical_ips = security_log.get_user_common_locations(user_id) or []
            if typical_ips and source_ip not in typical_ips:
                if source_ip.startswith(PRIVATE_IP_PREFIXES):
                    trust_delta -= TRUST_SCORES["internal_ip_penalty"]
                else:
                    trust_delta -= TRUST_SCORES["external_ip_penalty"]
        except Exception:
            pass

    # æ™‚é–“å¸¯é€¸è„±
    timestamp = event.get("timestamp", "")
    if timestamp:
        try:
            hour = int(timestamp.split()[1].split(":")[0])
            active_hours = security_log.get_user_active_hours(user_id)
            if "weekday_hours" in active_hours and hour not in active_hours["weekday_hours"]:
                trust_delta -= TRUST_SCORES["time_penalty"]
        except Exception:
            pass

    # ãƒ“ã‚¸ãƒã‚¹æ–‡è„ˆã«ã‚ˆã‚‹ãƒœãƒ¼ãƒŠã‚¹
    contexts = event.get("_business_context", [])
    if "year_end" in contexts or "quarter_end" in contexts:
        trust_delta += TRUST_SCORES["business_context_year_end_bonus"]
    if "cross_dept_allowed" in contexts:
        trust_delta += TRUST_SCORES["business_context_cross_dept_bonus"]

    # --- ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®— ---
    trust_score = base_trust * (1.0 - recency_penalty - first_access_penalty) + operation_trust + trust_delta

    # ç¯„å›²åˆ¶é™
    trust_score = max(TRUST_SCORES["min_trust"], min(TRUST_SCORES["max_trust"], trust_score))
    return trust_score

def calculate_threat_context_from_log(event, security_log):
    threat_score = 0.0

    # 1. æ™‚é–“å¸¯ã®ç•°å¸¸
    timestamp = event.get("timestamp", "")
    if timestamp:
        time_anomaly = calculate_time_anomaly(timestamp)
        threat_score += time_anomaly * THREAT_CONTEXT_WEIGHTS["time_anomaly"]

        user_id = event.get("user_id", "unknown")
        hour = int(timestamp.split()[1].split(":")[0])
        try:
            active_hours = security_log.get_user_active_hours(user_id)
            weekday_hours = active_hours.get("weekday_hours", [])
        except Exception:
            weekday_hours = []
        if hour not in weekday_hours:
            threat_score += THREAT_CONTEXT_WEIGHTS["outside_active_hours"]

    # 2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç•°å¸¸ï¼ˆå¤–éƒ¨é€šä¿¡ï¼‰
    destination_ip = event.get("destination_ip", "")
    if destination_ip:
        if not destination_ip.startswith(PRIVATE_IP_PREFIXES):
            threat_score += THREAT_CONTEXT_WEIGHTS["external_network"]
        # GATEWAY_IP_SUFFIXESãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã§OK
        if 'GATEWAY_IP_SUFFIXES' in globals():
            if destination_ip.endswith(tuple(GATEWAY_IP_SUFFIXES)):
                threat_score += THREAT_CONTEXT_WEIGHTS["gateway_access"]

    # 3. ãƒ—ãƒ­ã‚»ã‚¹ã®ç•°å¸¸åº¦
    process_name = event.get("process_name", "").lower()
    user_id = event.get("user_id", "unknown")
    try:
        common_apps = [app.lower() for app in security_log.get_user_common_applications(user_id) or []]
    except Exception:
        common_apps = []

    DANGEROUS_PROCESSES = [name for name, _ in PROCESS_SCORES["dangerous"]]
    if any(proc in process_name for proc in DANGEROUS_PROCESSES):
        threat_score += THREAT_CONTEXT_WEIGHTS["dangerous_process"]
    elif process_name and process_name not in common_apps:
        threat_score += THREAT_CONTEXT_WEIGHTS["unknown_process"]

    # 4. æ“ä½œã®å¤±æ•—
    if event.get("status") == "FAILED":
        threat_score += THREAT_CONTEXT_WEIGHTS["operation_failed"]

    # 5. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ç•°å¸¸ï¼ˆã‚·ã‚¹ãƒ†ãƒ é ˜åŸŸã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
    file_path = event.get("file_path", "").lower()
    SYSTEM_PATHS = [p for (p, _) in PATH_SCORES["high"]]
    if file_path:
        if any(path in file_path for path in SYSTEM_PATHS):
            threat_score += THREAT_CONTEXT_WEIGHTS["system_path_access"]

    return min(threat_score, THREAT_CONTEXT_WEIGHTS["max_threat"])

def security_event_to_state(event, security_log, config=None):
    """
    ãƒ­ã‚°ã‹ã‚‰æ„å‘³æ§‹é€ ãƒ†ãƒ³ã‚½ãƒ«çŠ¶æ…‹ã‚’ç”Ÿæˆ
    """
    config = config or getattr(self, "tune_conf", None)
    event_score = calculate_event_score_from_real_log(event, security_log)
    severity_level = calculate_severity_level(event_score)

    operation = event.get("operation", "FileRead")
    action_vector = calculate_action_vector_from_operation(operation)
    action_magnitude = np.linalg.norm(action_vector)

    threat_context = calculate_threat_context_from_log(event, security_log)
    trust_score = calculate_user_trust_from_history(event, security_log)

    # ğŸŒŸ ãƒ¢ãƒ¼ãƒ‰åˆ¤å®šã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å®šæ•°å„ªå…ˆ
    mode_map = getattr(config, "security_mode_mapping")
    thresholds = MODE_THRESHOLDS if 'MODE_THRESHOLDS' in globals() else {"critical": 60, "suspicious": 30, "investigating": 15}
    if event_score > thresholds["critical"]:
        mode_value = mode_map["critical"]
    elif event_score > thresholds["suspicious"]:
        mode_value = mode_map["suspicious"]
    elif event_score > thresholds["investigating"]:
        mode_value = mode_map["investigating"]
    else:
        mode_value = mode_map["normal"]

    # ğŸŒŸ extraæ‹¡å¼µãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚å°†æ¥æ‹¡å¼µã—ã‚„ã™ã
    extra = {
        "vector": vectorize_state({
            "severity_level": severity_level,
            "action_magnitude": action_magnitude,
            "threat_context": threat_context,
            "trust_score": trust_score,
            "security_mode": mode_value,
            "event_score": event_score,
        }),
        # è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã“ã“ã«è¿½è¨˜OK
    }

    state = create_state_dict(
        severity=severity_level,
        action_mag=action_magnitude,
        threat=threat_context,
        trust=trust_score,
        mode=mode_value,
        score=event_score,
        extra=extra
    )
    return state

def calculate_time_anomaly(timestamp_str):
    """
    â° æ™‚é–“ç•°å¸¸ã‚¹ã‚³ã‚¢ã®è¨ˆç®— - ãƒ†ãƒ¼ãƒ–ãƒ«ï¼†å®šæ•°åˆ©ç”¨ç‰ˆ
    """
    try:
        from datetime import datetime
        if isinstance(timestamp_str, str):
            date_time = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
        else:
            date_time = timestamp_str

        hour = date_time.hour
        weekday = date_time.weekday()

        # æ™‚é–“å¸¯ã‚¹ã‚³ã‚¢
        base_score = 0.0
        for (start, end), score in TIME_ANOMALY_SCORES:
            if start <= hour < end or (end < start and (hour >= start or hour < end)):
                base_score = score
                break

        # é€±æœ«åŠ ç®—
        if weekday >= 5:  # åœŸæ—¥
            base_score = min(base_score + WEEKEND_BONUS, 1.0)

        # æœˆæ›œæ—©æœ
        if weekday == MONDAY_MORNING[0] and MONDAY_MORNING[1] <= hour <= MONDAY_MORNING[2]:
            base_score *= MONDAY_MORNING[3]
        # é‡‘æ›œå¤œ
        if weekday == FRIDAY_NIGHT[0] and FRIDAY_NIGHT[1] <= hour <= FRIDAY_NIGHT[2]:
            base_score *= FRIDAY_NIGHT[3]

        return base_score

    except Exception:
        return 0.3

# ===== ã‚µãƒãƒ¼ãƒˆã‚¯ãƒ©ã‚¹ =====
class EMAFilter:
    """æŒ‡æ•°ç§»å‹•å¹³å‡ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå‹•çš„Î±å¯¾å¿œÎ›Â³æ‹¡å¼µï¼‰"""
    def __init__(self, alpha=0.3):
        self.default_alpha = alpha
        self.value = None

    def _compute_alpha(self, new_value, context):
        previous_ema = self.value if self.value is not None else new_value
        change_magnitude = abs(new_value - previous_ema)
        alpha = self.default_alpha
        if change_magnitude > 30:
            alpha = 0.8
        elif change_magnitude < 5:
            alpha = 0.3
        if context:
            if context.get('is_critical_operation', False):
                alpha = min(0.9, alpha * 1.2)
            if context.get('is_routine_operation', False):
                alpha = max(0.2, alpha * 0.8)
            if context.get('sync_rate', 1.0) < 0.7:
                alpha = max(0.2, alpha * 0.85)
            if context.get('semantic_density', 1.0) > 1.5:
                alpha = min(0.95, alpha * 1.2)
        return alpha

    def update(self, new_value, context=None):
        alpha = self._compute_alpha(new_value, context)
        if self.value is None:
            self.value = new_value
        else:
            self.value = alpha * new_value + (1 - alpha) * self.value
        return self.value

# -------------------------------
# ğŸ”¥ SecurityBlock ã‚¯ãƒ©ã‚¹
# -------------------------------

class SecurityBlock:
    def __init__(self, index, data, previous_hash, state_params, divergence, metadata=None):
        self.index = index
        self.data = data
        self.previous_hash = previous_hash
        self.state_params = state_params
        self.divergence = divergence
        self.metadata = metadata or {}
        self.hash = self.hashing()

    def hashing(self):
        key = hashlib.sha256()
        key.update(str(self.index).encode('utf-8'))
        key.update(str(self.data).encode('utf-8'))
        key.update(str(self.previous_hash).encode('utf-8'))
        for k in sorted(self.state_params.keys()):
            key.update(str(k).encode('utf-8'))
            val = self.state_params[k]
            key.update(val.tobytes() if isinstance(val, np.ndarray) else str(val).encode('utf-8'))
        for k in sorted(self.metadata.keys()):
            key.update(str(k).encode('utf-8'))
            key.update(str(self.metadata[k]).encode('utf-8'))
        key.update(str(self.divergence).encode('utf-8'))
        return key.hexdigest()

    def verify(self):
        return self.hash == self.hashing()

def add_block(self, data, state_params, divergence, metadata=None, fork_threshold=None):
    prev_block = self.blocks[-1] if self.blocks else None
    event_hash = ScalableSecurityChain.compute_event_hash(event)
    new_block = SecurityBlock(
        index=len(self.blocks),
        data=data,
        previous_hash=prev_block.hash if prev_block else "0"*64,
        state_params=state_params,
        divergence=divergence,
        metadata=metadata or {}
    )

    # --- ãƒ•ã‚©ãƒ¼ã‚¯åˆ¤å®š ---
    prev_vector = prev_block.state_params.get("vector") if prev_block else None
    new_vector  = state_params.get("vector")
    forked = False
    # ğŸŒŸå€‹äºº/çµ„ç¹”ã”ã¨è‡ªå‹•åˆ‡æ›¿
    if fork_threshold is None and hasattr(self, 'tune_conf'):
        if hasattr(self, 'user_id'):  # UserSecurityChainåˆ¤å®š
            fork_threshold = getattr(self.tune_conf, 'fork_threshold_user', 12.0)
        else:  # çµ„ç¹”
            fork_threshold = getattr(self.tune_conf, 'fork_threshold_org', 15.0)
        if prev_vector is not None and new_vector is not None and fork_threshold:
            diff = np.linalg.norm(np.array(new_vector) - np.array(prev_vector))
            if diff > fork_threshold:
                forked = True
                new_block.metadata["forked"] = True
                alert_fork(self, prev_block, new_block)
            else:
                new_block.metadata["forked"] = False
        else:
            self.blocks.append(new_block)
            new_block.metadata["forked"] = False

def alert_fork(chain, prev_block, new_block, threshold=12.0):
    """
    ãƒ•ã‚©ãƒ¼ã‚¯ï¼ˆåˆ†å²ï¼‰ã‚’æ¤œçŸ¥ã—ãŸéš›ã«å‘¼ã°ã‚Œã‚‹ã‚¢ãƒ©ãƒ¼ãƒˆé–¢æ•°ã€‚
    threshold: ã“ã®å€¤æœªæº€ã®divergenceã¯ç„¡è¦–ï¼ˆã‚¢ãƒ©ãƒ¼ãƒˆå‡ºã•ãªã„ï¼‰
    """
    divergence = new_block.divergence
    if divergence < threshold:
        # å¾®å°ãƒ•ã‚©ãƒ¼ã‚¯ã¯ã‚¢ãƒ©ãƒ¼ãƒˆå‡ºã•ãªã„ï¼ˆé™ã‹ã«ã‚¹ãƒ«ãƒ¼ï¼‰
        return  # ä½•ã‚‚ã›ãšçµ‚äº†

    user_id = new_block.metadata.get("user_id", "unknown")
    dept = new_block.metadata.get("department", "unknown")
    index = new_block.index
    ts = new_block.metadata.get("timestamp", "N/A")

    print(f"ğŸš¨ [FORK ALERT] Block {index} in chain (User: {user_id}, Dept: {dept})")
    print(f"ã€€æ™‚åˆ»: {ts}")
    print(f"ã€€é€¸è„±ã‚¹ã‚³ã‚¢: {divergence:.2f}")
    print(f"ã€€å‰ãƒ–ãƒ­ãƒƒã‚¯: {prev_block.index} ã¨ã®åˆ†å²ç™ºç”Ÿï¼")

    if hasattr(chain, "fork_alerts"):
        fork_alert = {
            "block_index": index,
            "user_id": user_id,
            "department": dept,
            "timestamp": ts,
            "divergence": divergence,
            "prev_block_index": prev_block.index,
            "event_data": new_block.data,
            "vector_diff": np.linalg.norm(
                np.array(new_block.state_params.get("vector")) -
                np.array(prev_block.state_params.get("vector"))
            ) if new_block.state_params.get("vector") is not None and prev_block.state_params.get("vector") is not None else None
        }
        chain.fork_alerts.append(fork_alert)
        print(f"ã€€[LOG] fork_alertsã«è¿½åŠ : {fork_alert}")
    else:
        print("ã€€[WARN] chainã«fork_alertså±æ€§ãŒç„¡ã„ã‚ˆï¼")

    # ã“ã“ã§é€šçŸ¥ã‚„ãƒ¡ãƒ¼ãƒ«ã€Webhook/Slackç­‰ã«é€£æºå¯èƒ½
    # send_fork_alert_to_slack(user_id, dept, index, divergence, ts) ã¿ãŸã„ã«

def cross_dept_alert_sync(manager, window_minutes=180):
    """
    å…¨éƒ¨ç½²ã®é‡å¤§ã‚¢ãƒ©ãƒ¼ãƒˆï¼ˆcritical, suspiciousï¼‰ãŒ
    æŒ‡å®šwindow_minutesä»¥å†…ã§è¤‡æ•°éƒ¨é–€ã«è·¨ã£ã¦åŒæ™‚å¤šç™ºã—ã¦ã„ãªã„ã‹æ¤œå‡ºï¼
    """
    all_alerts = []
    # 1. å…¨éƒ¨ç½²ãƒã‚§ãƒ¼ãƒ³ã‚’èµ°æŸ»
    for dept, chain in manager.department_chains.items():
        for block in chain.blocks:
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¢ãƒ¼ãƒ‰ï¼†æ™‚åˆ»å–å¾—
            mode = block.metadata.get("security_mode", "")
            timestamp = block.metadata.get("timestamp", None)
            if not timestamp and hasattr(block, "event"):
                timestamp = block.event.get("timestamp", None)
            if timestamp and isinstance(timestamp, str):
                try:
                    timestamp = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
                except Exception:
                    continue
            if mode in ["critical", "suspicious"] and timestamp:
                all_alerts.append({
                    "dept": dept,
                    "time": timestamp,
                    "mode": mode,
                    "user": block.metadata.get("user_id", "")
                })
    # 2. æ™‚ç³»åˆ—ã§ã‚½ãƒ¼ãƒˆ
    all_alerts.sort(key=lambda a: a["time"])

    # 3. window_minutesä»¥å†…ã§ã€Œéƒ¨ç½²ãŒé•ã†ã€ã‚¢ãƒ©ãƒ¼ãƒˆåŒå£«ã‚’æ¢ç´¢
    found = False
    for i in range(len(all_alerts)):
        for j in range(i + 1, len(all_alerts)):
            # windowå¤–ã¯break
            if (all_alerts[j]["time"] - all_alerts[i]["time"]) > timedelta(minutes=window_minutes):
                break
            if all_alerts[i]["dept"] == all_alerts[j]["dept"]:
                continue
            # çµ„ã¿åˆã‚ã›ãŒãƒ’ãƒƒãƒˆ
            print(f"âš¡ï¸ Cross-dept alert: {all_alerts[i]['dept']} ({all_alerts[i]['user']}, {all_alerts[i]['mode']}, {all_alerts[i]['time']}) "
                  f"<-> {all_alerts[j]['dept']} ({all_alerts[j]['user']}, {all_alerts[j]['mode']}, {all_alerts[j]['time']})")
            found = True

    if not found:
        print("âœ… éƒ¨ç½²æ¨ªæ–­ã®åŒæœŸã‚¢ãƒ©ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“")

def calc_event_priority(event):
    """
    ã‚¤ãƒ™ãƒ³ãƒˆã®å„ªå…ˆåº¦ã‚’è¨ˆç®—ã™ã‚‹çµ±ä¸€é–¢æ•°
    å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹
    """
    op = event.get("operation", "").lower()
    status = event.get("status", "SUCCESS")
    file_size_kb = event.get("file_size_kb", 0)
    destination = event.get("destination_ip", "")
    source_ip = event.get("source_ip", "")
    base_score = 0.1

    # æ“ä½œã®å±é™ºæ€§ã«å¿œã˜ã¦æ˜ç¢ºã«å·®åˆ¥åŒ–ï¼
    if op == "filedelete":
        base_score += 0.5  # å‰Šé™¤ã¯æ¥µã‚ã¦é‡è¦
    elif op == "filecopy":
        base_score += 0.6  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼ã¯ã•ã‚‰ã«é‡è¦ï¼ˆå¤–éƒ¨é€ä¿¡ã®å¯èƒ½æ€§ï¼‰
    elif op == "filewrite":
        base_score += 0.4  # æ›¸ãè¾¼ã¿ã¯é‡è¦ã ãŒã‚³ãƒ”ãƒ¼ã‚ˆã‚Šä½ã‚ã«
    elif op == "networkconnect":
        base_score += 0.3  # å¤–éƒ¨é€šä¿¡ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ã‚’å¼·åŒ–
    elif op in ["processcreate", "processterminate"]:
        base_score += 0.2  # ãƒ—ãƒ­ã‚»ã‚¹ç³»ã¯å¾®å¢—ã§å·®åˆ¥åŒ–

    # å¤–éƒ¨ã¸ã®é€šä¿¡ãªã‚‰è¿½åŠ ã§ã•ã‚‰ã«å¼·åŒ–
    if destination and not destination.startswith(("192.168.", "10.", "172.")):
        base_score += 0.3

    # ç‰¹å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚ˆã‚Šé‡è¦è¦–
    if file_size_kb and file_size_kb > 50000:
        base_score += 0.3  # ç‰¹å¤§ãƒ•ã‚¡ã‚¤ãƒ«åŠ ç‚¹ã‚’å¼·åŒ–ï¼ˆ0.2â†’0.3ï¼‰

    # æ™‚é–“å¸¯ï¼ˆæ·±å¤œãƒ»æ—©æœï¼‰
    try:
        hour = int(event.get("timestamp", "00:00:00").split()[1].split(":")[0])
        if hour < 6 or hour > 22:
            base_score += 0.2  # æ·±å¤œå¸¯ã¯å¤‰æ›´ãªã—
    except Exception:
        pass

    # å¤±æ•—ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¯ãã®ã¾ã¾
    if status == "FAILED":
        base_score += 0.15

    # æœ€å¤§1.0ã«åˆ¶é™
    return min(base_score, 1.0)

# -------------------------------
# ğŸ”¥ çµ„ç¹”ã®SecurityEventChain ã‚¯ãƒ©ã‚¹ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰
# -------------------------------
class SecurityEventChain:
    def __init__(self, genesis_state_params=None, metadata=None,
                 tune_conf: ChainTuneConfig = ChainTuneConfig()):
        self.tune_conf = tune_conf # configã§ONæ™‚ã ã‘ç”Ÿæˆ
        self.scalable_backend = ScalableSecurityChain(tune_conf) if tune_conf.enable_scalable_backend else None
        self.feature_dim = len(genesis_state_params) if genesis_state_params else 5
        self.blocks = [self.get_genesis_block(genesis_state_params, metadata)]
        self.ema_filter = EMAFilter(alpha=tune_conf.ema_alpha_org)
        self.info_tensor = None
        self.cluster_model = None
        self.department = metadata.get("department") if metadata else None
        self.user_histories = {}
        self.resource_access_patterns = {}
        self.scaler = StandardScaler()
        self.is_fitted = False
        self.exceptional_model = None
        self.anomaly_model = None
        self.chain_context = { "pattern_history": {}, "anomaly_score": 0.0 }
        self.fork_alerts = []

    # === ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ©Ÿèƒ½APIï¼ˆçµ„ç¹”ãƒã‚§ãƒ¼ãƒ³ç”¨ãƒ»æ”¹è‰¯ç‰ˆï¼‰ ===

    def compress_chain_history(self):
        if not self.tune_conf.enable_scalable_backend or not self.scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        if not self.blocks:
            print("âš ï¸ çµ„ç¹”å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return None
        compressor = self.scalable_backend.implement_time_series_compression()
        data = np.array([block.state_params for block in self.blocks[-1000:]])
        if data.size == 0:
            print("âš ï¸ åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return None
        return compressor.compress_block(data)

    def aggregate_chain_stats(self, window=1000):
        if not self.tune_conf.enable_scalable_backend or not self.scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        if not self.blocks:
            return {"mean": 0, "std": 0}
        data = np.array([block.state_params for block in self.blocks[-window:]])
        if data.size == 0:
            return {"mean": 0, "std": 0}
        return { "mean": np.mean(data, axis=0), "std": np.std(data, axis=0) }

    def run_chain_mini_batch_clustering(self, batch_size=500):
        if not self.tune_conf.enable_scalable_backend or not self.scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        if len(self.blocks) < 2:
            print("âš ï¸ çµ„ç¹”ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return None
        clusters = self.scalable_backend.hierarchical_clustering()
        data = [block.state_params for block in self.blocks[-batch_size*5:]]
        return clusters.fit_incremental(data, batch_size=batch_size)

    def check_chain_bloom_duplicate(self, known_events, new_event):
        if not self.tune_conf.enable_scalable_backend or not self.scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        bloom = self.scalable_backend.implement_bloom_filter()
        for e in known_events:
            bloom.add(str(e))
        return bloom.contains(str(new_event))

    def adaptive_chain_sampling_run(self, event):
        if not self.tune_conf.enable_scalable_backend or not self.scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        sampler = self.scalable_backend.implement_adaptive_sampling()
        priority_score = calc_event_priority(event)  # ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ã‚’ä½¿ç”¨
        return sampler.should_sample(priority_score)

    def get_genesis_block(self, state_params=None, metadata=None):
        state_params = state_params or {
            "severity_level": 0.0,
            "action_magnitude": 0.0,
            "threat_context": 0.0,
            "trust_score": 1.0,
            "security_mode": 0.0
        }
        metadata = metadata or {
            "department": self.department,
            "created_at": datetime.now().isoformat()
        }
        return SecurityBlock(
            index=0,
            data="GENESIS",
            previous_hash="arbitrary",
            state_params=state_params,
            divergence=0.0,
            metadata=metadata
        )

    # === åŸºæº–å€¤è¨­è¨ˆ ===
    def set_normal_model(self, state_vector_list, n_clusters=3, min_cluster_size=10):
        """
        å€‹äººã¾ãŸã¯éƒ¨ç½²ã®æ­£å¸¸ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿æ•°è‡ªå‹•èª¿æ•´ä»˜ãï¼‰
        """
        # 1. å…¥åŠ›ã‚’npé…åˆ—ã¸
        if isinstance(state_vector_list, np.ndarray):
            vectors = state_vector_list
        elif state_vector_list and isinstance(state_vector_list[0], dict):
            vectors = np.array([vectorize_state(s) for s in state_vector_list])
        else:
            # å¿µã®ãŸã‚safe_values()é€šã—ã¦ã‚‚OK
            vectors = np.array([safe_values(v) for v in state_vector_list])

        self.feature_dim = vectors.shape[1]
        vectors_scaled = self.scaler.fit_transform(vectors)
        self.is_fitted = True

        # 2. æƒ…å ±ãƒ†ãƒ³ã‚½ãƒ«ã®è¨ˆç®—
        self.info_tensor = compute_information_tensor(vectors_scaled)

        # 3. ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’ãƒ‡ãƒ¼ã‚¿é‡ã§è‡ªå‹•èª¿æ•´ï¼ˆæœ€ä½1ã€æœ€å¤§n_clustersã€æ¨å¥¨: 10ä»¶/ã‚¯ãƒ©ã‚¹ã‚¿ç›®å®‰ï¼‰
        n_data = len(vectors_scaled)
        actual_clusters = min(n_clusters, max(1, n_data // min_cluster_size))
        self.cluster_model = fit_normal_clusters(vectors_scaled, n_clusters=actual_clusters)

    # === è¿½åŠ ã®æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–° ===
    def update_normal_model(self, additional_states):
        if self.cluster_model is None:
            # åˆå›å­¦ç¿’
            self.set_normal_model(additional_states)
        else:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµåˆã—ã¦å†å­¦ç¿’
            existing_states = []
            for block in self.blocks[1:]:
                if block.metadata.get("security_mode") == "normal":
                    existing_states.append(block.state_params)

            all_states = existing_states + additional_states
            self.set_normal_model(all_states, n_clusters=3)

    def update_anomaly_model(self, abnormal_states):
        """
        ç•°å¸¸ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆanomaly clusterï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ ã—ã¦å†å­¦ç¿’ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã€‚
        - abnormal_states: ç•°å¸¸ãªçŠ¶æ…‹ã®ãƒªã‚¹ãƒˆï¼ˆdictå½¢å¼ã®å ´åˆãƒ†ãƒ³ã‚½ãƒ«åŒ–ãŒå¿…è¦ï¼‰
        """
        # --- dictå‹ãªã‚‰vectorize_stateã§ãƒ†ãƒ³ã‚½ãƒ«åŒ– ---
        if abnormal_states and isinstance(abnormal_states[0], dict):
            vectors = np.array([vectorize_state(s) for s in abnormal_states])
        else:
            vectors = np.array(abnormal_states)

        if not hasattr(self, "anomaly_model") or self.anomaly_model is None:
            # åˆå›ãªã‚‰æ–°è¦ã‚¯ãƒ©ã‚¹ã‚¿å­¦ç¿’
            self.anomaly_model = fit_normal_clusters(vectors, n_clusters=2)
        else:
            existing_states = getattr(self, "anomaly_states", [])
            if existing_states and isinstance(existing_states[0], dict):
                existing_states = [vectorize_state(s) for s in existing_states]
            all_states = np.vstack([existing_states, vectors]) if len(existing_states) > 0 else vectors
            self.anomaly_model = fit_normal_clusters(all_states, n_clusters=2)
            self.anomaly_states = all_states

        self.anomaly_states = vectors  # æœ€æ–°çŠ¶æ…‹ã‚‚ä¸Šæ›¸ã

    def evaluate_by_cluster(self, new_state, weights_l1=None, weights_l2=None, config=None):
        """å¤–éƒ¨é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿è©•ä¾¡ï¼ˆconfigå¯¾å¿œï¼‰"""
        config = config or getattr(self, "tune_conf", None)
        if self.info_tensor is None or self.cluster_model is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒæœªè¨­å®šã§ã™")
        if config is not None:
            if weights_l1 is None:
                weights_l1 = config.weights_l1.copy()
            if weights_l2 is None:
                weights_l2 = config.weights_l2.copy()

        return evaluate_security_event_by_cluster(
            new_state, self.cluster_model, self.info_tensor, weights_l1, weights_l2, config=config
        )

    def evaluate_by_multi_cluster(self, new_state, weights_l1=None, weights_l2=None, config=None):
        """
        ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’è€ƒæ…®ã—ãŸãƒãƒ«ãƒã‚¯ãƒ©ã‚¹ã‚¿è©•ä¾¡
        """
        config = config or getattr(self, "tune_conf", None)
        vec = vectorize_state(new_state)
        if getattr(self, "is_fitted", False):  # self.is_fitted ãŒTrueãªã‚‰
            vec_scaled = self.scaler.transform(vec.reshape(1, -1))[0]
        else:
            vec_scaled = vec

        return classify_event_by_multi_clusters(
            vec_scaled,
            self.cluster_model,
            self.exceptional_model,
            self.anomaly_model,
            self.info_tensor,
            config,
            weights_l1,
            weights_l2
        )

    # ===== å…±é€šã®add_block_by_cluster_evalãƒ¡ã‚½ãƒƒãƒ‰ =====
    def add_block_by_cluster_eval(
        self, data, state_params, raw_event=None, step=None,
        system="SecurityMonitor", experiment="DivergenceCluster"
    ):
        """ã‚¯ãƒ©ã‚¹ã‚¿è©•ä¾¡ã«ã‚ˆã‚‹ãƒ–ãƒ­ãƒƒã‚¯è¿½åŠ ï¼ˆåŸºåº•å®Ÿè£…ï¼‰"""
        user_id = raw_event.get("user_id") if raw_event else "unknown"
        user_history = self.get_user_history(user_id)

        # 1. ã‚¯ãƒ©ã‚¹ã‚¿ï¼†divergenceï¼ˆå­ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å¯èƒ½ï¼‰
        score, cluster, verdict = self._evaluate_cluster(state_params, user_history, raw_event)

        # 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åˆ¤å®š
        mode, weighted_score, reason = self._apply_security_rules(
            score, verdict, state_params, raw_event, user_history
        )

        # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆå­ã‚¯ãƒ©ã‚¹ã§æ‹¡å¼µå¯èƒ½ï¼‰
        metadata = self._create_metadata(
            mode, weighted_score, step, system, experiment,
            cluster, score, verdict, reason, raw_event
        )

        # 4. ãƒ–ãƒ­ãƒƒã‚¯è¿½åŠ 
        add_block(self, data, state_params, divergence=score, metadata=metadata)

        # 5. å¾Œå‡¦ç†ï¼ˆå±¥æ­´æ›´æ–°ãªã©ï¼‰
        self._post_process(user_id, raw_event, mode, weighted_score)

        # 6. çµæœè¿”å´ï¼ˆå­ã‚¯ãƒ©ã‚¹ã§æ‹¡å¼µå¯èƒ½ï¼‰
        return self._create_result(mode, score, cluster, verdict, reason)

    def _evaluate_cluster(self, state_params, user_history, raw_event):
        """ã‚¯ãƒ©ã‚¹ã‚¿è©•ä¾¡ï¼ˆå­ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼‰"""
        result = self.evaluate_by_multi_cluster(state_params)
        score = result["best_divergence"]
        cluster = result["cluster"]
        verdict = result["verdict"]

        score = self.calculate_context_aware_divergence(score, user_history, raw_event)
        return score, cluster, verdict

    def _apply_security_rules(self, score, verdict, state_params, raw_event, user_history):
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ«ãƒ¼ãƒ«é©ç”¨"""
        if not hasattr(self, "chain_context"):
            self.chain_context = {"pattern_history": {}, "anomaly_score": 0.0}

        # åˆæœŸå€¤
        mode = verdict
        reason = ""

        if raw_event:
            trust_score = user_history.get("trust_score", 0.75)
            verdict2, reason = unified_security_judge(
                raw_event, user_history, self.chain_context,
                normal_div=score, trust_score=trust_score
            )
            if verdict2 != "normal":
                verdict = verdict2
                if verdict == "latent_suspicious":
                    score = max(score, 8.0)

        # æ™‚åˆ»ç³»
        timestamp = raw_event.get("timestamp", raw_event.get("event_time")) if raw_event else datetime.now()
        if isinstance(timestamp, str):
            event_time = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
        else:
            event_time = timestamp

        time_of_day = self.get_time_of_day(event_time.hour)
        dynamic_threshold = self.get_adaptive_threshold(time_of_day)

        # ãƒ¢ãƒ¼ãƒ‰åˆ†é¡
        mode, weighted_score = classify_security_mode_auto(
            score, state_params,
            previous_ema=self.ema_filter.value,
            config=self._get_config_dict(),
            threshold_override=dynamic_threshold,
            operation=raw_event.get("operation") if raw_event else None
        )

        # å†åº¦ãƒ«ãƒ¼ãƒ«åˆ¤å®šï¼ˆå„ªå…ˆåº¦ã«ã‚ˆã‚‹ä¸Šæ›¸ãï¼‰
        if raw_event:
            trust_score = user_history.get("trust_score", 0.75)
            verdict2, reason = unified_security_judge(
                raw_event, user_history, self.chain_context,
                normal_div=score, trust_score=trust_score
            )
            priority = {
                "normal": 0,
                "investigating": 1,
                "latent_suspicious": 2,
                "suspicious": 3,
                "critical": 4
            }
            if priority.get(verdict2, 0) > priority.get(mode, 0):
                print(f"[OVERRIDEâ†‘] {mode} â†’ {verdict2} by unified_security_judge: {reason}")
                mode = verdict2

        return mode, weighted_score, reason

    def _get_config_dict(self):
        """è¨­å®šè¾æ›¸å–å¾—ï¼ˆå­ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼‰"""
        return self.tune_conf.to_org_dict()

    def _create_metadata(self, mode, weighted_score, step, system, experiment,
                        cluster, score, verdict, reason, raw_event):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        time_of_day = None
        dynamic_threshold = None

        if raw_event:
            timestamp = raw_event.get("timestamp", raw_event.get("event_time"))
            if isinstance(timestamp, str):
                event_time = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
            else:
                event_time = timestamp or datetime.now()

            time_of_day = self.get_time_of_day(event_time.hour)
            dynamic_threshold = self.get_adaptive_threshold(time_of_day)

        metadata = {
            "security_mode": mode,
            "weighted_score": float(weighted_score),
            "step": step,
            "system": system,
            "experiment": experiment,
            "cluster_id": cluster,
            "divergence_score": score,
            "verdict": verdict,
            "department": self.department,
            "context_adjusted": True,
            "dynamic_threshold": dynamic_threshold
        }

        if raw_event:
            for k in [
                "event_time", "user_id", "target_resource", "event_type",
                "source_ip", "event_score", "action", "department",
                "timestamp", "operation", "file_path", "file_size_kb",
                "process_name", "destination_ip", "status"
            ]:
                if k in raw_event:
                    metadata[k] = raw_event[k]

        return metadata

    def _post_process(self, user_id, raw_event, mode, weighted_score):
        """å¾Œå‡¦ç†ï¼ˆå±¥æ­´æ›´æ–°ãªã©ï¼‰"""
        self.ema_filter.update(weighted_score)

        if raw_event:
            timestamp = raw_event.get("timestamp", raw_event.get("event_time"))
            if isinstance(timestamp, str):
                event_time = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
            else:
                event_time = timestamp or datetime.now()

            self.update_user_history(user_id, event_time, mode)

            if "file_path" in raw_event:
                self.update_resource_pattern(user_id, raw_event["file_path"], event_time)

    def _create_result(self, mode, score, cluster, verdict, reason):
        """çµæœè¾æ›¸ä½œæˆ"""
        return {
            "status": mode,
            "divergence": score,
            "added": True,
            "cluster_id": cluster,
            "verdict": verdict,
            "alert_level": "HIGH" if mode in ["critical", "suspicious", "latent_suspicious"] else "LOW",
            "reason": reason if verdict != "normal" else "",
        }

    def get_time_of_day(self, hour):
        """æ™‚é–“å¸¯ã‚’åˆ¤å®š"""
        if BUSINESS_HOURS["start"] <= hour < 12:
            return "morning"
        elif 12 <= hour < BUSINESS_HOURS["end"]:
            return "afternoon"
        elif BUSINESS_HOURS["end"] <= hour < 22:
            return "evening"
        else:
            return "night"

    def get_adaptive_threshold(self, time_of_day):
        """æ™‚é–“å¸¯ã«å¿œã˜ãŸé©å¿œçš„é–¾å€¤ã‚’å–å¾—"""
        dept_tbl = self.tune_conf.dept_threshold_table
        dept_thresholds = dept_tbl.get(self.department, {})
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚‚å®šæ•°åŒ–
        return dept_thresholds.get(time_of_day, THRESHOLDS["default_adaptive"])

    def calculate_context_aware_divergence(self, base_divergence, user_history, raw_event=None):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸDivergenceè¨ˆç®—"""
        adjusted_divergence = base_divergence

        # æœ€å¾Œã®æ­£å¸¸ã‚¢ã‚¯ã‚»ã‚¹ã‹ã‚‰ã®çµŒéæ™‚é–“ã«ã‚ˆã‚‹èª¿æ•´
        if user_history.get("last_normal_access"):
            hours_since_normal = (datetime.now() - user_history["last_normal_access"]).total_seconds() / 3600
            if hours_since_normal < NORMAL_ACCESS_GRACE_PERIOD:
                adjusted_divergence *= DIVERGENCE_MULTIPLIERS["recent_normal_access"]

        # ã‚¢ã‚¯ã‚»ã‚¹å›æ•°ã«ã‚ˆã‚‹èª¿æ•´
        if user_history.get("access_count", 0) > MIN_ACCESS_COUNT:
            adjusted_divergence *= DIVERGENCE_MULTIPLIERS["frequent_user"]

        if raw_event:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¢ã‚¯ã‚»ã‚¹é »åº¦ã«ã‚ˆã‚‹èª¿æ•´
            if "file_path" in raw_event:
                directory = self._extract_directory(raw_event["file_path"])
                if self.is_frequent_directory(user_history.get("user_id"), directory):
                    adjusted_divergence *= DIVERGENCE_MULTIPLIERS["frequent_directory"]

            # å®‰å…¨ãªãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã‚‹èª¿æ•´
            process = raw_event.get("process_name", "").lower()
            if process in SAFE_PROCESSES:
                adjusted_divergence *= DIVERGENCE_MULTIPLIERS["safe_process"]

            # ãƒ“ã‚¸ãƒã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã‚ˆã‚‹èª¿æ•´
            if "_business_context" in raw_event:
                contexts = raw_event["_business_context"]
                for context, multiplier in BUSINESS_CONTEXT_MULTIPLIERS.items():
                    if context in contexts:
                        adjusted_divergence *= multiplier

        return adjusted_divergence

    def update_resource_pattern(self, user_id, file_path, access_time):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨˜éŒ²"""
        directory = self._extract_directory(file_path)

        if user_id not in self.resource_access_patterns:
            self.resource_access_patterns[user_id] = {}

        if directory not in self.resource_access_patterns[user_id]:
            self.resource_access_patterns[user_id][directory] = {
                "count": 0,
                "first_access": access_time,
                "last_access": None
            }

        pattern = self.resource_access_patterns[user_id][directory]
        pattern["count"] += 1
        pattern["last_access"] = access_time

    def is_frequent_directory(self, user_id, directory):
        """é »ç¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ãƒã‚§ãƒƒã‚¯"""
        if user_id in self.resource_access_patterns:
            if directory in self.resource_access_patterns[user_id]:
                access_count = self.resource_access_patterns[user_id][directory]["count"]
                return access_count > FREQUENT_ACCESS_THRESHOLD
        return False

    def update_user_history(self, user_id, event_time, status):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¢ã‚¯ã‚»ã‚¹å±¥æ­´ã‚’æ›´æ–°"""
        if user_id not in self.user_histories:
            self.user_histories[user_id] = self._create_empty_user_history()

        history = self.user_histories[user_id]
        history["access_count"] += 1
        history["last_access"] = event_time

        if status == "normal":
            history["last_normal_access"] = event_time
            history["failed_attempts"] = 0
        elif status in CRITICAL_STATUSES:
            history["failed_attempts"] += 1
            history["last_failed"] = event_time

    def get_user_history(self, user_id):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å±¥æ­´ã‚’å–å¾—"""
        return self.user_histories.get(user_id, self._create_empty_user_history())

    def get_recent_alerts(self, n=10, min_severity="suspicious"):
        """æœ€è¿‘ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’å–å¾—"""
        min_level = SEVERITY_ORDER.get(min_severity, 1)
        alerts = []

        # æ¤œç´¢ç¯„å›²ã‚’åŠ¹ç‡åŒ–
        search_range = min(len(self.blocks), n * 2)

        for block in reversed(self.blocks[-search_range:]):
            if self._should_include_alert(block, min_level):
                alert = self._create_alert_from_block(block)
                alerts.append(alert)

                if len(alerts) >= n:
                    break

        return alerts

    def verify(self, verbose=True):
        """ãƒã‚§ãƒ¼ãƒ³ã®æ•´åˆæ€§ã‚’æ¤œè¨¼"""
        if len(self.blocks) < 2:
            if verbose:
                print("âœ… Chain has only genesis block")
            return True

        for i in range(1, len(self.blocks)):
            # å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if self.blocks[i].previous_hash != self.blocks[i-1].hash:
                if verbose:
                    print(f"âŒ Block {i} has invalid previous hash")
                    print(f"  Expected: {self.blocks[i-1].hash}")
                    print(f"  Got: {self.blocks[i].previous_hash}")
                return False

            # ãƒ–ãƒ­ãƒƒã‚¯è‡ªä½“ã®æ¤œè¨¼
            if not self.blocks[i].verify():
                if verbose:
                    print(f"âŒ Block {i} failed self-verification")
                return False

        if verbose:
            print(f"âœ… Chain is valid ({len(self.blocks)} blocks)")
        return True

    @classmethod
    def from_json(cls, filepath):
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒã‚§ãƒ¼ãƒ³ã‚’èª­ã¿è¾¼ã¿"""
        with open(filepath, "r", encoding="utf-8") as f:
            blocks_data = json.load(f)

        if not isinstance(blocks_data, list):
            raise ValueError("JSON file must contain a list of blocks")

        if not blocks_data:
            raise ValueError("JSON file contains no blocks")

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®åˆ¤å®š
        if all(isinstance(item, dict) for item in blocks_data):
            # LanScope Catå½¢å¼
            if self._is_lanscope_format(blocks_data):
                print("ğŸ§ª LanScope Cat format log detected. Building initial chain...")
                return build_initial_security_chain(blocks_data, SecurityLogData())

            # ãƒ¬ã‚¬ã‚·ãƒ¼å½¢å¼
            elif self._is_legacy_format(blocks_data):
                print("ğŸ§ª Legacy security event log detected. Building initial chain...")
                return build_initial_security_chain(blocks_data, SecurityLogData())

            # ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³å½¢å¼
            elif self._is_blockchain_format(blocks_data):
                return cls._from_blockchain_format(blocks_data)

        raise ValueError("Unknown data format in JSON file")

    @staticmethod
    def _serialize_anything(obj):
        import numpy as np
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, dict):
            return {k: SecurityEventChain._serialize_anything(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [SecurityEventChain._serialize_anything(x) for x in obj]
        else:
            return obj

    def export_to_json(self, filepath):
        blocks_data = []
        for block in self.blocks:
            block_data = {
                "index": block.index,
                "data": SecurityEventChain._serialize_anything(block.data),
                "previous_hash": block.previous_hash,
                "state_params": SecurityEventChain._serialize_anything(block.state_params),
                "divergence": block.divergence,
                "metadata": SecurityEventChain._serialize_anything(block.metadata),
            }
            blocks_data.append(block_data)

        with open(filepath, "w", encoding="utf-8") as f:
            import json
            json.dump(blocks_data, f, ensure_ascii=False, indent=2)

        print(f"âœ… Exported {len(blocks_data)} blocks to {filepath}")

    #===== ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ =====
    def _serialize_state_params(self, state_params):
        """
        state_paramsï¼ˆdictï¼‰ã‚’JSONã§ä¿å­˜å¯èƒ½ãªå½¢ã«å†å¸°çš„ã«å¤‰æ›ï¼ˆä¾‹ï¼šnp.ndarrayâ†’list, np.floatâ†’float ãªã©ï¼‰
        """
        def serialize(v):
            # ndarray â†’ list
            if isinstance(v, np.ndarray):
                return v.tolist()
            # NumPyã‚¹ã‚«ãƒ©ãƒ¼å‹ â†’ Pythonå‹
            elif isinstance(v, np.floating):
                return float(v)
            elif isinstance(v, np.integer):
                return int(v)
            # dict â†’ å†å¸°
            elif isinstance(v, dict):
                return {kk: serialize(vv) for kk, vv in v.items()}
            # list/tuple â†’ å†å¸°
            elif isinstance(v, (list, tuple)):
                return [serialize(x) for x in v]
            else:
                return v

        return {k: serialize(v) for k, v in state_params.items()}

    def _extract_directory(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŠ½å‡º"""
        return "\\".join(file_path.split("\\")[:-1])

    def _create_empty_user_history(self):
        """ç©ºã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å±¥æ­´ã‚’ä½œæˆ"""
        return {
            "access_count": 0,
            "last_normal_access": None,
            "last_access": None,
            "failed_attempts": 0,
            "last_failed": None
        }

    def _should_include_alert(self, block, min_level):
        """ã‚¢ãƒ©ãƒ¼ãƒˆã«å«ã‚ã‚‹ã¹ãã‹ãƒã‚§ãƒƒã‚¯"""
        mode = block.metadata.get("security_mode")
        if mode in SEVERITY_ORDER:
            return SEVERITY_ORDER[mode] >= min_level
        return False

    def _create_alert_from_block(self, block):
        """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ä½œæˆ"""
        alert = {
            "block_index": block.index,
            "time": block.metadata.get("timestamp", block.metadata.get("event_time", "Unknown")),
            "user": block.metadata.get("user_id", "Unknown"),
            "mode": block.metadata.get("security_mode"),
            "score": block.divergence,
            "department": block.metadata.get("department", "Unknown")
        }

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæƒ…å ±ã®è¿½åŠ 
        if "file_path" in block.metadata:
            alert["target"] = block.metadata["file_path"]
        elif "destination_ip" in block.metadata:
            alert["target"] = f"Network: {block.metadata['destination_ip']}"
        else:
            alert["target"] = block.metadata.get("target_resource", "Unknown")

        # æ“ä½œæƒ…å ±ã®è¿½åŠ 
        if "operation" in block.metadata:
            alert["operation"] = block.metadata["operation"]

        return alert

# -------------------------------
# ğŸ‘¤ UserSecurityChain ã‚¯ãƒ©ã‚¹ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰
# -------------------------------
class UserSecurityChain(SecurityEventChain):
    def __init__(self, user_id, department, tune_conf: ChainTuneConfig = ChainTuneConfig()):
        super().__init__(metadata={"user_id": user_id, "department": department}, tune_conf=tune_conf)
        self.user_id = user_id
        self.department = department
        self.scaler = StandardScaler()
        self.is_fitted = False
        self.ema_filter = EMAFilter(alpha=tune_conf.ema_alpha_user)
        self.fork_alerts = []
        self.baseline_behavior = {
            "typical_hours": [],
            "common_directories": [],
            "common_processes": [],
            "typical_locations": [],
            "operation_frequency": {},
            "file_size_patterns": {},
            "file_access_patterns": {}
        }

        # ğŸŸ© å€‹äººç”¨chain_contextï¼ˆéƒ¨ç½²ãƒ»ã‚°ãƒ­ãƒ¼ãƒãƒ«ã¨ç‹¬ç«‹å¯ï¼‰
        self.chain_context = { "pattern_history": {}, "anomaly_score": 0.0 }

        # ğŸŸ© ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«è£æ–¹ãƒã‚§ãƒ¼ãƒ³ï¼ˆå€‹äººç”¨ï¼‰ configã§ONæ™‚ã®ã¿ç”Ÿæˆ
        self.conf = tune_conf
        self.scalable_backend = ScalableSecurityChain(tune_conf) if tune_conf.enable_scalable_backend else None

    # ===== ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«å€‹äººAPIï¼ˆæ”¹è‰¯ç‰ˆï¼‰ =====
    def compress_user_history(self):
        if not self.conf.enable_scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        compressor = self.scalable_backend.implement_time_series_compression()
        # ç›´è¿‘100å›åˆ†ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’åœ§ç¸®ï¼ˆç©ºãªã‚‰Noneã‚’è¿”ã™ï¼‰
        if len(self.blocks) == 0:
            print("âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return None
        data = np.array([block.state_params for block in self.blocks[-100:]])
        if data.size == 0:
            print("âš ï¸ åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return None
        return compressor.compress_block(data)

    def aggregate_user_behavior(self):
        if not self.conf.enable_scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        if len(self.blocks) == 0:
            return {"mean": 0, "std": 0}
        data = np.array([block.state_params for block in self.blocks[-24:]])
        if data.size == 0:
            return {"mean": 0, "std": 0}
        return {
            "mean": np.mean(data, axis=0),
            "std": np.std(data, axis=0)
        }

    def run_user_mini_batch_clustering(self, batch_size=30):
        if not self.conf.enable_scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        clusters = self.scalable_backend.hierarchical_clustering()
        if len(self.blocks) < 2:
            print("âš ï¸ å€‹äººã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return None
        data = [block.state_params for block in self.blocks[-100:]]
        return clusters.fit_incremental(data, batch_size=batch_size)

    def check_bloom_duplicate(self, known_events, new_event):
        if not self.conf.enable_scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        bloom = self.scalable_backend.implement_bloom_filter()
        for e in known_events:
            bloom.add(str(e))
        return bloom.contains(str(new_event))

    def adaptive_sampling_run(self, event):
        if not self.conf.enable_scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        sampler = self.scalable_backend.implement_adaptive_sampling()
        priority_score = calc_event_priority(event)  # ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ã‚’ä½¿ç”¨
        return sampler.should_sample(priority_score)

    def set_normal_model(self, state_vector_list, n_clusters=3, min_cluster_size=10):
        """
        å€‹äººã¾ãŸã¯éƒ¨ç½²ã®æ­£å¸¸ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿æ•°è‡ªå‹•èª¿æ•´ä»˜ãï¼‰
        """
        # 1. å…¥åŠ›ã‚’npé…åˆ—ã¸
        if isinstance(state_vector_list, np.ndarray):
            vectors = state_vector_list
        elif state_vector_list and isinstance(state_vector_list[0], dict):
            vectors = np.array([vectorize_state(s) for s in state_vector_list])
        else:
            # å¿µã®ãŸã‚safe_values()é€šã—ã¦ã‚‚OK
            vectors = np.array([safe_values(v) for v in state_vector_list])

        self.feature_dim = vectors.shape[1]
        vectors_scaled = self.scaler.fit_transform(vectors)
        self.is_fitted = True

        # 2. æƒ…å ±ãƒ†ãƒ³ã‚½ãƒ«ã®è¨ˆç®—
        self.info_tensor = compute_information_tensor(vectors_scaled)

        # 3. ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’ãƒ‡ãƒ¼ã‚¿é‡ã§è‡ªå‹•èª¿æ•´ï¼ˆæœ€ä½1ã€æœ€å¤§n_clustersã€æ¨å¥¨: 10ä»¶/ã‚¯ãƒ©ã‚¹ã‚¿ç›®å®‰ï¼‰
        n_data = len(vectors_scaled)
        actual_clusters = min(n_clusters, max(1, n_data // min_cluster_size))
        self.cluster_model = fit_normal_clusters(vectors_scaled, n_clusters=actual_clusters)

    def set_abnormal_model(self, abnormal_states, n_clusters=2):
        """å¤–éƒ¨é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦é€¸è„±ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š"""
        vectors_scaled = self.scaler.transform(vectors) if self.is_fitted else vectors
        self.abnormal_model = fit_normal_clusters(vectors_scaled, n_clusters=n_clusters)

    def set_exceptional_model(self, exceptional_states, n_clusters=2):
        """å¤–éƒ¨é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ä¾‹å¤–ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š"""
        vectors = np.array([vectorize_state(s) for s in exceptional_states])
        vectors_scaled = self.scaler.transform(vectors) if self.is_fitted else vectors
        self.exceptional_model = fit_normal_clusters(vectors_scaled, n_clusters=n_clusters)

    def evaluate_by_multi_cluster(self, new_state, weights_l1=None, weights_l2=None):
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’è€ƒæ…®ã—ãŸãƒãƒ«ãƒã‚¯ãƒ©ã‚¹ã‚¿è©•ä¾¡"""
        vec = vectorize_state(new_state)
        if self.is_fitted:
            vec_scaled = self.scaler.transform(vec.reshape(1, -1))[0]
        else:
            vec_scaled = vec

        result = classify_event_by_multi_clusters(
            vec_scaled,
            self.cluster_model,
            self.exceptional_model,
            self.abnormal_model if hasattr(self, 'abnormal_model') else None,
            self.info_tensor,
            config=getattr(self, 'tune_conf', None),
            weights_l1=weights_l1,
            weights_l2=weights_l2
        )

        return result["verdict"], result["best_divergence"], result["cluster"]

    # ===== å€‹äººç”¨ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ =====
    def _evaluate_cluster(self, state_params, user_history, raw_event):
        """å€‹äººç”¨ã®ã‚¯ãƒ©ã‚¹ã‚¿è©•ä¾¡"""
        verdict, score, cluster = self.evaluate_by_multi_cluster(state_params)
        score = self.calculate_context_aware_divergence(score, user_history, raw_event)
        return score, cluster, verdict

    def _get_config_dict(self):
        """å€‹äººç”¨ã®è¨­å®šè¾æ›¸"""
        return self.tune_conf.to_user_dict()

    def _create_result(self, mode, score, cluster, verdict, reason):
        """å€‹äººç”¨ã®çµæœè¾æ›¸ï¼ˆdeviation_scoreã‚’è¿½åŠ ï¼‰"""
        result = super()._create_result(mode, score, cluster, verdict, reason)

        # å€‹äººç‰¹æœ‰ã®æƒ…å ±ã‚’è¿½åŠ 
        if hasattr(self, '_last_raw_event'):
            result["deviation_score"] = self.calculate_behavior_deviation(self._last_raw_event)
        else:
            result["deviation_score"] = 0.0

        return result

    def _post_process(self, user_id, raw_event, mode, weighted_score):
        """å€‹äººç”¨ã®å¾Œå‡¦ç†ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°ã‚’è¿½åŠ ï¼‰"""
        super()._post_process(user_id, raw_event, mode, weighted_score)

        # raw_eventã‚’ä¿å­˜ï¼ˆdeviation_scoreè¨ˆç®—ç”¨ï¼‰
        self._last_raw_event = raw_event

        # å€‹äººç‰¹æœ‰ã®å‡¦ç†
        if raw_event and "file_path" in raw_event:
            self.update_file_access_pattern(raw_event)

    # ===== å€‹äººç‰¹æœ‰ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰ =====

    def update_normal_model(self, additional_states):
        """
        è¿½åŠ ã®æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°
        """
        if self.cluster_model is None:
            # åˆå›å­¦ç¿’
            self.set_normal_model(additional_states)
        else:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµåˆã—ã¦å†å­¦ç¿’
            existing_states = []
            for block in self.blocks[1:]:
                if block.metadata.get("security_mode") == "normal":
                    existing_states.append(block.state_params)

            all_states = existing_states + additional_states
            self.set_normal_model(all_states, n_clusters=3)

    def calculate_behavior_deviation(self, event):
        """è¡Œå‹•é€¸è„±åº¦ã®è¨ˆç®—"""
        deviation_score = 0.0

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å‡¦ç†
        event_time = self._parse_event_time(event)
        hour = event_time.hour

        # 1. æ™‚é–“å¸¯ã®é€¸è„±ãƒã‚§ãƒƒã‚¯
        if self.baseline_behavior["typical_hours"]:
            if hour not in self.baseline_behavior["typical_hours"]:
                min_diff = min([abs(hour - h) for h in self.baseline_behavior["typical_hours"]])
                # æœ€å¤§12æ™‚é–“å·®ã‚’0-1ã«æ­£è¦åŒ–ã—ã€é‡ã¿ä»˜ã‘
                deviation_score += (min_diff / DEVIATION_SCORES["hour_diff_divisor"]) * DEVIATION_SCORES["hour_weight"]

        # 2. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¢ã‚¯ã‚»ã‚¹ã®é€¸è„±ãƒã‚§ãƒƒã‚¯
        file_path = event.get("file_path", "")
        if file_path and self.baseline_behavior["common_directories"]:
            directory = self._extract_directory(file_path)
            if directory not in self.baseline_behavior["common_directories"]:
                deviation_score += DEVIATION_SCORES["unknown_directory"]

                # é«˜ãƒªã‚¹ã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
                if self._is_high_risk_directory(directory):
                    deviation_score += DEVIATION_SCORES["high_risk_directory"]

        # 3. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç•°å¸¸ãƒã‚§ãƒƒã‚¯
        if file_path and "file_size_kb" in event:
            extension = self._extract_extension(file_path)
            file_size = event["file_size_kb"]

            if extension in self.baseline_behavior["file_size_patterns"]:
                sizes = self.baseline_behavior["file_size_patterns"][extension]
                if sizes:
                    avg_size = sum(sizes) / len(sizes)
                    if file_size > avg_size * FILE_SIZE_ANOMALY_MULTIPLIER:
                        deviation_score += DEVIATION_SCORES["abnormal_file_size"]

        # 4. ãƒ—ãƒ­ã‚»ã‚¹ã®é€¸è„±ãƒã‚§ãƒƒã‚¯
        process_name = event.get("process_name", "")
        if process_name and self.baseline_behavior["common_processes"]:
            if process_name not in self.baseline_behavior["common_processes"]:
                deviation_score += DEVIATION_SCORES["unknown_process"]

                # å±é™ºãªãƒ—ãƒ­ã‚»ã‚¹ã®ãƒã‚§ãƒƒã‚¯
                if self._is_dangerous_process(process_name):
                    deviation_score += DEVIATION_SCORES["dangerous_process"]

        # 5. æ“ä½œé »åº¦ã®é€¸è„±ãƒã‚§ãƒƒã‚¯
        operation = event.get("operation", "")
        if operation and self.baseline_behavior["operation_frequency"]:
            total_ops = sum(self.baseline_behavior["operation_frequency"].values())
            if total_ops > 0:
                op_freq = self.baseline_behavior["operation_frequency"].get(operation, 0)

                if op_freq == 0:
                    deviation_score += DEVIATION_SCORES["new_operation"]
                elif op_freq / total_ops < OPERATION_FREQUENCY_THRESHOLD:
                    deviation_score += DEVIATION_SCORES["rare_operation"]

        # 6. IPã‚¢ãƒ‰ãƒ¬ã‚¹ã®é€¸è„±ãƒã‚§ãƒƒã‚¯
        source_ip = event.get("source_ip", "")
        if source_ip and self.baseline_behavior["typical_locations"]:
            if source_ip not in self.baseline_behavior["typical_locations"]:
                if self._is_internal_ip(source_ip):
                    deviation_score += DEVIATION_SCORES["unknown_internal_ip"]
                else:
                    deviation_score += DEVIATION_SCORES["unknown_external_ip"]

        # ã‚¹ã‚³ã‚¢ã‚’0-1ã®ç¯„å›²ã«åˆ¶é™
        return min(deviation_score, DEVIATION_SCORES["max_score"])

    def learn_baseline(self, historical_events):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¡Œå‹•ã‚’å­¦ç¿’"""
        for event in historical_events:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å‡¦ç†
            event_time = self._parse_event_time(event)
            hour = event_time.hour

            # 1. å…¸å‹çš„ãªæ´»å‹•æ™‚é–“ã®å­¦ç¿’
            if hour not in self.baseline_behavior["typical_hours"]:
                self.baseline_behavior["typical_hours"].append(hour)

            # 2. ãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’
            file_path = event.get("file_path", "")
            if file_path:
                self._learn_file_patterns(file_path, event)

            # 3. ãƒ—ãƒ­ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’
            process_name = event.get("process_name", "")
            if process_name and process_name not in self.baseline_behavior["common_processes"]:
                self.baseline_behavior["common_processes"].append(process_name)

            # 4. æ“ä½œé »åº¦ã®å­¦ç¿’
            operation = event.get("operation", event.get("event_type", ""))
            if operation:
                self.baseline_behavior["operation_frequency"][operation] = \
                    self.baseline_behavior["operation_frequency"].get(operation, 0) + 1

            # 5. ã‚¢ã‚¯ã‚»ã‚¹å…ƒIPã®å­¦ç¿’
            source_ip = event.get("source_ip", "")
            if source_ip and source_ip not in self.baseline_behavior["typical_locations"]:
                self.baseline_behavior["typical_locations"].append(source_ip)

        # å­¦ç¿’å¾Œã®çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        self._print_baseline_summary()

    def update_file_access_pattern(self, event):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ›´æ–°"""
        file_path = event.get("file_path", "")
        if not file_path:
            return

        directory = self._extract_directory(file_path)
        extension = self._extract_extension(file_path)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆæœŸåŒ–
        if directory not in self.baseline_behavior["file_access_patterns"]:
            self.baseline_behavior["file_access_patterns"][directory] = {
                "count": 0,
                "extensions": {},
                "avg_size": 0,
                "last_access": None,
                "first_access": datetime.now()
            }

        pattern = self.baseline_behavior["file_access_patterns"][directory]

        # ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±ã®æ›´æ–°
        pattern["count"] += 1
        pattern["last_access"] = datetime.now()

        # æ‹¡å¼µå­åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
        if extension not in pattern["extensions"]:
            pattern["extensions"][extension] = 0
        pattern["extensions"][extension] += 1

        # å¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®æ›´æ–°ï¼ˆã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«è¨ˆç®—ï¼‰
        if "file_size_kb" in event:
            current_avg = pattern["avg_size"]
            new_size = event["file_size_kb"]
            # æ–°ã—ã„å¹³å‡ = (ç¾åœ¨ã®å¹³å‡ * (n-1) + æ–°ã—ã„å€¤) / n
            pattern["avg_size"] = (current_avg * (pattern["count"] - 1) + new_size) / pattern["count"]

        # ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ãŒé«˜ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç‰¹å®š
        if pattern["count"] >= FREQUENT_DIRECTORY_ACCESS_THRESHOLD:
            pattern["is_frequent"] = True

    def calculate_context_aware_divergence(self, base_divergence, user_history, raw_event=None):
        """å€‹äººã®æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸDivergenceè¨ˆç®—"""
        # è¦ªã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
        adjusted_divergence = super().calculate_context_aware_divergence(
            base_divergence, user_history, raw_event
        )

        if raw_event:
            # 1. é »ç¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ãƒã‚§ãƒƒã‚¯
            file_path = raw_event.get("file_path", "")
            if file_path:
                directory = self._extract_directory(file_path)
                if directory in self.baseline_behavior["file_access_patterns"]:
                    access_pattern = self.baseline_behavior["file_access_patterns"][directory]
                    access_count = access_pattern["count"]

                    # é »ç¹ãªã‚¢ã‚¯ã‚»ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ä¿¡é ¼åº¦ãŒé«˜ã„
                    if access_count > FREQUENT_DIRECTORY_ACCESS_THRESHOLD:
                        adjusted_divergence *= DIVERGENCE_MULTIPLIERS["personal_frequent_directory"]

                    # æœ€è¿‘ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚è€ƒæ…®
                    if access_pattern.get("last_access"):
                        hours_since_access = (datetime.now() - access_pattern["last_access"]).total_seconds() / 3600
                        if hours_since_access < RECENT_ACCESS_HOURS:
                            adjusted_divergence *= DIVERGENCE_MULTIPLIERS["recent_access"]

            # 2. ã‚ˆãä½¿ã†ãƒ—ãƒ­ã‚»ã‚¹ã‹ãƒã‚§ãƒƒã‚¯
            process = raw_event.get("process_name", "")
            if process in self.baseline_behavior["common_processes"]:
                op_freq = self.baseline_behavior["operation_frequency"]
                total_operations = sum(op_freq.values())

                # ååˆ†ãªæ“ä½œå±¥æ­´ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹
                if total_operations > THRESHOLDS["abnormal_access_count"]:
                    adjusted_divergence *= DIVERGENCE_MULTIPLIERS["trusted_user_process"]

            # 3. å€‹äººã®ä½œæ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è€ƒæ…®
            operation = raw_event.get("operation", "")
            if operation in self.baseline_behavior["operation_frequency"]:
                op_count = self.baseline_behavior["operation_frequency"][operation]
                total_ops = sum(self.baseline_behavior["operation_frequency"].values())

                if total_ops > 0 and op_count / total_ops > HIGH_FREQUENCY_OPERATION_RATIO:
                    # é »ç¹ã«è¡Œã†æ“ä½œã¯ä¿¡é ¼åº¦ãŒé«˜ã„
                    adjusted_divergence *= DIVERGENCE_MULTIPLIERS["frequent_operation"]

        return adjusted_divergence

    # ===== ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ =====
    def _parse_event_time(self, event):
        """ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡ºã—ã¦è§£æ"""
        timestamp = event.get("timestamp", event.get("event_time"))
        if isinstance(timestamp, str):
            return datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
        return timestamp or datetime.now()

    def _extract_directory(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŠ½å‡º"""
        return "\\".join(file_path.split("\\")[:-1])

    def _extract_extension(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰æ‹¡å¼µå­ã‚’æŠ½å‡º"""
        return file_path.split(".")[-1].lower() if "." in file_path else "unknown"

    def _is_high_risk_directory(self, directory):
        """é«˜ãƒªã‚¹ã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ãƒã‚§ãƒƒã‚¯"""
        directory_lower = directory.lower()
        return any(risk_path in directory_lower for risk_path in HIGH_RISK_PATHS)

    def _is_dangerous_process(self, process_name):
        """å±é™ºãªãƒ—ãƒ­ã‚»ã‚¹ã‹ãƒã‚§ãƒƒã‚¯"""
        process_lower = process_name.lower()
        return any(danger in process_lower for danger in DANGEROUS_PROCESSES)

    def _is_internal_ip(self, ip_address):
        """å†…éƒ¨IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‹ãƒã‚§ãƒƒã‚¯"""
        return ip_address.startswith(PRIVATE_IP_PREFIXES)

    def _learn_file_patterns(self, file_path, event):
        """ãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’"""
        directory = self._extract_directory(file_path)

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’
        if directory and directory not in self.baseline_behavior["common_directories"]:
            self.baseline_behavior["common_directories"].append(directory)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’
        file_size = event.get("file_size_kb", 0)
        extension = self._extract_extension(file_path)

        if extension not in self.baseline_behavior["file_size_patterns"]:
            self.baseline_behavior["file_size_patterns"][extension] = []
        self.baseline_behavior["file_size_patterns"][extension].append(file_size)

    def _print_baseline_summary(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å­¦ç¿’çµæœã®ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        print(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼ {self.user_id} ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å­¦ç¿’å®Œäº†:")
        print(f"  - æ´»å‹•æ™‚é–“å¸¯: {len(self.baseline_behavior['typical_hours'])}ãƒ‘ã‚¿ãƒ¼ãƒ³")
        print(f"  - å…±é€šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {len(self.baseline_behavior['common_directories'])}ç®‡æ‰€")
        print(f"  - ä½¿ç”¨ãƒ—ãƒ­ã‚»ã‚¹: {len(self.baseline_behavior['common_processes'])}ç¨®é¡")
        print(f"  - æ“ä½œç¨®åˆ¥: {len(self.baseline_behavior['operation_frequency'])}ç¨®é¡")
        print(f"  - ã‚¢ã‚¯ã‚»ã‚¹å…ƒIP: {len(self.baseline_behavior['typical_locations'])}ç®‡æ‰€")

# -----------------
# ğŸ”¥ çµ„ç¹”æ¨ªæ–­ãƒ»å…¨ä½“ç®¡ç†ã‚¯ãƒ©ã‚¹
# -----------------
class DepartmentSecurityChainManager:
    def __init__(self, tune_conf: ChainTuneConfig = ChainTuneConfig()):
        self.conf = tune_conf
        self.department_chains = {}
        self.user_department_map = {}
        self.security_log = SecurityLogData()
        self.cross_department_access = {}
        self.exceptional_patterns = {}
        self.user_chains = {}
        self.file_path_to_dept_map = {}
        # â˜…ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è£æ–¹ãƒã‚§ãƒ¼ãƒ³å°å…¥ï¼ˆãƒãƒƒãƒçµ±è¨ˆã‚„åœ§ç¸®/é›†ç´„/é«˜é€Ÿåˆ¤å®šã§åˆ©ç”¨å¯ï¼ï¼‰
        self.scalable_backend = ScalableSecurityChain(tune_conf)

    def initialize_department_chain(self, dept_name, normals, exc=None, ano=None):
        print(f"ğŸ¢ {dept_name}éƒ¨é–€ã®ãƒã‚§ãƒ¼ãƒ³ã‚’åˆæœŸåŒ–ä¸­...")
        chain = SecurityEventChain(metadata={"department": dept_name}, tune_conf=self.conf)
        chain.department = dept_name

        # --- configæ¸¡ã—ã‚’å¾¹åº• ---
        my_conf = self.conf  # æ˜ç¤ºï¼ˆèª­ã¿ã‚„ã™ã•ï¼†ä¿å®ˆæ€§ã®ãŸã‚ï¼‰

        # ã‚¹ãƒ†ãƒ¼ãƒˆå¤‰æ›ï¼†å­¦ç¿’
        normal_states = [security_event_to_state(e, self.security_log, config=my_conf) for e in normals]
        chain.set_normal_model(normal_states, n_clusters=3)
        print(f"  âœ… {len(normal_states)}ä»¶ã®æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’")

        if exc:
            exceptional_states = [security_event_to_state(e, self.security_log, config=my_conf) for e in exc]
            chain.exceptional_model = fit_normal_clusters(exceptional_states, n_clusters=2)
            print(f"  ğŸ“Œ {len(exceptional_states)}ä»¶ã®ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’")

        if ano:
            anomaly_states = [security_event_to_state(e, self.security_log, config=my_conf) for e in ano]
            chain.anomaly_model = fit_normal_clusters(anomaly_states, n_clusters=2)
            print(f"  ğŸš¨ {len(anomaly_states)}ä»¶ã®ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’")

        self.department_chains[dept_name] = chain
        self.scalable_backend.minute_aggregates.extend(normal_states)

    def process_event(self, event):
        """ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰"""
        user_id = event.get("user_id", "unknown")
        dept_name = event.get("department") or self.user_department_map.get(user_id, "unknown")

        # 1. ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
        if self._should_skip_as_exception(event, user_id, dept_name):
            return self._create_exception_result()

        # 2. æœªçŸ¥ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼/éƒ¨ç½²ãƒã‚§ãƒƒã‚¯
        if dept_name == "unknown" or dept_name not in self.department_chains:
            return self._create_unknown_result(user_id, dept_name)

        # 3. ã‚¤ãƒ™ãƒ³ãƒˆçŠ¶æ…‹ç”Ÿæˆ
        event_state = security_event_to_state(event, self.security_log, config=self.conf)
        self.scalable_backend.add_event_optimized(event_state)

        # 4. éƒ¨ç½²ãƒã‚§ãƒ¼ãƒ³å‡¦ç†
        result_dept = self._process_department_chain(event, event_state, user_id, dept_name)

        # 5. å€‹äººãƒã‚§ãƒ¼ãƒ³å‡¦ç†
        result_user = self._process_user_chain(event, event_state, user_id)

        # 6. çµæœçµ±åˆ
        return self._merge_results(result_dept, result_user, user_id, dept_name)

    def _should_skip_as_exception(self, event, user_id, dept_name):
        """ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã©ã†ã‹ã®åˆ¤å®š"""
        if self.is_exceptional_case(event) and user_id != "unknown" and dept_name != "unknown":
            source_ip = event.get("source_ip", "")
            return source_ip.startswith(PRIVATE_IP_PREFIXES)
        return False

    def _create_exception_result(self):
        """ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµæœä½œæˆ"""
        return {
            "status": "normal",
            "divergence": 5.0,
            "alert_level": "LOW",
            "reason": "ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³å¸å"
        }

    def _create_unknown_result(self, user_id, dept_name):
        """æœªçŸ¥ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼/éƒ¨ç½²ã®çµæœä½œæˆ"""
        print(f"âš ï¸ æœªçŸ¥ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼/éƒ¨ç½²: {user_id} / {dept_name}")
        return {
            "status": "suspicious",
            "divergence": 30.0,
            "alert_level": "HIGH",
            "reason": "Unknown user or department"
        }

    def _process_department_chain(self, event, event_state, user_id, dept_name):
        """éƒ¨ç½²ãƒã‚§ãƒ¼ãƒ³ã®å‡¦ç†"""
        dept_chain = self.department_chains[dept_name]
        dept_state = dict(event_state)

        # éƒ¨ç½²é–“ã‚¢ã‚¯ã‚»ã‚¹ãƒã‚§ãƒƒã‚¯
        is_cross_dept, accessed_dept = self.check_cross_department_access(dept_name, event)
        if is_cross_dept:
            dept_state["threat_context"] = min(dept_state["threat_context"] + 0.3, 1.0)
            dept_state["trust_score"] = max(dept_state["trust_score"] - 0.2, 0.0)

        result_dept = dept_chain.add_block_by_cluster_eval(
            data=f"Event_{user_id}_{event.get('timestamp', 'now')}",
            state_params=dept_state,
            raw_event=event
        )

        if is_cross_dept:
            self.record_cross_department_access(user_id, dept_name, accessed_dept, event)
            result_dept["cross_dept_warning"] = True
            result_dept["accessed_dept"] = accessed_dept

        return result_dept

    def _process_user_chain(self, event, event_state, user_id):
        """å€‹äººãƒã‚§ãƒ¼ãƒ³ã®å‡¦ç†"""
        if user_id in self.user_chains:
            user_chain = self.user_chains[user_id]
            user_state = dict(event_state)
            return user_chain.add_block_by_cluster_eval(
                data=f"UserEvent_{user_id}_{event.get('timestamp', 'now')}",
                state_params=user_state,
                raw_event=event
            )
        else:
            deviation_score = self.estimate_user_deviation(user_id, event)
            return {
                "status": "normal",
                "divergence": 0.0,
                "alert_level": "LOW",
                "deviation_score": deviation_score
            }

    def _merge_results(self, result_dept, result_user, user_id, dept_name):
        """éƒ¨ç½²ã¨å€‹äººã®çµæœã‚’çµ±åˆ"""
        merged_status = self.merge_status(
            result_dept["status"],
            result_user["status"],
            result_dept["divergence"],
            result_user["divergence"],
            result_user.get("deviation_score", 0.0),
            user_id in self.user_chains
        )

        merged_divergence = max(result_dept["divergence"], result_user["divergence"])
        merged_alert = "HIGH" if merged_status in ["suspicious", "critical"] else "LOW"

        return {
            "status": merged_status,
            "divergence": merged_divergence,
            "alert_level": merged_alert,
            "dept_result": result_dept,
            "user_result": result_user,
            "has_user_chain": user_id in self.user_chains,
            "cross_dept_access": result_dept.get("cross_dept_warning", False)
        }

    def check_cross_department_access(self, user_dept, event):
        """éƒ¨ç½²é–“ã‚¢ã‚¯ã‚»ã‚¹ã‚’åˆ¤å®šï¼ˆå®šæ•°ã‚’ä½¿ç”¨ï¼‰"""
        file_path = event.get("file_path", "")
        destination_ip = event.get("destination_ip", "")

        if file_path:
            path_lower = file_path.lower()

            # é«˜ãƒªã‚¹ã‚¯ãƒ‘ã‚¹ãƒã‚§ãƒƒã‚¯
            for risk_path in HIGH_RISK_PATHS:
                if risk_path in path_lower:
                    return True, "system"

            # éƒ¨ç½²ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯ï¼ˆDEPT_PATH_RULESã‹ã‚‰å–å¾—ï¼‰
            for dept, dept_info in DEPT_PATH_RULES.items():
                if dept != user_dept and "patterns" in dept_info:
                    for pattern in dept_info["patterns"]:
                        if pattern in path_lower:
                            return True, dept

        if destination_ip:
            # IPãƒ¬ãƒ³ã‚¸ãƒã‚§ãƒƒã‚¯
            for dept, ip_range in DEPT_IP_RANGES.items():
                if dept != user_dept and destination_ip.startswith(ip_range):
                    return True, dept

        return False, None

    def record_cross_department_access(self, user_id, from_dept, to_dept, event):
        """éƒ¨ç½²é–“ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨˜éŒ²"""
        key = f"{user_id}_{from_dept}_{to_dept}"

        if key not in self.cross_department_access:
            self.cross_department_access[key] = {
                "count": 0,
                "first_seen": datetime.now(),
                "last_seen": None,
                "file_paths": [],
                "operations": []
            }

        access_info = self.cross_department_access[key]
        access_info["count"] += 1
        access_info["last_seen"] = datetime.now()

        self._update_access_info(access_info, event)

    def _update_access_info(self, access_info, event):
        """ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±ã®æ›´æ–°"""
        if "file_path" in event:
            file_path = event["file_path"]
            if file_path not in access_info["file_paths"]:
                access_info["file_paths"].append(file_path)

        if "operation" in event:
            operation = event["operation"]
            if operation not in access_info["operations"]:
                access_info["operations"].append(operation)

    def detect_lateral_movement(self, time_window_minutes=30):
        """æ¨ªå±•é–‹æ”»æ’ƒã®æ¤œçŸ¥"""
        now = datetime.now()
        user_accesses = self._aggregate_recent_accesses(now, time_window_minutes)
        return self._identify_suspicious_users(user_accesses)

    def _aggregate_recent_accesses(self, now, time_window_minutes):
        """ç›´è¿‘ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’é›†è¨ˆ"""
        user_accesses = {}

        for key, access_info in self.cross_department_access.items():
            # ã‚­ãƒ¼ã®å®‰å…¨ãªåˆ†å‰²
            user_id, from_dept, to_dept = self._parse_access_key(key)
            if not user_id:
                continue

            # æ™‚é–“çª“å†…ã®ã‚¢ã‚¯ã‚»ã‚¹ã®ã¿
            if not self._is_within_time_window(access_info, now, time_window_minutes):
                continue

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±ã®æ›´æ–°
            if user_id not in user_accesses:
                user_accesses[user_id] = {
                    "departments": set(),
                    "file_count": 0,
                    "operations": set()
                }

            self._update_user_access_stats(user_accesses[user_id], from_dept, to_dept, access_info)

        return user_accesses

    def _parse_access_key(self, key):
        """ã‚¢ã‚¯ã‚»ã‚¹ã‚­ãƒ¼ã‚’å®‰å…¨ã«è§£æ"""
        parts = key.split("_")
        if len(parts) < 3:
            return None, None, None

        user_id = "_".join(parts[:-2])
        from_dept = parts[-2]
        to_dept = parts[-1]
        return user_id, from_dept, to_dept

    def _is_within_time_window(self, access_info, now, window_minutes):
        """ã‚¢ã‚¯ã‚»ã‚¹ãŒæ™‚é–“çª“å†…ã‹ãƒã‚§ãƒƒã‚¯"""
        if not access_info["last_seen"]:
            return False

        time_diff = (now - access_info["last_seen"]).total_seconds()
        return time_diff < window_minutes * 60

    def _update_user_access_stats(self, user_stats, from_dept, to_dept, access_info):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚»ã‚¹çµ±è¨ˆã®æ›´æ–°"""
        user_stats["departments"].add(from_dept)
        user_stats["departments"].add(to_dept)
        user_stats["file_count"] += len(access_info["file_paths"])
        user_stats["operations"].update(access_info["operations"])

    def _identify_suspicious_users(self, user_accesses):
        """ç–‘ã‚ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç‰¹å®š"""
        suspicious_users = []

        for user_id, access_data in user_accesses.items():
            unique_depts = access_data["departments"]
            if len(unique_depts) > 2:
                risk_score = self._calculate_risk_score(unique_depts, access_data["operations"])

                suspicious_users.append({
                    "user_id": user_id,
                    "departments_accessed": list(unique_depts),
                    "file_access_count": access_data["file_count"],
                    "operations": list(access_data["operations"]),
                    "risk_score": risk_score
                })

        return suspicious_users

    def _calculate_risk_score(self, departments, operations):
        """ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        # éƒ¨ç½²æ•°ã«åŸºã¥ãåŸºæœ¬ã‚¹ã‚³ã‚¢
        base_score = min(len(departments) * 0.3, 1.0)

        # å±é™ºãªæ“ä½œã®ãƒã‚§ãƒƒã‚¯
        dangerous_ops = ["FileDelete", "FileMove", "FileCopy"]
        if any(op in operations for op in dangerous_ops):
            base_score = min(base_score + 0.2, 1.0)

        return base_score

    def merge_status(self, status_dept, status_user, dept_divergence, user_divergence, deviation_score, has_user_chain):
        """éƒ¨ç½²ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’çµ±åˆ"""
        order = ["normal", "investigating", "latent_suspicious", "suspicious", "critical"]

        # å€‹äººãƒã‚§ãƒ¼ãƒ³ãŒã‚ã‚‹å ´åˆã®è£œæ­£
        if has_user_chain and deviation_score > 0.7:
            if user_divergence > 15.0:
                return "suspicious"
            elif user_divergence > 10.0:
                return "investigating"

        # ä¸¡æ–¹suspiciousä»¥ä¸Šãªã‚‰ critical
        dept_idx = order.index(status_dept)
        user_idx = order.index(status_user)

        if dept_idx >= order.index("suspicious") and user_idx >= order.index("suspicious"):
            return "critical"

        # æœ€ã‚‚é‡ã„åˆ¤å®šã‚’æ¡ç”¨
        return order[max(dept_idx, user_idx)]

    def is_exceptional_case(self, event):
        """ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ¤å®š"""
        patterns = self.exceptional_patterns.get(event.get("department"), [])

        for pattern in patterns:
            if self._matches_pattern(event, pattern):
                return True

        return False

    def _matches_pattern(self, event, pattern):
        """ã‚¤ãƒ™ãƒ³ãƒˆãŒãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        match_count = 0
        required_matches = 0

        # å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒãƒƒãƒãƒ³ã‚°ãƒã‚§ãƒƒã‚¯
        fields_to_check = [
            ("user_id", lambda e, p: e.get("user_id") == p["user_id"]),
            ("file_path", lambda e, p: "file_path" in p and "file_path" in e and p["file_path"] in e["file_path"]),
            ("operation", lambda e, p: e.get("operation") == p["operation"]),
            ("_business_context", lambda e, p: "_business_context" in p and "_business_context" in e and
                                              set(p["_business_context"]).intersection(set(e["_business_context"])))
        ]

        for field, check_func in fields_to_check:
            if field in pattern:
                required_matches += 1
                if check_func(event, pattern):
                    match_count += 1

        # 70%ä»¥ä¸Šãƒãƒƒãƒã—ãŸã‚‰è©²å½“
        return required_matches > 0 and match_count / required_matches >= 0.7

    def estimate_user_deviation(self, user_id, event):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é€¸è„±åº¦ã‚’æ¨å®š"""
        if user_id in self.user_chains:
            return self.user_chains[user_id].calculate_behavior_deviation(event)
        return 0.0

    def get_department_summary(self):
        """éƒ¨ç½²åˆ¥ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        summary = {}
        for dept_name, chain in self.department_chains.items():
            summary[dept_name] = self._create_dept_summary(chain)
        return summary

    def _create_dept_summary(self, chain):
        """å€‹åˆ¥éƒ¨ç½²ã®ã‚µãƒãƒªãƒ¼ä½œæˆ"""
        recent_alerts = chain.get_recent_alerts(n=5)
        operation_stats = self._calculate_operation_stats(chain)

        return {
            "total_events": len(chain.blocks) - 1,
            "recent_alerts": len(recent_alerts),
            "alert_details": recent_alerts,
            "chain_valid": chain.verify(verbose=False),
            "operation_stats": operation_stats
        }

    def _calculate_operation_stats(self, chain):
        """æ“ä½œçµ±è¨ˆã®è¨ˆç®—"""
        operation_stats = {}
        for block in chain.blocks[-100:]:
            if "operation" in block.metadata:
                op = block.metadata["operation"]
                operation_stats[op] = operation_stats.get(op, 0) + 1
        return operation_stats

    # ===== ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ©Ÿèƒ½ï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰ =====
    def adaptive_sampling_run(self, event):
        if not self.conf.enable_scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        sampler = self.scalable_backend.implement_adaptive_sampling()
        priority_score = calc_event_priority(event)  # ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ã‚’ä½¿ç”¨
        return sampler.should_sample(priority_score)

    def compress_monthly_data(self):
        if not self.conf.enable_scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        compressor = self.scalable_backend.implement_time_series_compression()
        data = np.array(self.scalable_backend.day_aggregates)
        if data.size == 0:
            print("âš ï¸ åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã ã‚ˆï¼")
            return None
        return compressor.compress_block(data)

    def aggregate_daily_stats(self):
        if not self.conf.enable_scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        data = np.array(self.scalable_backend.hour_aggregates)
        if data.size == 0:
            return {"mean": 0, "std": 0}
        return {"mean": np.mean(data, axis=0), "std": np.std(data, axis=0)}

    def run_mini_batch_clustering(self, batch_size=500):
        if not self.conf.enable_scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        clusters = self.scalable_backend.hierarchical_clustering()
        # --- dictå‹ãªã‚‰vectorize_stateã§ãƒ™ã‚¯ãƒˆãƒ«åŒ– ---
        data = list(self.scalable_backend.minute_aggregates)
        if data and isinstance(data[0], dict):
            vectors = np.array([vectorize_state(s) for s in data])
        else:
            vectors = np.array(data)
        if vectors.size == 0:
            raise ValueError("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ï¼")
        return clusters.fit_incremental(vectors, batch_size=batch_size)

    def check_bloom_duplicate(self, known_events, new_event):
        if not self.conf.enable_scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        bloom = self.scalable_backend.implement_bloom_filter()
        for e in known_events:
            bloom.add(str(e))
        return bloom.contains(str(new_event))

    def parallel_reaggregate(self):
        if not self.conf.enable_scalable_backend:
            raise RuntimeError("ScalableBackendæ©Ÿèƒ½ã¯configã§OFFã§ã™ï¼")
        events_by_dept = {
            dept: [block.state_params for block in chain.blocks]
            for dept, chain in self.department_chains.items()
        }
        return self.scalable_backend.parallel_department_processing(events_by_dept)

    def initialize_user_chain(self, user_id, department, historical_events=None):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒã‚§ãƒ¼ãƒ³ã®åˆæœŸåŒ–"""
        print(f"ğŸ‘¤ {user_id}ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒã‚§ãƒ¼ãƒ³ã‚’åˆæœŸåŒ–ä¸­...")

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
        user_chain = UserSecurityChain(user_id, department, tune_conf=self.conf)

        # å±¥æ­´ã‚¤ãƒ™ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å­¦ç¿’
        if historical_events:
            # æ­£å¸¸ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿æŠ½å‡ºã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å­¦ç¿’
            normal_events = [e for e in historical_events if e.get("expected", "normal") == "normal"]
            if normal_events:
                user_chain.learn_baseline(normal_events)

                # æ­£å¸¸ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
                normal_states = [
                    security_event_to_state(e, self.security_log, config=self.conf)
                    for e in normal_events
                ]
                user_chain.set_normal_model(normal_states, n_clusters=3)
                print(f"  âœ… {len(normal_events)}ä»¶ã®æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’")

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒã‚§ãƒ¼ãƒ³ã‚’ç™»éŒ²
        self.user_chains[user_id] = user_chain
        self.user_department_map[user_id] = department

        return user_chain
# -----------------
# ğŸ”¥ ãƒ¬ãƒãƒ¼ãƒ†ã‚£ãƒ³ã‚°
# -----------------
    def get_cross_department_report(self, time_window_minutes=30):
        """éƒ¨ç½²é–“ã‚¢ã‚¯ã‚»ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report = {
            "summary": {
                "total_cross_dept_accesses": len(self.cross_department_access),
                "suspicious_users": [],
                "department_matrix": {}
            },
            "details": []
        }

        # æ¨ªå±•é–‹æ”»æ’ƒã®æ¤œçŸ¥
        suspicious_users = self.detect_lateral_movement(time_window_minutes)
        report["summary"]["suspicious_users"] = suspicious_users

        # éƒ¨ç½²é–“ã‚¢ã‚¯ã‚»ã‚¹ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã®ä½œæˆ
        dept_matrix = {}
        for key, access_info in self.cross_department_access.items():
            _, from_dept, to_dept = self._parse_access_key(key)
            if from_dept and to_dept:
                if from_dept not in dept_matrix:
                    dept_matrix[from_dept] = {}
                if to_dept not in dept_matrix[from_dept]:
                    dept_matrix[from_dept][to_dept] = 0
                dept_matrix[from_dept][to_dept] += access_info["count"]

        report["summary"]["department_matrix"] = dept_matrix

        # è©³ç´°æƒ…å ±ã®è¿½åŠ 
        for key, access_info in self.cross_department_access.items():
            user_id, from_dept, to_dept = self._parse_access_key(key)
            if user_id:
                detail = {
                    "user_id": user_id,
                    "from_department": from_dept,
                    "to_department": to_dept,
                    "access_count": access_info["count"],
                    "first_seen": access_info["first_seen"].isoformat() if access_info["first_seen"] else None,
                    "last_seen": access_info["last_seen"].isoformat() if access_info["last_seen"] else None,
                    "unique_files": len(access_info["file_paths"]),
                    "operations": list(set(access_info["operations"]))
                }
                report["details"].append(detail)

        return report

    def analyze_department_patterns(self, department, time_range_hours=24):
        """éƒ¨ç½²ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        if department not in self.department_chains:
            return {"error": f"Department {department} not found"}

        chain = self.department_chains[department]
        now = datetime.now()
        cutoff_time = now - timedelta(hours=time_range_hours)

        analysis = {
            "department": department,
            "time_range": f"Last {time_range_hours} hours",
            "patterns": {
                "operations": {},
                "file_access": {},
                "time_distribution": {},
                "alert_summary": {
                    "critical": 0,
                    "suspicious": 0,
                    "investigating": 0,
                    "normal": 0
                }
            }
        }

        # æŒ‡å®šæ™‚é–“ç¯„å›²å†…ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆ†æ
        for block in chain.blocks:
            if "timestamp" in block.metadata:
                block_time = datetime.strptime(block.metadata["timestamp"], "%Y-%m-%d %H:%M:%S")
                if block_time < cutoff_time:
                    continue

                # æ“ä½œçµ±è¨ˆ
                operation = block.metadata.get("operation", "unknown")
                analysis["patterns"]["operations"][operation] = \
                    analysis["patterns"]["operations"].get(operation, 0) + 1

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹çµ±è¨ˆ
                if "file_path" in block.metadata:
                    file_path = block.metadata["file_path"]
                    directory = "\\".join(file_path.split("\\")[:-1])
                    analysis["patterns"]["file_access"][directory] = \
                        analysis["patterns"]["file_access"].get(directory, 0) + 1

                # æ™‚é–“åˆ†å¸ƒ
                hour = block_time.hour
                hour_key = f"{hour:02d}:00-{hour:02d}:59"
                analysis["patterns"]["time_distribution"][hour_key] = \
                    analysis["patterns"]["time_distribution"].get(hour_key, 0) + 1

                # ã‚¢ãƒ©ãƒ¼ãƒˆã‚µãƒãƒªãƒ¼
                mode = block.metadata.get("security_mode", "normal")
                if mode in analysis["patterns"]["alert_summary"]:
                    analysis["patterns"]["alert_summary"][mode] += 1

        return analysis

# ===== 1. build_initial_security_chainé–¢æ•° =====
def build_initial_security_chain(events_data, security_log, config=None):
    """
    ç”Ÿã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰åˆæœŸãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
    SecurityEventChainï¼ˆçµ„ç¹”ãƒã‚§ãƒ¼ãƒ³ï¼‰ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™
    """
    if config is None:
        config = ChainTuneConfig()

    # çµ„ç¹”ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ
    chain = SecurityEventChain(tune_conf=config)

    print(f"ğŸ“Š åˆæœŸãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰é–‹å§‹: {len(events_data)}ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆ")

    for i, event in enumerate(events_data):
        # ã‚¹ãƒ†ãƒ¼ãƒˆå¤‰æ›
        state = security_event_to_state(event, security_log, config=config)

        # âœ¨ ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯è£œæ­£
        operation = event.get("operation", "")
        file_path = event.get("file_path", "").lower()
        file_size = event.get("file_size_kb", 0)
        status = event.get("status", "")

        if "FAILED" in status:
            state["normal_div"] *= 1.25
        if "confidential" in file_path:
            state["ano_div"] *= 1.20
        if operation == "FileCopy" and file_size > 100000:
            state["ano_div"] *= 1.3

        # åˆæœŸå­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º
        if i == INITIAL_LEARNING_THRESHOLD:
            states = [block.state_params for block in chain.blocks[1:]]
            if len(states) > INITIAL_CLUSTER_COUNT:
                print(f"  ğŸ¯ {i}ä»¶ç›®ã§æ­£å¸¸ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’é–‹å§‹...")
                chain.set_normal_model(states, n_clusters=3)

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ add_block_by_cluster_eval ã‚’ä½¿ç”¨
        if chain.cluster_model is not None:
            chain.add_block_by_cluster_eval(
                data=f"Event_{i}",
                state_params=state,
                raw_event=event,
                step=i
            )
        else:
            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å‰ã¯ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ–ãƒ­ãƒƒã‚¯è¿½åŠ 
            add_block(
                chain,
                data=f"Event_{i}",
                state_params=state,
                divergence=0.0,
                metadata=_create_initial_metadata(event, i)
            )

    return chain

    def _create_initial_metadata(event, index):
        """åˆæœŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
        metadata = {
            "index": index,
            "timestamp": event.get("timestamp", datetime.now().isoformat()),
            "initial_load": True
        }

        # ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰ä¸»è¦ãªæƒ…å ±ã‚’ã‚³ãƒ”ãƒ¼
        for key in ["user_id", "department", "operation", "file_path", "source_ip"]:
            if key in event:
                metadata[key] = event[key]

        return metadata

    def add_block_by_cluster_eval(
        self,
        data,
        state_params,
        raw_event=None,
        step=None,
        system="PersonalSecurityMonitor",
        experiment="UserDivergenceCluster"
    ):
        """å€‹äººç”¨ã®ã‚¯ãƒ©ã‚¹ã‚¿è©•ä¾¡ã«ã‚ˆã‚‹ãƒ–ãƒ­ãƒƒã‚¯è¿½åŠ ï¼ˆç‰¹ä¾‹åˆ¤å®šå«ã‚€ï¼‰"""
        user_history = self.get_user_history(self.user_id)

        # 1. ã‚¯ãƒ©ã‚¹ã‚¿ï¼†divergence
        verdict, score, cluster = self.evaluate_by_multi_cluster(state_params)
        score = self.calculate_context_aware_divergence(score, user_history, raw_event)

        # 2. unified_security_judge ã§ç‰¹ä¾‹åˆ¤å®š
        if not hasattr(self, "chain_context"):
            self.chain_context = {"pattern_history": {}, "anomaly_score": 0.0}

        if raw_event:
            # å¿…è¦ã«å¿œã˜ã¦normal_div/scoreã‚„trust_scoreï¼ˆäºˆæ¸¬ãƒ»æ—¢å­˜å€¤ï¼‰ã‚‚å¼•æ•°ã§æ¸¡ã›ã‚‹
            normal_div = score
            trust_score = user_history.get("trust_score", 0.75)
            verdict2, reason = unified_security_judge(
                raw_event, user_history, self.chain_context, normal_div, trust_score
            )
            if verdict2 != "normal":
                verdict = verdict2
                if verdict == "latent_suspicious":
                    score = max(score, 8.0)

        # 3. æ™‚åˆ»ç³»
        timestamp = raw_event.get("timestamp", raw_event.get("event_time")) if raw_event else datetime.now()
        if isinstance(timestamp, str):
            event_time = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
        else:
            event_time = timestamp

        time_of_day = self.get_time_of_day(event_time.hour)
        dynamic_threshold = self.get_adaptive_threshold(time_of_day)

        # 4. ãƒ¢ãƒ¼ãƒ‰åˆ†é¡
        mode, weighted_score = classify_security_mode_auto(
            divergence=score,
            state_params=state_params,
            previous_ema=self.ema_filter.value,
            config=self.tune_conf.to_user_dict(),
            threshold_override=dynamic_threshold,
            operation=operation
        )

        # 2. unified_security_judge ã§æ§‹é€ ãƒ«ãƒ¼ãƒ«åˆ¤å®š
        if raw_event:
            trust_score = user_history.get("trust_score", 0.75)
            verdict2, reason = unified_security_judge(
                raw_event, user_history, self.chain_context,
                normal_div=score,
                trust_score=trust_score
            )
            priority = {
                "normal": 0,
                "investigating": 1,
                "latent_suspicious": 2,
                "suspicious": 3,
                "critical": 4
            }
            if priority.get(verdict2, 0) > priority.get(mode, 0):
                print(f"[OVERRIDEâ†‘] {mode} â†’ {verdict2} by unified_security_judge: {reason}")
                mode = verdict2

        self.ema_filter.update(weighted_score)
        self.update_user_history(self.user_id, event_time, mode)

        if raw_event and "file_path" in raw_event:
            self.update_file_access_pattern(raw_event)

        metadata = {
            "security_mode": mode,
            "weighted_score": float(weighted_score),
            "step": step,
            "system": system,
            "experiment": experiment,
            "cluster_id": cluster,
            "divergence_score": score,
            "verdict": verdict,
            "department": self.department,
            "context_adjusted": True,
            "dynamic_threshold": dynamic_threshold
        }
        if raw_event:
            for k in [
                "event_time", "user_id", "target_resource", "event_type",
                "source_ip", "event_score", "action", "department",
                "timestamp", "operation", "file_path", "file_size_kb",
                "process_name", "destination_ip", "status"
            ]:
                if k in raw_event:
                    metadata[k] = raw_event[k]

        add_block(self, data, state_params, divergence=score, metadata=metadata)

        return {
            "status": mode,
            "divergence": score,
            "added": True,
            "cluster_id": cluster,
            "deviation_score": self.calculate_behavior_deviation(raw_event) if raw_event else 0.0,
            "alert_level": "HIGH" if mode in ["critical", "suspicious", "latent_suspicious"] else "LOW",
            "verdict": verdict,
            "reason": reason if verdict != "normal" else "",
        }

# -----------------
# ğŸ”¥ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒã‚§ãƒ¼ãƒ³
# -----------------
class ScalableSecurityChain:
    """
    ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æœ€é©åŒ–æ¸ˆã¿ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒ¼ãƒ³
    - ã‚³ãƒ³ãƒ•ã‚£ã‚°é§†å‹•
    - éšå±¤é›†ç´„/åœ§ç¸®/ä¸¦åˆ—/Bloom/é©å¿œã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å†…è”µ
    """

    def __init__(self, tune_conf):
        self.conf = tune_conf
        self.max_memory_mb = tune_conf.max_memory_mb
        self.cache_size = tune_conf.cache_size

        # éšå±¤çš„é›†ç´„
        self.minute_aggregates = deque(maxlen=tune_conf.block_history_min)
        self.hour_aggregates   = deque(maxlen=tune_conf.block_history_hr)
        self.day_aggregates    = deque(maxlen=tune_conf.block_history_day)

        # LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.divergence_cache = OrderedDict()

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.user_index = {}
        self.time_index = {}
        self.anomaly_index = []

        # åœ§ç¸®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.compressed_blocks = []
        self.active_blocks = deque(maxlen=1000)

    def serialize_event(event):
        """eventè¾æ›¸ä¸­ã®np.ndarrayã‚’listã«å¤‰æ›ã—ã¦è¿”ã™"""
        def convert(x):
            if isinstance(x, np.ndarray):
                return x.tolist()
            elif isinstance(x, dict):
                return {k: convert(v) for k, v in x.items()}
            elif isinstance(x, list):
                return [convert(i) for i in x]
            else:
                return x
        return convert(event)

    def add_event_optimized(self, event):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥+ä¸¦åˆ—åŒ–+ãƒãƒƒãƒæŠ•å…¥"""
        event_str = str(event)
        event_hash = self.compute_event_hash(event)
        if event_hash in self.divergence_cache:
            return self.divergence_cache[event_hash]

        # éåŒæœŸãƒãƒƒãƒå‡¦ç†ã‚’èµ·å‹•
        asyncio.create_task(self._async_tensor_conversion(event))
        asyncio.create_task(self._async_rule_check(event))
        self._add_to_batch_queue(event)
        return event_hash

    @staticmethod
    def compute_event_hash(event):
        """ã‚¤ãƒ™ãƒ³ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒãƒƒã‚·ãƒ¥åŒ–ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯IDåŒ–ï¼ˆndarrayå¯¾å¿œï¼‰"""
        def convert(x):
            if isinstance(x, np.ndarray):
                return x.tolist()
            elif isinstance(x, dict):
                return {k: convert(v) for k, v in x.items()}
            elif isinstance(x, list):
                return [convert(i) for i in x]
            else:
                return x
        event_serializable = convert(event)
        event_str = json.dumps(event_serializable, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(event_str.encode('utf-8')).hexdigest()

    async def _async_tensor_conversion(self, event):
        action_vector = operation_type_vector(event.get("operation", "FileRead"))
        """éåŒæœŸãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›ï¼ˆä¾‹:ãƒ™ã‚¯ãƒˆãƒ«åŒ–, PCAç­‰ï¼‰"""
        tensor = np.empty(6, dtype=np.float32)
        tensor[0] = event.get('event_score', 0) / 100.0
        tensor[1] = self._calculate_severity_vectorized(event)
        tensor[2] = float(event.get("file_size_kb", 0)) / 1e6  # MBå˜ä½ã§æ­£è¦åŒ–
        tensor[3] = 1.0 if event.get("status", "") == "FAILED" else 0.0
        tensor[4] = self._operation_type_vector(event.get("operation", ""))
        tensor[5] = self._risk_path_flag(event.get("file_path", ""))
        return tensor

    def _calculate_severity_vectorized(self, event):
        # é‡å¤§åº¦è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã‚’å¿…è¦ã«å¿œã˜ã¦å·®ã—æ›¿ãˆ
        return event.get("severity_level", 0.0)

    def _add_to_batch_queue(self, event):
        if not hasattr(self, "batch_queue"):
            self.batch_queue = []
        self.batch_queue.append(event)
        # ã‚‚ã—ãƒãƒƒãƒ•ã‚¡ãŒNå€‹æºœã¾ã£ãŸã‚‰ãƒãƒƒãƒå‡¦ç†èµ·å‹•ã€ã¨ã‹ã‚‚å¯èƒ½

    # ======================
    # ä¸‹å±¤ã‚µãƒ–æ§‹é€ ã‚‚configé§†å‹•ã§ãƒªãƒ•ã‚¡ã‚¯ã‚¿
    # ======================

    def implement_sliding_window(self, window_size=None):
        """çµ±è¨ˆçš„ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦"""
        win_size = window_size or self.conf.cache_size
        class SlidingWindowChain:
            def __init__(self, size):
                self.window = deque(maxlen=size)
                self.mean = np.zeros(6)
                self.std = np.ones(6)
                self.count = 0

            def update(self, new_event):
                old_mean = self.mean.copy()
                self.count += 1
                delta = new_event - old_mean
                self.mean += delta / self.count
                self.std = np.sqrt(
                    (self.std**2 * (self.count-1) + delta * (new_event - self.mean)) / self.count
                )
                self.window.append(new_event)
        return SlidingWindowChain(win_size)

    def hierarchical_clustering(self):
        """MiniBatchéšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆç”¨ï¼‰"""
        class HierarchicalClusters:
            def __init__(self, levels=3, clusters_per_level=10):
                self.levels = levels
                self.clusters_per_level = clusters_per_level
                self.models = {}
            def fit_incremental(self, data_stream, batch_size=1000):
                mbk = MiniBatchKMeans(n_clusters=self.clusters_per_level, batch_size=batch_size, n_init=3)
                for batch in self._batch_generator(data_stream, batch_size):
                    mbk.partial_fit(batch)
                return mbk
            def _batch_generator(self, data, batch_size):
                batch = []
                for item in data:
                    batch.append(item)
                    if len(batch) >= batch_size:
                        yield np.array(batch)
                        batch = []
                if batch:
                    yield np.array(batch)
        return HierarchicalClusters()

    def implement_bloom_filter(self, expected_items=1_000_000, fp_rate=0.01):
        """ãƒ–ãƒ«ãƒ¼ãƒ ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆconfigé€£å‹•ï¼‰"""
        size = int(-expected_items * math.log(fp_rate) / (math.log(2) ** 2))
        hash_count = int((size / expected_items) * math.log(2))
        class BloomFilter:
            def __init__(self, size, hash_count):
                self.size = size
                self.hash_count = hash_count
                self.bit_array = np.zeros(size, dtype=bool)
            def add(self, item):
                for seed in range(self.hash_count):
                    index = self._hash(item, seed) % self.size
                    self.bit_array[index] = True
            def contains(self, item):
                for seed in range(self.hash_count):
                    index = self._hash(item, seed) % self.size
                    if not self.bit_array[index]:
                        return False
                return True
            def _hash(self, item, seed):
                h = hashlib.md5(f"{item}{seed}".encode()).digest()
                return int.from_bytes(h[:4], 'big')
        return BloomFilter(size, hash_count)

    def implement_time_series_compression(self):
        """Î”ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°+gzipæ™‚ç³»åˆ—åœ§ç¸®"""
        class TimeSeriesCompressor:
            def __init__(self):
                self.compression_ratio = 0.0
            def compress_block(self, block_data):
                if len(block_data) > 1:
                    deltas = np.diff(block_data, axis=0)
                    base = block_data[0]
                else:
                    deltas = np.array([])
                    base = block_data[0] if len(block_data) > 0 else None
                quantized = (deltas * 1000).astype(np.int16) if len(deltas) > 0 else np.array([], dtype=np.int16)
                compressed = gzip.compress(pickle.dumps({'base': base, 'deltas': quantized, 'shape': block_data.shape}))
                original_size = block_data.nbytes
                compressed_size = len(compressed)
                self.compression_ratio = 1 - (compressed_size / original_size)
                return compressed
            def decompress_block(self, compressed_data):
                data = pickle.loads(gzip.decompress(compressed_data))
                if len(data['deltas']) > 0:
                    deltas = data['deltas'].astype(np.float64) / 1000
                    reconstructed = np.cumsum(np.vstack([data['base'], deltas]), axis=0)
                else:
                    reconstructed = np.array([data['base']])
                return reconstructed
        return TimeSeriesCompressor()

    def implement_adaptive_sampling(self):
        """é«˜è² è·æ™‚ã®é©å¿œã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿configé€£å‹•ï¼‰"""
        tr = self.conf.target_sample_rate
        hl = self.conf.high_load_thresh
        class AdaptiveSampler:
            def __init__(self):
                self.target_rate   = tr
                self.high_load_th  = hl
                self.current_load = 0.0
                self.sample_probability = 1.0
            def should_sample(self, priority_score):
                if priority_score > 0.8:
                    return True
                if self.current_load > self.high_load_th:
                    self.sample_probability = self.target_rate
                else:
                    self.sample_probability = min(1.0, self.target_rate + (1 - self.current_load))
                return np.random.random() < self.sample_probability
            def update_load(self, processing_time, threshold=0.001):
                self.current_load = min(1.0, processing_time / threshold)
        return AdaptiveSampler()

    # ä»®ã‚³ãƒ¼ãƒ‰æœ¬ç•ªã§ã®å¸Œæœ›ã«ã‚ã‚ã›ã¦
    def parallel_department_processing(self, events_by_dept):
        def aggregate(events):
            arr = np.array([vectorize_state(ev) if isinstance(ev, dict) else ev for ev in events])
            return {"mean": arr.mean(axis=0), "std": arr.std(axis=0)} if arr.size > 0 else {"mean": 0, "std": 0}
        results = {}
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = {dept: executor.submit(aggregate, events) for dept, events in events_by_dept.items()}
            for dept, future in futures.items():
                results[dept] = future.result()
        return results

    async def _async_rule_check(self, event):
        # ---------------------------------------------------------------
        # ğŸš¦ã€ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã€‘
        # ---------------------------------------------------------------
        #
        # ç›®çš„ï¼š
        #   ãƒ»ç¾å ´ã®ã€Œã“ã†ãªã£ãŸã‚‰çµ¶å¯¾ã‚¢ã‚¦ãƒˆ/è­¦æˆ’ï¼ã€ã¨ã„ã†æ˜ç¤ºçš„ãªåŸºæº–ã‚’ã‚·ã‚¹ãƒ†ãƒ åŒ–ã™ã‚‹ã€‚
        #   ãƒ»AIã‚¯ãƒ©ã‚¹ã‚¿ã‚„æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã¯æ°—ã¥ãã«ãã„ã€è¶…å…·ä½“çš„ãªæ¥­å‹™ãƒ«ãƒ¼ãƒ«ã‚„ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã€
        #     æ³•è¦åˆ¶ã€ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆã€çªç™ºçš„ãªé‹ç”¨ãƒ«ãƒ¼ãƒ«ãªã©ã®ã€Œçµ¶å¯¾æ¡ä»¶ã€ã‚’å¸åã™ã‚‹ã€‚
        #
        # ä»£è¡¨ä¾‹ï¼š
        #   - å¤œé–“ã®ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ãƒ»ç®¡ç†ãƒ•ã‚©ãƒ«ãƒ€æ“ä½œã¯å¸¸ã«ã‚¢ãƒ©ãƒ¼ãƒˆ
        #   - ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆIPãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
        #   - å¹´åº¦æœ«ãƒ»æ±ºç®—æ™‚æœŸãªã©ç‰¹å®šæ™‚æœŸã®ã¿è¨±ã•ã‚Œã‚‹æ“ä½œï¼ˆä¾‹å¤–å¸åï¼‰
        #   - æ€¥ãªãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰é€£ç¶šå¤±æ•—ã‚„ãƒ­ã‚°å‰Šé™¤ãªã©â€œèª°ãŒè¦‹ã¦ã‚‚å±é™ºâ€ãªå‹•ä½œ
        #
        # ã€AIåˆ¤å®šã¨ã®é•ã„ã€‘
        #   - AIã‚¯ãƒ©ã‚¹ã‚¿åˆ¤å®š:ã€Œæ™®æ®µã®æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã®é€¸è„±ã€ã‚’â€œæ•°å€¤çš„ãªè·é›¢â€ã§ã‚¹ã‚³ã‚¢åŒ–ã—ã€æœ€ã‚‚è¿‘ã„ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆnormal/exceptional/anomalyï¼‰ã‚’åˆ†é¡ã™ã‚‹ã€‚
        #   - ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹:ã€Œçµ¶å¯¾ã‚¢ã‚¦ãƒˆã€ã€Œç¾å ´ã®å¸¸è­˜ã€ãªã©ã€â€œ1ç™ºåˆ¤å®šâ€ã§æ±ºã‚ã‚‹ã€‚AIã§ã¯æ‹¾ãˆãªã„ã€Œäººé–“ãªã‚‰ã§ã¯ã®é‹ç”¨ãƒã‚¦ãƒã‚¦ã€ã‚’å¸åï¼
        #   - åˆ¤å®šç«¶åˆæ™‚ã¯ã€Œå±é™ºå´ï¼ˆcriticalãªã©ï¼‰ã€ã‚’å„ªå…ˆã—ã¦modeã‚’ä¸Šæ›¸ãã™ã‚‹ã®ãŒå¤šã„
        #
        # ã€å®Ÿè£…ç›®çš„ã¾ã¨ã‚ã€‘
        #   - é‹ç”¨ç¾å ´ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹çš„ãªçŸ¥è¦‹ãƒ»æš«å®šé‹ç”¨ãƒ«ãƒ¼ãƒ«ã‚’ã™ãåæ˜ ã§ãã‚‹
        #   - AIåˆ¤å®šã‚’è£œåŠ©ã—ã€èª¤æ¤œçŸ¥ã‚„è¦‹é€ƒã—ã®ãƒªã‚¹ã‚¯ã‚’æ¸›ã‚‰ã™
        #   - æ³•ä»¤éµå®ˆã‚„ç¾å ´ç‹¬è‡ªã®å¼·åˆ¶ãƒ«ãƒ¼ãƒ«ã«æŸ”è»Ÿå¯¾å¿œ
        # ---------------------------------------------------------------
        await asyncio.sleep(0)  # ä½•ã‚‚ã—ãªã„ã§å³return
        return None

# -----------------
# ğŸ”¥ å®Ÿè¡Œé–¢æ•°
# -----------------
if __name__ == "__main__":
    print("=== ğŸ›¡ï¸ Security Divergence Chain System (LanScope Cat Edition) ===\n")

    # è¨­å®šã‚¯ãƒ©ã‚¹ï¼ˆChainTuneConfigï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤–éƒ¨YAML/ENVã‹ã‚‰ã‚‚OKï¼‰
    tune_conf = ChainTuneConfig()

    # 2. éƒ¨ç½²åˆ¥ãƒã‚§ãƒ¼ãƒ³ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
    manager = DepartmentSecurityChainManager(tune_conf=tune_conf)

    # 3. æ­£å¸¸ãƒ­ã‚°ã®èª­ã¿è¾¼ã¿
    with open(tune_conf.security_chain_filepath, "r", encoding="utf-8") as f:
        normal_events = json.load(f)
    print(f"ğŸ“ {len(normal_events)}ä»¶ã®å­¦ç¿’ç”¨ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # éƒ¨ç½²ã”ã¨ã«ã‚¤ãƒ™ãƒ³ãƒˆã‚’åˆ†é¡ï¼†æ“ä½œçµ±è¨ˆ
    dept_events, operation_stats = {}, {}
    for event in normal_events:
        dept = event.get("department", "unknown")
        dept_events.setdefault(dept, []).append(event)
        operation = event.get("operation", "unknown")
        operation_stats[operation] = operation_stats.get(operation, 0) + 1

    print("\nğŸ“Š æ“ä½œã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ:")
    for op, count in sorted(operation_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  {op}: {count}ä»¶")

    # å„éƒ¨ç½²ã®ãƒã‚§ãƒ¼ãƒ³ã‚’åˆæœŸåŒ–
    for dept_name, events in dept_events.items():
        if dept_name != "unknown":
            manager.initialize_department_chain(dept_name, events)

    # æ™‚é–“å¸¯åˆ¥è¿½åŠ å­¦ç¿’
    print("\nğŸ“š æ™‚é–“å¸¯åˆ¥æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ å­¦ç¿’ä¸­...")
    for dept_name, chain in manager.department_chains.items():
        my_chain_conf = manager.conf
        patterns = []
        for gen_func in (generate_morning_patterns, generate_noon_patterns, generate_evening_patterns):
            patterns += gen_func(dept_name)
        additional_states = [
            security_event_to_state(p, manager.security_log, config=my_chain_conf) for p in patterns
        ]
        if additional_states:
            chain.update_normal_model(additional_states)
            print(f"  âœ… {dept_name}: {len(additional_states)}ä»¶ã®æ™‚é–“å¸¯åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ ")

    # ---â˜… (3) operation Ã— process_name ã®çµ„ã¿åˆã‚ã›ã§åˆ†å‰²ã§æ­£å¸¸ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ ---
    print("\nğŸ” æ“ä½œãƒ»ãƒ—ãƒ­ã‚»ã‚¹åˆ¥ã§æ­£å¸¸ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’...")
    for dept_name, events in dept_events.items():
        op_proc_states = defaultdict(list)
        for e in events:
            # é€šå¸¸ã®æ­£å¸¸ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿
            if e.get("expected", "normal") == "normal":
                op = e.get('operation', '').lower()
                proc = e.get('process_name', '').lower()
                key = (op, proc)
                state = security_event_to_state(e, manager.security_log, config=manager.conf)
                op_proc_states[key].append(state)
        # å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã‚’åˆä½“
        all_states = []
        for states in op_proc_states.values():
            all_states.extend(states)
        if all_states:
            manager.department_chains[dept_name].set_normal_model(all_states, n_clusters=3)
            print(f"  âœ… {dept_name}: {len(all_states)}ä»¶ã®å…¨æ­£å¸¸æ“ä½œï¼ˆoperationÃ—process_nameå˜ä½ã§çµ±åˆï¼‰ã§å†å­¦ç¿’ï¼")

    print(f"\nâœ… {len(manager.department_chains)}å€‹ã®éƒ¨ç½²ãƒã‚§ãƒ¼ãƒ³ã‚’åˆæœŸåŒ–å®Œäº†")

    # === ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç™»éŒ² ===
    print("\nğŸ¯ ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã‚’ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã—ã¦ç™»éŒ²ä¸­...")
    exceptional_patterns = {}
    for dept_name in manager.department_chains:
        year_end_patterns = generate_year_end_patterns(dept_name)
        if year_end_patterns:
            exceptional_patterns[dept_name] = year_end_patterns
            print(f"  âœ… {dept_name}: {len(year_end_patterns)}ä»¶ã®å¹´åº¦æœ«å”åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¾‹å¤–ãƒªã‚¹ãƒˆã«è¿½åŠ ")
    manager.exceptional_patterns = exceptional_patterns
    print(f"\n  åˆè¨ˆ {sum(len(v) for v in exceptional_patterns.values())}ä»¶ã®éƒ¨ç½²é–“å”åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¾‹å¤–ç®¡ç†ï¼")

    print("\nğŸš¨ ç•°å¸¸ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆanomaly_cluster.jsonï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    try:
        with open(tune_conf.error_chain_filepath, "r", encoding="utf-8") as f:
            anomaly_clusters = json.load(f)
        print(f"  ğŸ“ {len(anomaly_clusters)}ä»¶ã®ç•°å¸¸ã‚¯ãƒ©ã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡º")

        dept_event_map = {}

        for item in anomaly_clusters:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³A: {"department": "xxx", "events": [...]}
            if isinstance(item, dict) and "department" in item and "events" in item:
                dept = item["department"]
                dept_event_map.setdefault(dept, []).extend(item["events"])
            # ãƒ‘ã‚¿ãƒ¼ãƒ³B: ç›´æ¥ã‚¤ãƒ™ãƒ³ãƒˆ(dict)ã®å ´åˆ
            elif isinstance(item, dict) and "department" in item:
                dept = item["department"]
                dept_event_map.setdefault(dept, []).append(item)
            # ãƒ‘ã‚¿ãƒ¼ãƒ³C: ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆï¼ˆdepartmentã‚­ãƒ¼ãªã— or unknownï¼‰
            else:
                dept_event_map.setdefault("unknown", []).append(item)

        # éƒ¨ç½²ã”ã¨ã«å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æŠ•å…¥
        for dept, events in dept_event_map.items():
            if dept in manager.department_chains:
                abnormal_states = [
                    security_event_to_state(e, manager.security_log, config=my_chain_conf)
                    for e in events
                ]
                if abnormal_states:
                    manager.department_chains[dept].update_anomaly_model(abnormal_states)
                    print(f"  ğŸš¨ {dept}: {len(abnormal_states)}ä»¶ã®ç•°å¸¸ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¿½åŠ ")
            else:
                print(f"  âš ï¸ éƒ¨ç½²æœªç™»éŒ²: {dept}ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    except Exception as e:
        print(f"  âš ï¸ ç•°å¸¸ã‚¯ãƒ©ã‚¹ã‚¿ã®èª­è¾¼å¤±æ•—: {e}")

    print(f"\nâœ… ã™ã¹ã¦ã®æ­£å¸¸ãƒ»ä¾‹å¤–ãƒ»ç•°å¸¸ã‚¯ãƒ©ã‚¹ã‚¿ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    # === ğŸŒŸã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£APIå‘¼ã³å‡ºã—ã€‘ ===
    if manager.conf.enable_scalable_backend:
        print("\nğŸš€ ScalableBackendãƒãƒƒãƒAPIå‘¼ã³å‡ºã—ãƒ‡ãƒ¢")
        try:
            manager.compress_monthly_data()
            print("  âœ… åœ§ç¸®å®Œäº†")
            stats = manager.aggregate_daily_stats()
            print(f"  âœ… æ—¥æ¬¡çµ±è¨ˆ: {stats}")
            labels = manager.run_mini_batch_clustering().labels_
            print(f"  âœ… ãƒŸãƒ‹ãƒãƒƒãƒã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°labels: {labels[:5]} ...")
            is_dup = manager.check_bloom_duplicate(normal_events[:100], normal_events[0])
            print(f"  âœ… Bloomé‡è¤‡åˆ¤å®š: {is_dup}")
            par_results = manager.parallel_reaggregate()
            print(f"  âœ… ä¸¦åˆ—å†é›†è¨ˆ: éƒ¨ç½²æ•°={len(par_results)}")
            sample_event = normal_events[0]
            if manager.adaptive_sampling_run(sample_event):
                print("  âœ… é©å¿œã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æœ¬å‡¦ç†å¯ï¼")
            else:
                print("  â„¹ï¸ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ã‚¹ã‚­ãƒƒãƒ—")
        except Exception as ex:
            print(f"  âš ï¸ ScalableBackend APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {ex}")
    else:
        print("\nï¼ˆScalableBackendã¯configã§OFFã§ã™ï¼‰")

    # 4. æ­£å¸¸ã‚¢ã‚¯ã‚»ã‚¹ã®æ¤œè¨¼ï¼ˆèª¤æ¤œçŸ¥ãƒã‚§ãƒƒã‚¯ï¼‰
    print("\n=== âœ… æ­£å¸¸ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼ï¼ˆèª¤æ¤œçŸ¥ãƒã‚§ãƒƒã‚¯ï¼‰ ===")
    with open(tune_conf.normal_test_events_file, "r", encoding="utf-8") as f:
        normal_test_events = json.load(f)

    false_positive_count = 0
    false_positive_details = []

    for i, event in enumerate(normal_test_events):
        result = manager.process_event(event)

        # çµæœã®è¡¨ç¤º
        icon = "ğŸ”´" if result["alert_level"] == "HIGH" else "ğŸŸ¡" if result["status"] == "investigating" else "ğŸŸ¢"

        is_false_positive = False
        if result["alert_level"] == "HIGH" or result["status"] in ["suspicious", "critical"]:
            false_positive_count += 1
            is_false_positive = True
            icon = "âŒ FALSE POSITIVE"

            false_positive_details.append({
                "event_id": i,
                "user": event['user_id'],
                "operation": event.get('operation', 'unknown'),
                "file_path": event.get('file_path', 'N/A'),
                "dept_status": result.get('dept_result', {}).get('status', 'N/A'),
                "dept_divergence": result.get('dept_result', {}).get('divergence', 0),
                "user_status": result.get('user_result', {}).get('status', 'N/A'),
                "user_divergence": result.get('user_result', {}).get('divergence', 0),
                "has_user_chain": result.get('has_user_chain', False)
            })

        # åŸºæœ¬æƒ…å ±ã®è¡¨ç¤º
        print(f"{icon} Normal {i}: {event.get('operation', 'unknown')}")
        print(f"   User: {event['user_id']} | Dept: {event['department']}")
        if 'file_path' in event:
            print(f"   File: {event['file_path']}")
        print(f"   Status: {result['status']} | Divergence: {result['divergence']:.4f}")

        if is_false_positive:
            print("   ğŸ“Š è©³ç´°åˆ†æ:")
            print(f"      éƒ¨ç½²åˆ¤å®š: {result['dept_result']['status']} (Div: {result['dept_result']['divergence']:.2f})")
            if result.get('has_user_chain', False):
                print(f"      å€‹äººåˆ¤å®š: {result['user_result']['status']} (Div: {result['user_result']['divergence']:.2f})")
        print()

    # 5. æ··åˆã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆï¼ˆæœ€ã‚‚ãƒªã‚¢ãƒ«ï¼‰
    with open(tune_conf.suspicious_events_file, "r", encoding="utf-8") as f:
        mixed_events = json.load(f)

    event_results = []
    correct_detections = 0

    for i, event in enumerate(mixed_events):
        result = manager.process_event(event)

        expected = event.get("expected", "unknown")
        actual = result["status"]

        def is_equivalent(expected, actual):
            normal_equiv = ["normal", "investigating"]
            suspicious_equiv = ["suspicious", "critical"]
            if expected in normal_equiv and actual in normal_equiv:
                return True
            if expected in suspicious_equiv and actual in suspicious_equiv:
                return True
            return expected == actual

        is_correct = is_equivalent(expected, actual)
        if is_correct:
            correct_detections += 1

        # å€‹äººãƒã‚§ãƒ¼ãƒ³ã®é€¸è„±åº¦ï¼ˆç„¡ã„å ´åˆNoneï¼‰
        user_chain = manager.user_chains.get(event.get("user_id", "unknown"))
        deviation_score = user_chain.calculate_behavior_deviation(event) if user_chain else None

        event_results.append({
            "No": i + 1,
            "Time": event.get('timestamp', '').split()[1] if "timestamp" in event else event.get('event_time', '').split()[1],
            "User": event['user_id'],
            "Dept": event.get('department', ""),
            "Operation": event.get('operation', event.get('action', '')),
            "Expected": expected,
            "Actual": actual,
            "Correct": is_correct,
            "Divergence": result["divergence"],
            "AlertLevel": result["alert_level"],
            "DeviationScore": result.get('user_result', {}).get('deviation_score', None),
            "UserStatus": result.get('user_result', {}).get('status', 'N/A'),
            "UserDivergence": result.get('user_result', {}).get('divergence', 0.0),
            "DeptStatus": result.get('dept_result', {}).get('status', 'N/A'),
            "DeptDivergence": result.get('dept_result', {}).get('divergence', 0.0),
            "CrossDeptWarning": result.get("cross_dept_warning", False)
        })

        # --- ãƒ­ã‚°å‡ºåŠ›ã‚’ã‚»ãƒ¼ãƒ– ---
        if i < 70:  # å…ˆé ­10ä»¶ã ã‘ã‚µãƒãƒªå‡ºåŠ›
            print(f"{'âœ…' if is_correct else 'âŒ'} Event {i+1}: {event.get('operation', '')} | Exp={expected} | Act={actual}")

    # DataFrameã§é›†è¨ˆ
    df_results = pd.DataFrame(event_results)
    print(f"\n=== ğŸ­ æ··åˆã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆçµæœï¼ˆå…ˆé ­60ä»¶ã®ã¿è¡¨ç¤º/å…¨{len(mixed_events)}ä»¶ï¼‰ ===")
    print(df_results.head(60).to_string(index=False))  # å…ˆé ­10ä»¶ã ã‘

    # ç·åˆåˆ¤å®šç²¾åº¦
    accuracy = correct_detections / len(mixed_events) * 100
    print(f"\nğŸ¯ ç·åˆã‚·ãƒŠãƒªã‚ªåˆ¤å®šç²¾åº¦: {accuracy:.1f}% ({correct_detections}/{len(mixed_events)})")

    # å¿…è¦ãªã‚‰CSVä¿å­˜
    df_results.to_csv("scaling_security_logs.csv", index=False)

    # 6. ã‚¼ãƒ­ãƒ‡ã‚¤æ”»æ’ƒæ¤œè¨¼ã¨ã‚µãƒãƒªãƒ¼
    print("\n=== ğŸš¨ ã‚¼ãƒ­ãƒ‡ã‚¤æ”»æ’ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ===")
    with open(tune_conf.zero_day_events_file, "r", encoding="utf-8") as f:
        zero_day_events = json.load(f)

    detected_attacks = 0
    zero_day_results = []
    for i, event in enumerate(zero_day_events):
        result = manager.process_event(event)
        zero_day_results.append({**event, **result})
        detected = result["alert_level"] == "HIGH" or result["status"] in ["suspicious", "critical"]
        if detected:
            detected_attacks += 1
        icon = (
            "ğŸ”´" if detected else
            "ğŸŸ¡" if result["status"] == "investigating" else
            "âŒ MISSED ATTACK"
        )
        cross_dept = "âš ï¸ CROSS-DEPT" if result.get("cross_dept_warning") else ""
        print(f"{icon} Event {i+1}: {event.get('operation', event.get('action', 'N/A'))}")
        print(f"   User: {event.get('user_id', 'unknown')} | Target: {event.get('target_resource')}")
        print(f"   Status: {result['status']} | Divergence: {result['divergence']:.4f} {cross_dept}")

        # è¿½åŠ ï¼šã‚¯ãƒ©ã‚¹ã‚¿è·é›¢ã‚„è©³ç´°
        if "dept_result" in result:
            dept = result["dept_result"]
            print(f"     [DEBUG] Dept: status={dept.get('status')}, divergence={dept.get('divergence')}, cluster={dept.get('cluster_id')}")
        if "user_result" in result:
            user = result["user_result"]
            print(f"     [DEBUG] User: status={user.get('status')}, divergence={user.get('divergence')}, cluster={user.get('cluster_id')}")
        print("")  # ç©ºè¡Œã§è¦‹ã‚„ã™ã

    # 7. æ”»æ’ƒãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º & æ™‚ç³»åˆ—
    with open(tune_conf.attack_patterns_file, "r", encoding="utf-8") as f:
        attack_patterns = json.load(f)
    with open(tune_conf.suspicious_events_file, "r", encoding="utf-8") as f:
        suspicious_events = json.load(f)

    attack_pattern_matches = {k: [] for k in attack_patterns}

    # é‡è¦ãªã‚¤ãƒ™ãƒ³ãƒˆã ã‘å…ˆé ­300ä»¶ç¨‹åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯
    for i, event in enumerate(suspicious_events[:300]):
        result = manager.process_event(event)
        event["divergence"] = result.get("divergence", 0)
        status = result["status"]
        action = event.get("operation", event.get("action", ""))

        for p_type, p_info in attack_patterns.items():
            if any(ind in action for ind in p_info["indicators"]) and status in ["suspicious", "critical"]:
                attack_pattern_matches[p_type].append(i + 1)

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®ã‚µãƒãƒªå‡ºåŠ›
    for pattern_type, indices in attack_pattern_matches.items():
        if indices:
            p = attack_patterns[pattern_type]
            print(f"\nã€{p['name']}ã€‘{p['description']}")
            print(f"  æ¤œå‡ºã‚¤ãƒ™ãƒ³ãƒˆç•ªå·: {', '.join(map(str, indices[:10]))} ... (å…¨{len(indices)}ä»¶)")
            print(f"  æŒ‡æ¨™: {', '.join(p['indicators'])}")

    # æ”»æ’ƒé€²è¡Œã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ï¼ˆæœ€å¤§10ä»¶ã ã‘ï¼‰
    attack_timeline = [
        {
            "time": e.get("timestamp", e.get("event_time", "")).split()[1] if "timestamp" in e or "event_time" in e else "",
            "user": e.get("user_id", "unknown"),
            "operation": e.get("operation", e.get("action", "")),
            "severity": e.get("expected", "")
        }
        for e in suspicious_events[:300] if e.get("expected") in ["suspicious", "critical"]
    ]

    if attack_timeline:
        print("\nâ° æ”»æ’ƒé€²è¡Œã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ï¼ˆæœ€å¤§10ä»¶è¡¨ç¤ºï¼‰:")
        print("  æ™‚åˆ»     | ãƒ¦ãƒ¼ã‚¶ãƒ¼    | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³                    | æ·±åˆ»åº¦")
        print("  " + "-" * 65)
        for entry in attack_timeline[:20]:
            print(f"  {entry['time']} | {entry['user']:<10} | {entry['operation']:<28} | {entry['severity']}")

    # 8. ç•°å¸¸è¡Œå‹•ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æï¼ˆDivergenceé™é †Top10ï¼‰
    print("\nğŸ‘¤ ç•°å¸¸è¡Œå‹•ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æï¼ˆDivergenceã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ï¼‰:")
    top_anomalies = sorted(
        suspicious_events,
        key=lambda e: e.get("divergence", 0),
        reverse=True
    )[:10]
    for i, e in enumerate(top_anomalies):
        # æ™‚åˆ»ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å®‰å…¨ãªå–å¾—
        t = e.get("timestamp", e.get("event_time", "")).split()[1] if "timestamp" in e or "event_time" in e else ""
        act = e.get("scenario", e.get("operation", e.get("action", "")))
        print(f"  {i+1}. {t} | User: {e.get('user_id', 'unknown')} | operation: {act} | Divergence: {e.get('divergence', 0):.1f}")
    if len(suspicious_events) > 10:
        print(f"  ... ä»–{len(suspicious_events)-10}ä»¶ã®ç•°å¸¸è¡Œå‹•ã‚ã‚Š")

    # 9. ç²¾åº¦ã‚µãƒãƒªãƒ¼
    print("\n=== ğŸ“ˆ ã‚·ã‚¹ãƒ†ãƒ æ¤œçŸ¥ç²¾åº¦ã‚µãƒãƒªãƒ¼ ===")
    print(f"æ­£å¸¸ã‚¢ã‚¯ã‚»ã‚¹åˆ¤å®š: {len(normal_test_events)}ä»¶")
    print(f"  â””â”€ æ­£å¸¸åˆ¤å®š: {len(normal_test_events) - false_positive_count}ä»¶")
    print(f"  â””â”€ èª¤æ¤œçŸ¥: {false_positive_count}ä»¶")
    print(f"æ”»æ’ƒã‚¤ãƒ™ãƒ³ãƒˆ: {len(zero_day_events)}ä»¶")
    print(f"  â””â”€ æ¤œçŸ¥æˆåŠŸ: {detected_attacks}ä»¶")
    print(f"  â””â”€ è¦‹é€ƒã—: {len(zero_day_events) - detected_attacks}ä»¶")
    precision = detected_attacks / (detected_attacks + false_positive_count) if (detected_attacks + false_positive_count) > 0 else 0
    recall = detected_attacks / len(zero_day_events) if len(zero_day_events) > 0 else 0
    print(f"\n  ç²¾åº¦ï¼ˆPrecisionï¼‰: {precision:.2%}")
    print(f"  å†ç¾ç‡ï¼ˆRecallï¼‰: {recall:.2%}")

    # 10. æ¨ªå±•é–‹æ”»æ’ƒæ¤œçŸ¥
    print("\n=== ğŸ”„ æ¨ªå±•é–‹æ”»æ’ƒã®æ¤œçŸ¥ ===")
    lateral_movements = manager.detect_lateral_movement(time_window_minutes=180)
    if lateral_movements:
        print("âš ï¸ è¤‡æ•°éƒ¨ç½²ã®ãƒªã‚½ãƒ¼ã‚¹ã«åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã„ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼:")
        for user_info in lateral_movements:
            print(f"   User: {user_info['user_id']}")
            print(f"   Departments: {', '.join(user_info['departments_accessed'])}")
            print(f"   Risk Score: {user_info['risk_score']:.2f}\n")
    else:
        print("âœ… æ¨ªå±•é–‹æ”»æ’ƒã®å…†å€™ãªã—")

    # 11. âš¡ éƒ¨ç½²æ¨ªæ–­ åŒæœŸã‚¢ãƒ©ãƒ¼ãƒˆ
    print("\n=== âš¡ éƒ¨ç½²æ¨ªæ–­ åŒæœŸã‚¢ãƒ©ãƒ¼ãƒˆ ===")
    cross_dept_alert_sync(manager, window_minutes=180)

    # 12. ãƒã‚§ãƒ¼ãƒ³ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    print("\n=== ğŸ’¾ ãƒã‚§ãƒ¼ãƒ³ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ===")
    for dept_name, chain in manager.department_chains.items():
        filename = f"security_chain_{dept_name}.json"
        chain.export_to_json(filename)
        print(f"âœ… {dept_name}éƒ¨é–€ã®ãƒã‚§ãƒ¼ãƒ³ã‚’ {filename} ã«ä¿å­˜")

    print("\nâœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åˆ†æ å®Œäº†ï¼")

    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§å¯è¦–åŒ–
    report = manager.get_cross_department_report(time_window_minutes=60*24)
    with open("cross_dept_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    # éƒ¨ç½²ã”ã¨ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§å¯è¦–åŒ–
    for dept in manager.department_chains:
        analysis = manager.analyze_department_patterns(dept, time_range_hours=72)
        print(json.dumps(analysis, ensure_ascii=False, indent=2))

    # --- ã“ã“ã§æ··åˆãƒ­ã‚°ã‚„æ”»æ’ƒãƒ‘ã‚¿ãƒ¼ãƒ³ãªã©ã‚’æŒ‡å®š ---
    with open(tune_conf.suspicious_events_file, "r", encoding="utf-8") as f:
        all_bench_events = json.load(f)

    # å¿…è¦ã«å¿œã˜ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° or å…¨ä»¶æŠ•å…¥
    BENCHMARK_SIZE = 60  # å¿…è¦ãªã‚‰len(all_bench_events)ã¨ã‹ã§ã‚‚
    benchmark_events = random.sample(all_bench_events, BENCHMARK_SIZE) if len(all_bench_events) > BENCHMARK_SIZE else all_bench_events

    start = time.perf_counter()

    for event in benchmark_events:
        result = manager.process_event(event)

    elapsed = time.perf_counter() - start
    throughput = len(benchmark_events) / elapsed if elapsed > 0 else 0

    print(f"\n=== â±ï¸ å‡¦ç†æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ===")
    print(f"ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {len(benchmark_events)}")
    print(f"å‡¦ç†æ™‚é–“: {elapsed:.3f} ç§’")
    print(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f} events/sec")
