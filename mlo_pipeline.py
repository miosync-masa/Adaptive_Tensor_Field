# =====================================
# å®Œå…¨çµ±åˆç‰ˆ MLOps + LambdaÂ³ãƒ™ã‚¤ã‚ºæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# =====================================

import asyncio
import json
import time
import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import pickle
import aiohttp
from kafka import KafkaConsumer, KafkaProducer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from scipy import stats
import mlflow
import kubernetes
from kubernetes import client, config
import schedule
import threading

# =====================================
# moduleå‘¼ã³å‡ºã—
# =====================================
from auto_minibatch import AdaptiveMiniBatchKMeans
from policy_manager import PolicyManager
from diversity_adversarial_defense 
import AdversarialDefense, DiversityAwareCluster, PolicyManager as AdvPolicyManager

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
auto_minibatch = AdaptiveMiniBatchKMeans()
policy_manager = PolicyManager()

# =====================================
# è¨­å®šã‚¯ãƒ©ã‚¹
# =====================================

@dataclass
class OptimizedL3Config:
    """æœ€é©åŒ–ã•ã‚ŒãŸLambdaÂ³è¨­å®š"""
    # LambdaÂ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    delta_percentile: float = 90.0
    local_window: int = 10
    local_jump_percentile: float = 95.0
    window: int = 20
    
    # MCMCæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    draws: int = 500
    tune: int = 200
    chains: int = 2
    target_accept: float = 0.8
    
    # é«˜é€ŸåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    use_jax: bool = True
    use_gpu: bool = False
    save_warmup: bool = True
    
    # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«å­¦ç¿’
    incremental_update: bool = True
    warmup_cache_path: str = "./warmup_cache"
    
    # MLOpsãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    kafka_servers: List[str] = None
    threat_intel_apis: List[str] = None
    min_batch_size: int = 100
    
    def __post_init__(self):
        if self.kafka_servers is None:
            self.kafka_servers = ['localhost:9092']
        if self.threat_intel_apis is None:
            self.threat_intel_apis = []

# =====================================
# 0. å¤šæ§˜æ€§ï¼†æ•µå¯¾çš„é˜²å¾¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆAdversarialDefenseç­‰ï¼‰
# =====================================

class SimpleAdversarialDefense:
    def __init__(self, normal_samples=None, event_buffer=None):
        self.normal_samples = normal_samples or []
        self.event_buffer = event_buffer or []

    def get_recent_anomalies(self):
        """
        (å‚è€ƒå®Ÿè£…) ãƒ¡ãƒ¢ãƒªä¸Šã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒƒãƒ•ã‚¡ã‹ã‚‰æŠ½å‡º
        æœ¬ç•ªé‹ç”¨ã¯DB,ãƒ­ã‚°APIç­‰ã§å¥½ãã«æ›¸ãæ›ãˆOK
        """
        return [ev for ev in self.event_buffer if ev.get("is_anomaly", False)]

    # ä»–ã«ã‚‚generate_attack_variations, train_against_variants...ãªã©è¿½åŠ 

# =====================================
# 1. ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆLambdaÂ³çµ±åˆï¼‰
# =====================================

class EnhancedDataCollector:
    """
    LambdaÂ³ç‰¹å¾´é‡ã‚’å«ã‚€å¼·åŒ–ãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹
    ãƒ»å³æ™‚å…ç–«åå¿œï¼ˆç•°å¸¸æ™‚ã¯è‡ªå‹•ã§ãƒ­ãƒ¼ã‚«ãƒ«å­¦ç¿’ãƒ»ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒ»EDRé€£æºï¼‰
    ãƒ»ç³»åˆ—ã”ã¨ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ï¼‹ãƒãƒƒãƒå­¦ç¿’ï¼ˆLambdaÂ³ç‰¹å¾´é‡æŠ½å‡ºï¼‰
    ãƒ»æŸ”è»Ÿãªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€£æºï¼ˆsend_to_preprocessingï¼‰
    """
    def __init__(
        self,
        config: OptimizedL3Config,
        auto_minibatch=None,
        kafka_producer=None,
        policy_manager=None,
        preprocessor=None
    ):
        self.config = config
        self.kafka_consumer = KafkaConsumer(
            'security-events',
            bootstrap_servers=config.kafka_servers,
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        self.data_buffer = []
        self.series_data = {}  # ç³»åˆ—ã”ã¨ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒƒãƒ•ã‚¡

        # å¤–éƒ¨é€£æºç³»
        self.auto_minibatch = auto_minibatch or globals().get("auto_minibatch")
        self.kafka_producer = kafka_producer
        self.policy_manager = policy_manager or globals().get("policy_manager")

        # å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å—ã‘å£ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        self.preprocessor = preprocessor

    def set_preprocessor(self, preprocessor):
        """å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€£æºã®å¾Œä»˜ã‘ç™»éŒ²ã‚‚å¯"""
        self.preprocessor = preprocessor

    async def collect_security_events(self):
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆã®éåŒæœŸåé›†ï¼‹å³æ™‚å…ç–«åå¿œ"""
        async for message in self.kafka_consumer:
            event = message.value

            # --- 1. å³æ™‚å…ç–«åå¿œ ---
            if self._is_anomaly(event):
                self._handle_immunity(event)

            # --- 2. ç³»åˆ—ã”ã¨ã®ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚° ---
            series_name = event.get('series', 'default')
            self.series_data.setdefault(series_name, []).append(event)
            self.data_buffer.append(event)

            # --- 3. ãƒãƒƒãƒå­¦ç¿’ã®ç™ºç« ---
            if len(self.data_buffer) >= self.config.min_batch_size:
                await self.process_batch()

    def _is_anomaly(self, event):
        """ã‚¤ãƒ™ãƒ³ãƒˆãŒç•°å¸¸ã‹ã©ã†ã‹åˆ¤å®šï¼ˆæ±ç”¨ï¼šdictå‹ãƒ»objectå‹ä¸¡å¯¾å¿œï¼‰"""
        if hasattr(event, "is_anomaly") and callable(event.is_anomaly):
            return event.is_anomaly()
        if isinstance(event, dict) and "anomaly" in event:
            return event["anomaly"]
        return False

    def _handle_immunity(self, event):
        """ç•°å¸¸ã‚¤ãƒ™ãƒ³ãƒˆæ™‚ã®å³æ™‚å…ç–«å‡¦ç†ï¼ˆä¸¦åˆ—ç™ºç«OKï¼‰"""
        # 1. ãƒ­ãƒ¼ã‚«ãƒ«ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’
        if self.auto_minibatch:
            self.auto_minibatch.add_event(event)
        # 2. å³æ™‚Kafkaãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
        if self.kafka_producer:
            self.kafka_producer.send('anomaly-topic', json.dumps(event).encode())
        # 3. ãƒãƒªã‚·ãƒ¼ï¼EDRã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
        if self.policy_manager:
            self.policy_manager.update_defense(event)

    async def process_batch(self):
        """ç³»åˆ—ãƒãƒƒãƒ•ã‚¡ã®ãƒãƒƒãƒå‡¦ç†ãƒ»LambdaÂ³ç‰¹å¾´é‡æŠ½å‡ºãƒ»å‰å‡¦ç†é€ä¿¡"""
        if not self.data_buffer:
            return

        features_by_series = {}
        for series_name, series_events in self.series_data.items():
            if len(series_events) > 10:
                values = np.array([e['value'] for e in series_events])
                features = self.calc_lambda3_features(values)
                features_by_series[series_name] = features

        # å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¸æ¸¡ã™ï¼ˆéåŒæœŸ/ãƒãƒƒãƒ•ã‚¡å¼ï¼‰
        await self.send_to_preprocessing(features_by_series)

        # ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
        self.data_buffer.clear()
        # series_dataå´ã¯å¿…è¦ã«å¿œã˜ã¦éƒ½åº¦ã‚¯ãƒªã‚¢orä¼¸ã°ã™
        # self.series_data[series_name] = []  # å¿…è¦ãªã‚‰å„ç³»åˆ—ã§åˆ‡ã£ã¦ã‚‚ã‚ˆã„

    async def send_to_preprocessing(self, features_by_series):
        """LambdaÂ³ç‰¹å¾´é‡ãƒãƒƒãƒã‚’å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¸éåŒæœŸé€ä¿¡"""
        if self.preprocessor:
            # å—ã‘å£ãŒasync/awaitå‹ãªã‚‰
            if hasattr(self.preprocessor, 'receive_features'):
                await self.preprocessor.receive_features(features_by_series)
            else:
                # éåŒæœŸã§ãªã„å ´åˆï¼ˆä¾‹ï¼šç›´åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å ´åˆã¯åŒæœŸå‘¼ã³å‡ºã—ã§ã‚‚OKï¼‰
                self.preprocessor.receive_features(features_by_series)
        else:
            print("[WARN] Preprocessoræœªç™»éŒ²: featuresæœªé€ä¿¡")

    def calc_lambda3_features(self, data: np.ndarray) -> Dict[str, np.ndarray]:
        """LambdaÂ³ç‰¹å¾´é‡ã®è¨ˆç®—ï¼ˆç³»åˆ—å˜ä½ã§å‘¼ã³å‡ºã—ï¼‰"""
        n = len(data)
        diff = np.diff(data, prepend=data[0])
        threshold = np.percentile(np.abs(diff), self.config.delta_percentile)

        # ã‚¸ãƒ£ãƒ³ãƒ—æ¤œå‡º
        delta_LambdaC_pos = (diff > threshold).astype(np.float32)
        delta_LambdaC_neg = (diff < -threshold).astype(np.float32)

        # ãƒ­ãƒ¼ã‚«ãƒ«æ¨™æº–åå·®
        window = self.config.local_window
        padded_data = np.pad(data, (window, window), mode='edge')
        local_std = np.array([
            np.std(padded_data[i:i + 2 * window + 1])
            for i in range(n)
        ])

        # ãƒ­ãƒ¼ã‚«ãƒ«ã‚¸ãƒ£ãƒ³ãƒ—
        score = np.abs(diff) / (local_std + 1e-8)
        local_threshold = np.percentile(score, self.config.local_jump_percentile)
        local_jump_detect = (score > local_threshold).astype(np.float32)

        # æ™‚é–“çª“æ¨™æº–åå·®
        rho_T = np.array([
            np.std(data[max(0, i - self.config.window):i + 1])
            for i in range(n)
        ])

        # æ™‚é–“ãƒˆãƒ¬ãƒ³ãƒ‰
        time_trend = np.arange(n, dtype=np.float32) / n

        return {
            'delta_LambdaC_pos': delta_LambdaC_pos,
            'delta_LambdaC_neg': delta_LambdaC_neg,
            'rho_T': rho_T,
            'time_trend': time_trend,
            'local_jump_detect': local_jump_detect,
            'raw_data': data
        }

# =====================================
# 2. å‰å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆLambdaÂ³å¯¾å¿œï¼‰
# =====================================

class Lambda3DataPreprocessor:
    """LambdaÂ³ç‰¹åŒ–å‹å‰å‡¦ç†"""
    
    def __init__(self, config: OptimizedL3Config):
        self.config = config
        self.scaler = StandardScaler()
        self.feature_history = []
        self.lambda3_memory = []  # LambdaÂ³ã‚¤ãƒ™ãƒ³ãƒˆãƒ¡ãƒ¢ãƒª
        
        # 1åˆ†ã”ã¨ã®å®šæœŸå®Ÿè¡Œè¨­å®š
        schedule.every(1).minutes.do(self.run_preprocessing)
        threading.Thread(target=self._run_scheduler, daemon=True).start()
    
    def _run_scheduler(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®å®Ÿè¡Œ"""
        while True:
            schedule.run_pending()
            time.sleep(1)
    
    def run_preprocessing(self):
        """å‰å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        batch_features = self.get_batch_from_queue()
        
        if batch_features:
            # LambdaÂ³ç‰¹å¾´é‡ã®çµ±åˆ
            integrated_features = self.integrate_lambda3_features(batch_features)
            
            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ¡ãƒ¢ãƒªã®æ›´æ–°
            self.update_lambda3_memory(integrated_features)
            
            # å› æœæ€§åˆ†æ
            causality_features = self.analyze_causality(integrated_features)
            
            # æœ€çµ‚ç‰¹å¾´é‡ã®æ§‹ç¯‰
            final_features = self.build_final_features(
                integrated_features, 
                causality_features
            )
            
            # å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«é€ä¿¡
            self.send_to_training_pipeline(final_features)
    
    def integrate_lambda3_features(self, batch_features: Dict) -> Dict:
        """è¤‡æ•°ç³»åˆ—ã®LambdaÂ³ç‰¹å¾´é‡ã‚’çµ±åˆ"""
        integrated = {}
        
        # å„ç³»åˆ—ã®ç‰¹å¾´é‡ã‚’çµåˆ
        for series_name, features in batch_features.items():
            for feature_name, values in features.items():
                key = f"{series_name}_{feature_name}"
                integrated[key] = values
        
        # ç³»åˆ—é–“ç›¸äº’ä½œç”¨ç‰¹å¾´é‡
        if len(batch_features) > 1:
            series_names = list(batch_features.keys())
            for i in range(len(series_names)):
                for j in range(i+1, len(series_names)):
                    # ç›¸äº’ç›¸é–¢
                    series_a = batch_features[series_names[i]]['raw_data']
                    series_b = batch_features[series_names[j]]['raw_data']
                    
                    if len(series_a) == len(series_b):
                        correlation = np.corrcoef(series_a, series_b)[0, 1]
                        integrated[f"corr_{series_names[i]}_{series_names[j]}"] = correlation
        
        return integrated
    
    def update_lambda3_memory(self, features: Dict):
        """LambdaÂ³ã‚¤ãƒ™ãƒ³ãƒˆãƒ¡ãƒ¢ãƒªã®æ›´æ–°"""
        event_dict = {}
        
        for key, values in features.items():
            if 'delta_LambdaC_pos' in key:
                series_name = key.split('_')[0]
                if series_name not in event_dict:
                    event_dict[series_name] = {}
                event_dict[series_name]['pos'] = np.sum(values) > 0
            elif 'delta_LambdaC_neg' in key:
                series_name = key.split('_')[0]
                if series_name not in event_dict:
                    event_dict[series_name] = {}
                event_dict[series_name]['neg'] = np.sum(values) > 0
        
        self.lambda3_memory.append({
            'timestamp': datetime.now(),
            'events': event_dict
        })
        
        # ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.lambda3_memory) > 1000:
            self.lambda3_memory = self.lambda3_memory[-1000:]
    
    def analyze_causality(self, features: Dict) -> Dict:
        """å› æœæ€§åˆ†æ"""
        causality_features = {}
        
        if len(self.lambda3_memory) < 10:
            return causality_features
        
        # å˜ä¸€ç³»åˆ—å†…ã®å› æœæ€§
        series_names = set([k.split('_')[0] for k in features.keys()])
        
        for series in series_names:
            # positive â†’ negative ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            pos_neg_count = 0
            for i in range(len(self.lambda3_memory) - 1):
                curr_event = self.lambda3_memory[i]['events'].get(series, {})
                next_event = self.lambda3_memory[i + 1]['events'].get(series, {})
                
                if curr_event.get('pos', False) and next_event.get('neg', False):
                    pos_neg_count += 1
            
            causality_features[f"{series}_pos_neg_causality"] = pos_neg_count / max(len(self.lambda3_memory) - 1, 1)
        
        return causality_features

# =====================================
# 3. éšå±¤çš„ãƒ™ã‚¤ã‚ºå­¦ç¿’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# =====================================

class HierarchicalBayesianTrainer:
    """éšå±¤çš„ãƒ™ã‚¤ã‚ºå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: OptimizedL3Config):
        self.config = config
        self.models = {
            'layer1_immediate': None,    # 1åˆ†: é«˜é€Ÿãƒ™ã‚¤ã‚º
            'layer2_fast': None,         # 1æ™‚é–“: æ¨™æº–ãƒ™ã‚¤ã‚º
            'layer3_deep': None,         # æ—¥æ¬¡: éšå±¤ãƒ™ã‚¤ã‚º
            'layer4_evolution': None     # é€±æ¬¡: ãƒ¢ãƒ‡ãƒ«é¸æŠ
        }
        self.last_traces = {}
        mlflow.set_tracking_uri("http://localhost:5000")
        
        # JAXãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®è¨­å®š
        if config.use_jax:
            import jax
            import numpyro
            pm.sampling_jax.sample_numpyro_nuts()
    
    def train_immediate_layer(self, features: Dict, data: np.ndarray):
        """Layer 1: å³æ™‚ãƒ™ã‚¤ã‚ºé©å¿œï¼ˆ1åˆ†ï¼‰"""
        with mlflow.start_run(run_name="immediate_bayesian"):
            start_time = time.time()
            
            # æœ€å°é™ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã§é«˜é€Ÿæ¨è«–
            with pm.Model() as model:
                # æƒ…å ±çš„äº‹å‰åˆ†å¸ƒï¼ˆå‰å›ã®çµæœã‚’ä½¿ç”¨ï¼‰
                if 'layer1' in self.last_traces:
                    trace = self.last_traces['layer1']
                    beta_0 = pm.Normal('beta_0', 
                                     mu=float(trace.posterior['beta_0'].mean()),
                                     sigma=float(trace.posterior['beta_0'].std()))
                else:
                    beta_0 = pm.Normal('beta_0', mu=0, sigma=2)
                
                # ã‚·ãƒ³ãƒ—ãƒ«ãªç·šå½¢ãƒ¢ãƒ‡ãƒ«
                mu = beta_0
                for feature_name, values in features.items():
                    if 'delta_LambdaC' in feature_name or 'rho_T' in feature_name:
                        beta = pm.Normal(f'beta_{feature_name}', mu=0, sigma=1)
                        mu += beta * values
                
                sigma = pm.HalfNormal('sigma', sigma=1)
                y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=data)
                
                # é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                trace = pm.sample(
                    draws=200,
                    tune=100,
                    chains=2,
                    nuts_sampler="numpyro" if self.config.use_jax else None,
                    progressbar=False
                )
            
            self.last_traces['layer1'] = trace
            self.models['layer1_immediate'] = (model, trace)
            
            mlflow.log_metric("layer1_training_time", time.time() - start_time)
            mlflow.log_metric("layer1_rhat_max", float(az.rhat(trace).max()))
    
    def train_fast_layer(self, features: Dict, data: np.ndarray):
        """Layer 2: é«˜é€Ÿãƒ™ã‚¤ã‚ºå­¦ç¿’ï¼ˆ1æ™‚é–“ï¼‰"""
        with mlflow.start_run(run_name="fast_bayesian"):
            # è¤‡æ•°ç³»åˆ—ã®ç›¸äº’ä½œç”¨ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«
            with pm.Model() as model:
                # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                beta_0 = pm.Normal('beta_0', mu=0, sigma=2)
                
                # å„ç³»åˆ—ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                mu = beta_0
                series_effects = {}
                
                for feature_name, values in features.items():
                    if any(key in feature_name for key in ['delta_LambdaC', 'rho_T', 'time_trend']):
                        series_name = feature_name.split('_')[0]
                        
                        # ç³»åˆ—å›ºæœ‰åŠ¹æœ
                        if series_name not in series_effects:
                            series_effects[series_name] = pm.Normal(
                                f'series_effect_{series_name}', mu=0, sigma=1
                            )
                        
                        beta = pm.Normal(f'beta_{feature_name}', mu=0, sigma=3)
                        mu += beta * values + series_effects[series_name]
                
                # ç›¸äº’ä½œç”¨é …
                if 'corr_' in str(features.keys()):
                    for key, value in features.items():
                        if key.startswith('corr_'):
                            beta_interaction = pm.Normal(f'beta_{key}', mu=0, sigma=2)
                            mu += beta_interaction * value
                
                sigma = pm.HalfNormal('sigma', sigma=1)
                y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=data)
                
                # æ¨™æº–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                trace = pm.sample(
                    draws=self.config.draws,
                    tune=self.config.tune,
                    chains=self.config.chains,
                    target_accept=self.config.target_accept,
                    nuts_sampler="numpyro" if self.config.use_jax else None,
                    progressbar=False
                )
            
            self.models['layer2_fast'] = (model, trace)
            mlflow.sklearn.log_model(model, "layer2_model")
    
    def train_deep_layer(self, features: Dict, data: pd.DataFrame):
        """Layer 3: éšå±¤ãƒ™ã‚¤ã‚ºï¼ˆæ—¥æ¬¡ï¼‰"""
        with mlflow.start_run(run_name="hierarchical_bayesian"):
            # ãƒ‡ãƒ¼ã‚¿ã®éšå±¤æ§‹é€ ã‚’æŠ½å‡º
            if 'department' in data.columns and 'user_id' in data.columns:
                dept_idx = pd.Categorical(data['department']).codes
                user_idx = pd.Categorical(data['user_id']).codes
                n_depts = len(data['department'].unique())
                n_users = len(data['user_id'].unique())
                
                with pm.Model() as model:
                    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼
                    mu_global = pm.Normal('mu_global', mu=0, sigma=5)
                    sigma_dept = pm.HalfNormal('sigma_dept', sigma=2)
                    sigma_user = pm.HalfNormal('sigma_user', sigma=1)
                    
                    # éšå±¤æ§‹é€ 
                    dept_effect = pm.Normal('dept_effect', mu=mu_global, 
                                          sigma=sigma_dept, shape=n_depts)
                    user_effect = pm.Normal('user_effect', mu=0, 
                                          sigma=sigma_user, shape=n_users)
                    
                    # LambdaÂ³ç‰¹å¾´é‡ã®åŠ¹æœ
                    beta_lambda = {}
                    for feature_name in features:
                        if 'delta_LambdaC' in feature_name:
                            beta_lambda[feature_name] = pm.Normal(
                                f'beta_{feature_name}', mu=0, sigma=3
                            )
                    
                    # ç·šå½¢äºˆæ¸¬å­
                    mu = dept_effect[dept_idx] + user_effect[user_idx]
                    for feature_name, beta in beta_lambda.items():
                        if feature_name in data.columns:
                            mu += beta * data[feature_name].values
                    
                    # è¦³æ¸¬ãƒ¢ãƒ‡ãƒ«
                    sigma_obs = pm.HalfNormal('sigma_obs', sigma=1)
                    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma_obs, 
                                    observed=data['anomaly_score'].values)
                    
                    # MCMC
                    trace = pm.sample(
                        draws=2000,
                        tune=1000,
                        target_accept=0.9,
                        nuts_sampler="numpyro" if self.config.use_jax else None
                    )
                
                self.models['layer3_deep'] = (model, trace)
    
    def train_evolution_layer(self, features: Dict, data: pd.DataFrame):
        """Layer 4: ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®é€²åŒ–ï¼ˆé€±æ¬¡ï¼‰"""
        # ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«é¸æŠ
        models_to_compare = []
        
        # ãƒ¢ãƒ‡ãƒ«1: ã‚·ãƒ³ãƒ—ãƒ«ç·šå½¢
        models_to_compare.append(self._build_simple_model(features, data))
        
        # ãƒ¢ãƒ‡ãƒ«2: éç·šå½¢å¤‰æ›
        models_to_compare.append(self._build_nonlinear_model(features, data))
        
        # ãƒ¢ãƒ‡ãƒ«3: ã‚¹ãƒ‘ãƒ¼ã‚¹å›å¸°
        models_to_compare.append(self._build_sparse_model(features, data))
        
        # WAIC/LOOã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
        best_model_idx = self._compare_models(models_to_compare)
        self.models['layer4_evolution'] = models_to_compare[best_model_idx]

# =====================================
# 4. A/Bãƒ†ã‚¹ãƒˆçµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# =====================================

class BayesianABTesting:
    """ãƒ™ã‚¤ã‚ºçš„A/Bãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self, config: OptimizedL3Config):
        self.config = config
        self.test_results = []
    
    def run_bayesian_ab_test(self, model_a_trace, model_b_trace, test_data):
        """ãƒ™ã‚¤ã‚ºçš„A/Bãƒ†ã‚¹ãƒˆ"""
        # äº‹å¾Œäºˆæ¸¬åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        ppc_a = pm.sample_posterior_predictive(
            model_a_trace, 
            progressbar=False
        )
        ppc_b = pm.sample_posterior_predictive(
            model_b_trace,
            progressbar=False
        )
        
        # æ€§èƒ½æŒ‡æ¨™ã®äº‹å¾Œåˆ†å¸ƒ
        perf_a = self._calculate_performance_distribution(ppc_a, test_data)
        perf_b = self._calculate_performance_distribution(ppc_b, test_data)
        
        # ãƒ™ã‚¤ã‚ºçš„æ¯”è¼ƒ
        prob_b_better = np.mean(perf_b > perf_a)
        expected_improvement = np.mean(perf_b - perf_a)
        
        result = {
            'timestamp': datetime.now(),
            'prob_b_better': prob_b_better,
            'expected_improvement': expected_improvement,
            'decision': 'deploy_b' if prob_b_better > 0.95 else 'keep_a'
        }
        
        self.test_results.append(result)
        return result

# =====================================
# 5. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
# =====================================

class IntelligentDeployment:
    """ãƒ™ã‚¤ã‚ºæ¨è«–çµæœã‚’æ´»ç”¨ã—ãŸãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ"""

    def __init__(self, cfg: OptimizedL3Config):
        self.config = cfg
        self.deployment_history = []
        
        # Kubernetesè¨­å®š
        try:
            config.load_incluster_config()
        except:
            config.load_kube_config()
        
        self.k8s_apps = client.AppsV1Api()
    
    def deploy_with_uncertainty(self, model_trace, confidence_threshold=0.95):
        """ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãŸãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ"""
        # ãƒ¢ãƒ‡ãƒ«ã®ä¸ç¢ºå®Ÿæ€§ã‚’è©•ä¾¡
        uncertainty = self._evaluate_model_uncertainty(model_trace)
        
        if uncertainty['confidence'] >= confidence_threshold:
            # é«˜ä¿¡é ¼åº¦: ãƒ•ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
            self.full_deployment(model_trace)
        elif uncertainty['confidence'] >= 0.8:
            # ä¸­ä¿¡é ¼åº¦: ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
            self.canary_deployment(model_trace, canary_percentage=20)
        else:
            # ä½ä¿¡é ¼åº¦: ã‚·ãƒ£ãƒ‰ã‚¦ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
            self.shadow_deployment(model_trace)
    
    def _evaluate_model_uncertainty(self, trace):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¸ç¢ºå®Ÿæ€§è©•ä¾¡"""
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸè¨ºæ–­
        rhat_values = az.rhat(trace)
        max_rhat = float(rhat_values.max())
        
        # æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
        ess_values = az.ess(trace)
        min_ess = float(ess_values.min())
        
        # äº‹å¾Œåˆ†å¸ƒã®åˆ†æ•£
        posterior_std = float(trace.posterior.std())
        
        # ç·åˆçš„ãªä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        confidence = 1.0
        if max_rhat > 1.01:
            confidence *= 0.8
        if min_ess < 400:
            confidence *= 0.9
        if posterior_std > 2.0:
            confidence *= 0.85
        
        return {
            'confidence': confidence,
            'max_rhat': max_rhat,
            'min_ess': min_ess,
            'posterior_std': posterior_std
        }

# =====================================
# 6. çµ±åˆMLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# =====================================

class IntegratedMLOpsPipeline:
    """å®Œå…¨çµ±åˆMLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, config: OptimizedL3Config = None):
        if config is None:
            config = OptimizedL3Config()
        
        self.config = config
        self.data_collector = EnhancedDataCollector(config)
        self.preprocessor = Lambda3DataPreprocessor(config)
        self.trainer = HierarchicalBayesianTrainer(config)
        self.ab_tester = BayesianABTesting(config)
        self.deployer = IntelligentDeployment(config)
        self.diversity_cluster = DiversityAwareCluster()
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        self.metrics_history = []
    
    async def run_pipeline(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®å®Ÿè¡Œ"""
        # éåŒæœŸã‚¿ã‚¹ã‚¯ã®èµ·å‹•
        tasks = [
            asyncio.create_task(self.data_collector.collect_security_events()),
            asyncio.create_task(self.monitor_performance()),
            asyncio.create_task(self.health_check_loop())
        ]
        
        # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
        while True:
            try:
                # 1åˆ†ã”ã¨ã®å‡¦ç†
                await self.minute_cycle()
                
                # 1æ™‚é–“ã”ã¨ã®å‡¦ç†
                if datetime.now().minute == 0:
                    await self.hourly_cycle()
                
                # æ—¥æ¬¡å‡¦ç†
                if datetime.now().hour == 0 and datetime.now().minute == 0:
                    await self.daily_cycle()
                
                # é€±æ¬¡å‡¦ç†
                if datetime.now().weekday() == 0 and datetime.now().hour == 0:
                    await self.weekly_cycle()
                
                await asyncio.sleep(60)
                
            except Exception as e:
                self.handle_pipeline_error(e)
    
    async def minute_cycle(self):
        """1åˆ†ã‚µã‚¤ã‚¯ãƒ«: å³æ™‚é©å¿œ"""
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        features, data = await self.get_latest_features()
        
        if features and len(data) >= self.config.min_batch_size:
            # Layer 1: é«˜é€Ÿãƒ™ã‚¤ã‚ºå­¦ç¿’
            self.trainer.train_immediate_layer(features, data)
            
            # å³æ™‚äºˆæ¸¬ã¨ç•°å¸¸æ¤œçŸ¥
            predictions = self.predict_anomalies(features)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
            self.record_metrics({
                'timestamp': datetime.now(),
                'predictions': predictions,
                'data_size': len(data)
            })
    
    async def hourly_cycle(self):
        """1æ™‚é–“ã‚µã‚¤ã‚¯ãƒ«: æ¨™æº–å­¦ç¿’ã¨A/Bãƒ†ã‚¹ãƒˆ"""
        # éå»1æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿é›†ç´„
        features, data = await self.aggregate_hourly_data()
        
        # Layer 2: æ¨™æº–ãƒ™ã‚¤ã‚ºå­¦ç¿’
        self.trainer.train_fast_layer(features, data)
        
        # A/Bãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
        if self.should_run_ab_test():
            old_model = self.trainer.models.get('layer2_fast')
            new_model = self.trainer.models.get('layer1_immediate')
            
            if old_model and new_model:
                test_result = self.ab_tester.run_bayesian_ab_test(
                    old_model[1], new_model[1], data
                )
                
                if test_result['decision'] == 'deploy_b':
                    await self.deploy_model(new_model)
    
    async def daily_cycle(self):
        """æ—¥æ¬¡ã‚µã‚¤ã‚¯ãƒ«: éšå±¤ãƒ™ã‚¤ã‚ºå­¦ç¿’ï¼‹æ•µå¯¾çš„å¤šæ§˜æ€§é€²åŒ–"""
        # ç›´è¿‘24hç•°å¸¸ï¼†æ­£å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®æŠ½å‡º
        recent_anomalies = self.get_recent_anomalies(hours=24)
        normal_samples = self.get_recent_normal_samples(hours=24)
    
        adv_variants = []
        if recent_anomalies and normal_samples:
            adv_def = AdversarialDefense(normal_samples)
            for atk in recent_anomalies:
                adv_variants += adv_def.generate_attack_variations(atk, n_variations=20)
            # å¤šæ§˜æ€§ã‚¯ãƒ©ã‚¹ã‚¿ã«åæ˜ 
            for ns in normal_samples:
                self.diversity_cluster.add_sample(ns["user_id"], ns, np.array([ns["score"]]))
            for av in adv_variants:
                self.diversity_cluster.add_sample(av["user_id"], av, np.array([av["score"]]))
            # å¿…è¦ãªã‚‰recluster
            for user in set([e["user_id"] for e in normal_samples + adv_variants]):
                if self.diversity_cluster.need_recluster(user):
                    self.diversity_cluster.recluster(user)
            # â€»â†“å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã«å¤‰ç¨®ãƒ»æ­£å¸¸ã‚’è¿½åŠ ã™ã‚‹å ´åˆã¯ã“ã“ã§daily_dataã«appendã™ã‚‹
        
        # ---æ—¥æ¬¡å­¦ç¿’---
        daily_data = await self.prepare_daily_data()
        # ã“ã“ã§adv_variantsç­‰ã‚‚å«ã‚ã¦ã€Œ1æ—¥åˆ†ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã€ã¨ã—ã¦daily_data['dataframe']ã«
        self.trainer.train_deep_layer(
            daily_data['features'],
            daily_data['dataframe']
        )
        evaluation = self.evaluate_model_performance()
        if evaluation['improvement'] > 0.05:
            model_trace = self.trainer.models['layer3_deep'][1]
            self.deployer.deploy_with_uncertainty(
                model_trace,
                confidence_threshold=0.9
            )
                
    async def weekly_cycle(self):
        """é€±æ¬¡ã‚µã‚¤ã‚¯ãƒ«: ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®é€²åŒ–"""
        # é€±æ¬¡ãƒ‡ãƒ¼ã‚¿é›†ç´„
        weekly_data = await self.prepare_weekly_data()
        
        # Layer 4: ãƒ¢ãƒ‡ãƒ«é€²åŒ–
        self.trainer.train_evolution_layer(
            weekly_data['features'],
            weekly_data['dataframe']
        )
        
        # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ›´æ–°
        await self.update_architecture()

    def on_security_event(self, event):
        # 1. ãƒ­ãƒ¼ã‚«ãƒ«å³æ™‚é©å¿œ
        if hasattr(self, "kafka_producer"):
            producer = self.kafka_producer
        else:
            self.kafka_producer = KafkaProducer(
                bootstrap_servers=self.config.kafka_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8')
            )
            producer = self.kafka_producer
    
        if event.is_anomaly():
            auto_minibatch.add_event(event)
            producer.send('anomaly-topic', event.to_json())
            policy_manager.update_defense(event)
    
    def handle_pipeline_error(self, error: Exception):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"""
        error_info = {
            'timestamp': datetime.now(),
            'error_type': type(error).__name__,
            'error_message': str(error),
            'pipeline_state': self.get_pipeline_state()
        }
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
        mlflow.log_metrics({
            'error_count': 1,
            'error_timestamp': time.time()
        })
        
        # å¾©æ—§å‡¦ç†
        if isinstance(error, MemoryError):
            self.cleanup_memory()
        elif isinstance(error, TimeoutError):
            self.reset_connections()
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
        self.send_alert(error_info)

# =====================================
# å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# =====================================

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # è¨­å®šã®åˆæœŸåŒ–
    config = OptimizedL3Config(
        draws=500,
        tune=200,
        chains=2,
        use_jax=True,
        incremental_update=True,
        kafka_servers=['localhost:9092'],
        min_batch_size=100
    )
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®èµ·å‹•
    pipeline = IntegratedMLOpsPipeline(config)
    
    print("ğŸš€ çµ±åˆMLOps + LambdaÂ³ãƒ™ã‚¤ã‚ºæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³èµ·å‹•")
    print(f"è¨­å®š: JAX={config.use_jax}, Draws={config.draws}, Chains={config.chains}")
    
    # éåŒæœŸå®Ÿè¡Œ
    await pipeline.run_pipeline()

if __name__ == "__main__":
    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ
    asyncio.run(main())
